{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91f13774-fb30-4e42-a694-3761b8724ddc"
      },
      "source": [
        "<h1 align='center'> Diabetes Diagnostic Model Based On Convolutional Neural Network</h1>\n",
        "\n",
        "\n",
        "-----\n",
        "\n",
        "|  **Contacts**  ||\n",
        "|--------------------------|--------\n",
        "|      **Supervisor**      |  **Dr Md Zakir Hossain**\n",
        "|        **Student**       |  **Zeyu Zhang**\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "91f13774-fb30-4e42-a694-3761b8724ddc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80f6aef4-9578-4514-b7bf-d2450b85905b"
      },
      "source": [
        "## I. Import Libraries"
      ],
      "id": "80f6aef4-9578-4514-b7bf-d2450b85905b"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "a9f32dc5-74ff-4394-8165-2afaa5c624a1"
      },
      "outputs": [],
      "source": [
        "# Data Analysis and Visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn')\n",
        "%matplotlib inline\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "# sklearn for SML\n",
        "\n",
        "# Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression  \n",
        "from sklearn.linear_model import LogisticRegressionCV   # Logistic Regression with Cross Validation\n",
        "\n",
        "# PCA\n",
        "from sklearn.decomposition import PCA \n",
        "\n",
        "# t-SNE\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier  \n",
        "\n",
        "# SVM\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# encoding variables\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "\n",
        "# Standard Scaler \n",
        "from sklearn.preprocessing import StandardScaler       \n",
        "\n",
        "# data split\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "# Resample unbalanced data\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# nominal variable\n",
        "#from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# metrics\n",
        "from sklearn import metrics\n",
        "# Validation scoring\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "# import sklearn metrics for validation\n",
        "import sklearn.metrics as skm\n",
        "# cross validation\n",
        "from sklearn.model_selection import cross_validate \n",
        "\n",
        "# import cdist for SSE (Distortion)\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "\n",
        "# decision trees\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor      \n",
        "from sklearn import tree                               \n",
        "\n",
        "# KMeans Clustering\n",
        "from sklearn.cluster import KMeans                     \n",
        "\n",
        "\n",
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "# PyTorch for DL\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as trans\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "# Keras for DL\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "# !pip install torchmetrics\n",
        "# from torchmetrics import ConfusionMatrix # Confusion matrix for validation\n",
        "\n",
        "from numpy import loadtxt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "# Grid Search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "a9f32dc5-74ff-4394-8165-2afaa5c624a1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59252687-25f2-4cbc-ad86-f7bb48f79398"
      },
      "source": [
        "## II. KNN Classifier & Validation\n",
        "\n",
        "The reason why we choose KNN is that it is very efficient unless the input data is high-dimensional.\n",
        "\n",
        "This section includes:\n",
        "\n",
        "- **Spliting** dataframe for training, testing and validation\n",
        "- **Scaling** the predictor to normalize the impact factor, since each predictor has a different degree of impact on the outcome\n",
        "    - Here, we use `StandardScaler()` to achieve this, we can also use `heatmap` alternatively.\n",
        "- Finding the **Optimal K** for knn model based on iteration\n",
        "- **Training** KNN model\n",
        "- Creating a **validation** set\n",
        "- **Output** the validation, training and testing scores for the best model"
      ],
      "id": "59252687-25f2-4cbc-ad86-f7bb48f79398"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "e3e38863-080f-436a-81dd-6aad5b07422b"
      },
      "outputs": [],
      "source": [
        "# Method StandardScaler()\n",
        "\n",
        "def data_split(data):\n",
        "    \n",
        "    X = data.iloc[:,:-1]\n",
        "    y = data.iloc[:,-1]\n",
        "    \n",
        "    train_x, test_x, train_y, test_y = train_test_split(X,y,test_size = 0.2, random_state=0)\n",
        "    return train_x, test_x, train_y, test_y\n",
        "\n",
        "def data_scaling(train_x, test_x):\n",
        "\n",
        "    ss = StandardScaler()\n",
        "    ss_model = ss.fit(train_x)\n",
        "    train_x_scaled = ss_model.transform(train_x)\n",
        "    test_x_scaled = ss_model.transform(test_x)\n",
        "    return train_x_scaled, test_x_scaled\n",
        "\n",
        "def knn_classifier(df):\n",
        "    train_x, test_x, train_y, test_y = data_split(df)\n",
        "    train_x_scaled, test_x_scaled = data_scaling(train_x, test_x)\n",
        "    #-------------------------\n",
        "    # find the optimal K\n",
        "    # create a validation set\n",
        "    train_x_scaled, val_x_scaled, train_y, val_y = train_test_split(train_x_scaled,train_y, test_size=0.2, random_state=0)\n",
        "\n",
        "    best_k = -1\n",
        "    best_score = -1\n",
        "    for k in [1,2,3,5,7,9,15,31,51,train_x_scaled.shape[0]]:\n",
        "        knn = KNeighborsClassifier(n_neighbors=k)    # just change the n_neighbors parameter\n",
        "        knn_model = knn.fit(train_x_scaled, train_y) # scaled X, un-scaled y\n",
        "        train_score = knn.score(train_x_scaled, train_y)\n",
        "        val_score = knn.score(val_x_scaled, val_y)\n",
        "        print(\"k:\", k, \"Training Score:\", train_score, \"Validation Score: \", val_score)\n",
        "        # find the best k\n",
        "        if best_score <= val_score:\n",
        "            best_score = val_score\n",
        "            best_model = knn_model\n",
        "            best_k = k\n",
        "    \n",
        "    tn, fp, fn, tp = confusion_matrix(test_y, best_model.predict(test_x_scaled)).ravel()\n",
        "    print(f'The best k is {best_k} and the best val score is {best_score:.4f}')\n",
        "\n",
        "\n",
        "    # Print the validation, training and testing scores for the best model  \n",
        "    print(\"Best Model Validation Score: {:.4f}\".format(best_model.score(val_x_scaled, val_y)), \n",
        "    \"\\n Training Score: {:.4f}\".format(best_model.score(train_x_scaled, train_y)),\n",
        "    \"\\n Test Score: {:.4f}\".format(best_model.score(test_x_scaled, test_y)))\n",
        "    print(\"Testing: Sensitivity/Recall:\", tp/(tp+fn), \"Specificity:\", tn/(tn+fp))"
      ],
      "id": "e3e38863-080f-436a-81dd-6aad5b07422b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20eaa91e-4280-4f01-8a89-bad59f372d73"
      },
      "source": [
        "## III. Logistic Regression with 5 Fold Cross Validation\n",
        "\n",
        "The reason why we choose Logistic Regression is that it's one of the most suitable classifier for binomial classification."
      ],
      "id": "20eaa91e-4280-4f01-8a89-bad59f372d73"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3183421d-b04a-4bff-9a08-16c2713d2083"
      },
      "outputs": [],
      "source": [
        "def lg_classifier(df):\n",
        "    train_x, test_x, train_y, test_y = data_split(df)\n",
        "    mdl = LogisticRegressionCV(cv=5, multi_class='ovr', random_state=0).fit(train_x, train_y)\n",
        "    tn, fp, fn, tp = confusion_matrix(test_y, mdl.predict(test_x)).ravel()\n",
        "\n",
        "\n",
        "    print(\"Training Accuracy:\", mdl.score(train_x, train_y))\n",
        "    print(\"Testing Accuracy:\", mdl.score(test_x, test_y))\n",
        "    print(\"Testing: Sensitivity/Recall:\", tp/(tp+fn), \"Specificity:\", tn/(tn+fp))\n"
      ],
      "id": "3183421d-b04a-4bff-9a08-16c2713d2083"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IV. SVM"
      ],
      "metadata": {
        "id": "gKPM-fKs2Hha"
      },
      "id": "gKPM-fKs2Hha"
    },
    {
      "cell_type": "code",
      "source": [
        "def svm(df):\n",
        "  train_x, test_x, train_y, test_y = data_split(df)\n",
        "  mdl = SVC(kernel='linear').fit(train_x, train_y)\n",
        "  tn, fp, fn, tp = confusion_matrix(test_y, mdl.predict(test_x)).ravel()\n",
        "\n",
        "  print(\"Training Accuracy:\", mdl.score(train_x, train_y))\n",
        "  print(\"Testing Accuracy:\", mdl.score(test_x, test_y))\n",
        "  print(\"Testing: Sensitivity/Recall:\", tp/(tp+fn), \"Specificity:\", tn/(tn+fp))\n"
      ],
      "metadata": {
        "id": "vTRpK7Oz2N_8"
      },
      "id": "vTRpK7Oz2N_8",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57c2052d-e420-4234-a357-9049c93c5eac"
      },
      "source": [
        "## V. Screening of Datasets\n",
        "\n",
        "First we need to study the correlation between the predictors and outcomes. In order to find the appropriate dataset as the research object, the screening of multiple is indispensable.  \n",
        "\n",
        "\n",
        "### Canidate Data Sources\n",
        "\n",
        "- **`CDC_BRFSS2015`** - It's a dataset of 253,680 survey responses to the CDC's BRFSS2015.\n",
        "- **`NIDDK_Pima`** - It's a dataset of 768 females at least 21 years old of Pima Indian heritage responses to National Institute of Diabetes and Digestive and Kidney Diseases's survey.\n",
        "- **`Sylhet`** - This dataset has been collected using direct questionnaires from 520 patients of Sylhet Diabetes Hospital in Sylhet, Bangladesh.\n",
        "- **`BIT_2019`** - This dataset was collected in 2019 by Neha Prerna Tigga and Dr. Shruti Garg of the Department of Computer Science and Engineering, BIT Mesra\n",
        "- **`John`** - These data are courtesy of Dr John Schorling, Department of Medicine, University of Virginia School of Medicine. The data consist of 19 variables on 403 subjects from 1046 subjects who were interviewed in a study to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia for African Americans. \n",
        "\n",
        "- **`John_2`** - We also keep another version of Diabetes data of Dr John Schorling\n",
        "\n",
        "-----\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "57c2052d-e420-4234-a357-9049c93c5eac"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4417397d-66cc-45c5-8f28-e8d068a5c9b9"
      },
      "source": [
        "## 1. CDC BRFSS2015 Database\n",
        "\n",
        "This is a clean dataset of 253,680 survey responses to the CDC's BRFSS2015 with 22 features. These features are either questions directly asked of participants, or calculated variables based on individual participant responses.\n",
        "\n",
        "####  Attribute Information\n",
        "| Column Name    | Expression    |\n",
        "| :------------- | :------------- |\n",
        "| Diabetes_binary| 0 = no diabetes 1 = diabetes |\n",
        "| HighBP      | 0 = no high BP 1 = high BP |\n",
        "| HighChol    | 0 = no high cholesterol 1 = high cholesterol |\n",
        "| CholCheck       | 0 = no cholesterol check in 5 years 1 = yes cholesterol check in 5 years |\n",
        "| BMI            | Body Mass Index |\n",
        "| Smoker      | Have you smoked at least 100 cigarettes in your entire life? [Note: 5 packs = 100 cigarettes] 0 = no 1 = yes |\n",
        "| Stroke       | (Ever told) you had a stroke. 0 = no 1 = yes |\n",
        "| HeartDiseaseorAttack        | coronary heart disease (CHD) or myocardial infarction (MI) 0 = no 1 = yes |\n",
        "| PhysActivity       | physical activity in past 30 days - not including job 0 = no 1 = yes |\n",
        "| Fruits        | Consume Fruit 1 or more times per day 0 = no 1 = yes |\n",
        "| Veggies        | Consume Vegetables 1 or more times per day 0 = no 1 = yes |\n",
        "| HvyAlcoholConsump        | (adult men >=14 drinks per week and adult women>=7 drinks per week) 0 = no 1 = yes |\n",
        "| AnyHealthcare        | Have any kind of health care coverage, including health insurance, prepaid plans such as HMO, etc. 0 = no 1 = yes |\n",
        "| NoDocbcCost        | Was there a time in the past 12 months when you needed to see a doctor but could not because of cost? 0 = no 1 = yes |\n",
        "| GenHlth       | Would you say that in general your health is: scale 1-5 1 = excellent 2 = very good 3 = good 4 = fair 5 = poor |\n",
        "| MentHlth        | days of poor mental health scale 1-30 days |\n",
        "| PhysHlth        | physical illness or injury days in past 30 days scale 1-30 |\n",
        "| DiffWalk        | Do you have serious difficulty walking or climbing stairs? 0 = no 1 = yes |\n",
        "| Sex        | 0 = female 1 = male |\n",
        "| Age        | 13-level age category 1 = 18-24 9 = 60-64 13 = 80 or older |\n",
        "| Education        | Education level scale 1-6 1 = Never attended school or only kindergarten 2 = elementary etc. |\n",
        "| Income        | Income scale 1-8 1 = less than `$10000`, 5 = less than `$35000`, 8 = `$75000` or more |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Loading data\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "4417397d-66cc-45c5-8f28-e8d068a5c9b9"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "f21c79c7-61d8-42a3-b510-b2a25d76b8f5",
        "outputId": "ad06ad41-c2ab-4b9d-e389-8e20fbf790fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Any null value: True\n",
            "Any NaN value: True\n",
            "Before Droping NaN Number of Rows: 253680\n",
            "After Droping NaN Number of Rows: 253680\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
              "253675     1.0       1.0        1.0  45.0     0.0     0.0   \n",
              "253676     1.0       1.0        1.0  18.0     0.0     0.0   \n",
              "253677     0.0       0.0        1.0  28.0     0.0     0.0   \n",
              "253678     1.0       0.0        1.0  23.0     0.0     0.0   \n",
              "253679     1.0       1.0        1.0  25.0     0.0     0.0   \n",
              "\n",
              "        HeartDiseaseorAttack  PhysActivity  Fruits  Veggies  ...  NoDocbcCost  \\\n",
              "253675                   0.0           0.0     1.0      1.0  ...          0.0   \n",
              "253676                   0.0           0.0     0.0      0.0  ...          0.0   \n",
              "253677                   0.0           1.0     1.0      0.0  ...          0.0   \n",
              "253678                   0.0           0.0     1.0      1.0  ...          0.0   \n",
              "253679                   1.0           1.0     1.0      0.0  ...          0.0   \n",
              "\n",
              "        GenHlth  MentHlth  PhysHlth  DiffWalk  Sex   Age  Education  Income  \\\n",
              "253675      3.0       0.0       5.0       0.0  1.0   5.0        6.0     7.0   \n",
              "253676      4.0       0.0       0.0       1.0  0.0  11.0        2.0     4.0   \n",
              "253677      1.0       0.0       0.0       0.0  0.0   2.0        5.0     2.0   \n",
              "253678      3.0       0.0       0.0       0.0  1.0   7.0        5.0     1.0   \n",
              "253679      2.0       0.0       0.0       0.0  0.0   9.0        6.0     2.0   \n",
              "\n",
              "        Diabetes_binary  \n",
              "253675              0.0  \n",
              "253676              1.0  \n",
              "253677              0.0  \n",
              "253678              0.0  \n",
              "253679              1.0  \n",
              "\n",
              "[5 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0af39d7a-a833-4860-a14f-09e8f326d845\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HighBP</th>\n",
              "      <th>HighChol</th>\n",
              "      <th>CholCheck</th>\n",
              "      <th>BMI</th>\n",
              "      <th>Smoker</th>\n",
              "      <th>Stroke</th>\n",
              "      <th>HeartDiseaseorAttack</th>\n",
              "      <th>PhysActivity</th>\n",
              "      <th>Fruits</th>\n",
              "      <th>Veggies</th>\n",
              "      <th>...</th>\n",
              "      <th>NoDocbcCost</th>\n",
              "      <th>GenHlth</th>\n",
              "      <th>MentHlth</th>\n",
              "      <th>PhysHlth</th>\n",
              "      <th>DiffWalk</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>Education</th>\n",
              "      <th>Income</th>\n",
              "      <th>Diabetes_binary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>253675</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253676</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253677</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253678</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253679</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0af39d7a-a833-4860-a14f-09e8f326d845')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0af39d7a-a833-4860-a14f-09e8f326d845 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0af39d7a-a833-4860-a14f-09e8f326d845');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "# Read 'CDC_BRFSS2015.csv'\n",
        "cdc_df = pd.read_csv(\"./data/CDC_BRFSS2015.csv\")\n",
        "\n",
        "# Drop NaN value\n",
        "print(\"Any null value:\", any(cdc_df.isnull()))\n",
        "print(\"Any NaN value:\", any(cdc_df.isna()))\n",
        "print(\"Before Droping NaN Number of Rows:\", len(cdc_df))\n",
        "\n",
        "cdc_df = cdc_df.dropna()\n",
        "print(\"After Droping NaN Number of Rows:\", len(cdc_df))\n",
        "\n",
        "# Move 'Diabetes_binary' column to the end of dataframe\n",
        "cdc_df['Diabetes_binary'] = cdc_df.pop('Diabetes_binary')\n",
        "\n",
        "cdc_df.tail()\n",
        "\n"
      ],
      "id": "f21c79c7-61d8-42a3-b510-b2a25d76b8f5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### Data Balance"
      ],
      "metadata": {
        "id": "nUCaJAdWhgN2"
      },
      "id": "nUCaJAdWhgN2"
    },
    {
      "cell_type": "code",
      "source": [
        "print((cdc_df.Diabetes_binary == 0).sum())\n",
        "print((cdc_df.Diabetes_binary == 1).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9P66M-bkJTw",
        "outputId": "bba62a3f-8887-4fbc-e98b-13990ca9e723"
      },
      "id": "b9P66M-bkJTw",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "218334\n",
            "35346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) Oversample"
      ],
      "metadata": {
        "id": "2TTMa-OmkJx4"
      },
      "id": "2TTMa-OmkJx4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Oversample\n",
        "#create two different dataframe of majority and minority class \n",
        "df_majority = cdc_df[(cdc_df['Diabetes_binary']==0)] \n",
        "df_minority = cdc_df[(cdc_df['Diabetes_binary']==1)] \n",
        "# oversample minority class\n",
        "df_minority_oversample = resample(df_minority, \n",
        "                                 replace=True,    # sample with replacement\n",
        "                                 n_samples= 218334, # to match majority class\n",
        "                                 random_state=42)  # reproducible results\n",
        "# Combine\n",
        "cdc_df = pd.concat([df_minority_oversample, df_majority])\n",
        "\n",
        "# Shuffle\n",
        "cdc_df = cdc_df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Th0ZqaPUhrXK"
      },
      "id": "Th0ZqaPUhrXK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((cdc_df.Diabetes_binary == 0).sum())\n",
        "print((cdc_df.Diabetes_binary == 1).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9Pb5vEzkn2t",
        "outputId": "f491fe35-b9ff-4f43-c129-4073f4d47a70"
      },
      "id": "B9Pb5vEzkn2t",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "218334\n",
            "218334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) Undersample"
      ],
      "metadata": {
        "id": "nMmWk74ri-Wz"
      },
      "id": "nMmWk74ri-Wz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Undersample\n",
        "#create two different dataframe of majority and minority class \n",
        "df_majority = cdc_df[(cdc_df['Diabetes_binary']==0)] \n",
        "df_minority = cdc_df[(cdc_df['Diabetes_binary']==1)] \n",
        "# oversample minority class\n",
        "df_majority_undersample = resample(df_majority, \n",
        "                                 replace=True,    # sample with replacement\n",
        "                                 n_samples= 35346, # to match minority class\n",
        "                                 random_state=42)  # reproducible results\n",
        "# Combine\n",
        "cdc_df = pd.concat([df_majority_undersample, df_minority])\n",
        "\n",
        "# Shuffle\n",
        "cdc_df = cdc_df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Dyerkne5kreG"
      },
      "id": "Dyerkne5kreG",
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((cdc_df.Diabetes_binary == 0).sum())\n",
        "print((cdc_df.Diabetes_binary == 1).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-YSJ3GtlWgS",
        "outputId": "3c322436-aaac-456d-f479-74878e8392c6"
      },
      "id": "8-YSJ3GtlWgS",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35346\n",
            "35346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "ca754f69-d709-4a98-b447-2af55408b071",
        "outputId": "138202ec-6eea-4516-fd2a-87e135f89818"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        HighBP  HighChol  CholCheck       BMI    Smoker  \\\n",
              "HighBP                1.000000  0.311236   0.099884  0.236281  0.089950   \n",
              "HighChol              0.311236  1.000000   0.083554  0.127300  0.095690   \n",
              "CholCheck             0.099884  0.083554   1.000000  0.042715  0.000212   \n",
              "BMI                   0.236281  0.127300   0.042715  1.000000  0.016177   \n",
              "Smoker                0.089950  0.095690   0.000212  0.016177  1.000000   \n",
              "Stroke                0.126744  0.094960   0.021334  0.023881  0.056371   \n",
              "HeartDiseaseorAttack  0.208756  0.179297   0.041997  0.060535  0.121253   \n",
              "PhysActivity         -0.135254 -0.093568   0.001372 -0.170975 -0.078780   \n",
              "Fruits               -0.040702 -0.047099   0.019675 -0.088202 -0.073219   \n",
              "Veggies              -0.070827 -0.045464   0.002930 -0.062403 -0.026060   \n",
              "HvyAlcoholConsump    -0.035925 -0.030222  -0.026800 -0.063586  0.085625   \n",
              "AnyHealthcare         0.038252  0.036827   0.114613 -0.013365 -0.011749   \n",
              "NoDocbcCost           0.028051  0.033963  -0.048852  0.068475  0.035422   \n",
              "GenHlth               0.321972  0.240876   0.055604  0.267767  0.148567   \n",
              "MentHlth              0.065890  0.087067  -0.006995  0.108843  0.089474   \n",
              "PhysHlth              0.170594  0.143679   0.032952  0.162341  0.113456   \n",
              "DiffWalk              0.233808  0.163677   0.045078  0.242992  0.116286   \n",
              "Sex                   0.042522  0.017362  -0.012597 -0.000057  0.113016   \n",
              "Age                   0.338744  0.242178   0.096109 -0.043518  0.108172   \n",
              "Education            -0.146916 -0.083378  -0.007685 -0.105073 -0.133464   \n",
              "Income               -0.189234 -0.109895   0.001493 -0.128645 -0.104473   \n",
              "Diabetes_binary       0.376526  0.286841   0.111563  0.285256  0.088019   \n",
              "\n",
              "                        Stroke  HeartDiseaseorAttack  PhysActivity    Fruits  \\\n",
              "HighBP                0.126744              0.208756     -0.135254 -0.040702   \n",
              "HighChol              0.094960              0.179297     -0.093568 -0.047099   \n",
              "CholCheck             0.021334              0.041997      0.001372  0.019675   \n",
              "BMI                   0.023881              0.060535     -0.170975 -0.088202   \n",
              "Smoker                0.056371              0.121253     -0.078780 -0.073219   \n",
              "Stroke                1.000000              0.222207     -0.076980 -0.016357   \n",
              "HeartDiseaseorAttack  0.222207              1.000000     -0.099269 -0.021003   \n",
              "PhysActivity         -0.076980             -0.099269      1.000000  0.131511   \n",
              "Fruits               -0.016357             -0.021003      0.131511  1.000000   \n",
              "Veggies              -0.046955             -0.037118      0.150867  0.241294   \n",
              "HvyAlcoholConsump    -0.027784             -0.040163      0.018412 -0.028663   \n",
              "AnyHealthcare         0.008870              0.021352      0.022745  0.026888   \n",
              "NoDocbcCost           0.035878              0.034954     -0.062051 -0.042598   \n",
              "GenHlth               0.188516              0.279887     -0.278534 -0.099497   \n",
              "MentHlth              0.081412              0.076207     -0.134311 -0.062810   \n",
              "PhysHlth              0.162759              0.198494     -0.239389 -0.044264   \n",
              "DiffWalk              0.190998              0.236190     -0.279045 -0.052373   \n",
              "Sex                   0.001387              0.097965      0.056067 -0.090690   \n",
              "Age                   0.123750              0.220956     -0.102783  0.058032   \n",
              "Education            -0.076382             -0.100573      0.194089  0.099870   \n",
              "Income               -0.135411             -0.148558      0.200856  0.077209   \n",
              "Diabetes_binary       0.122279              0.209694     -0.158864 -0.055899   \n",
              "\n",
              "                       Veggies  ...  NoDocbcCost   GenHlth  MentHlth  \\\n",
              "HighBP               -0.070827  ...     0.028051  0.321972  0.065890   \n",
              "HighChol             -0.045464  ...     0.033963  0.240876  0.087067   \n",
              "CholCheck             0.002930  ...    -0.048852  0.055604 -0.006995   \n",
              "BMI                  -0.062403  ...     0.068475  0.267767  0.108843   \n",
              "Smoker               -0.026060  ...     0.035422  0.148567  0.089474   \n",
              "Stroke               -0.046955  ...     0.035878  0.188516  0.081412   \n",
              "HeartDiseaseorAttack -0.037118  ...     0.034954  0.279887  0.076207   \n",
              "PhysActivity          0.150867  ...    -0.062051 -0.278534 -0.134311   \n",
              "Fruits                0.241294  ...    -0.042598 -0.099497 -0.062810   \n",
              "Veggies               1.000000  ...    -0.035863 -0.117172 -0.052749   \n",
              "HvyAlcoholConsump     0.020708  ...    -0.000768 -0.061421  0.014268   \n",
              "AnyHealthcare         0.026171  ...    -0.221377 -0.030046 -0.054014   \n",
              "NoDocbcCost          -0.035863  ...     1.000000  0.166084  0.192966   \n",
              "GenHlth              -0.117172  ...     0.166084  1.000000  0.321608   \n",
              "MentHlth             -0.052749  ...     0.192966  0.321608  1.000000   \n",
              "PhysHlth             -0.069110  ...     0.148831  0.552195  0.378340   \n",
              "DiffWalk             -0.085216  ...     0.125478  0.478556  0.253435   \n",
              "Sex                  -0.052097  ...    -0.047845 -0.020582 -0.088719   \n",
              "Age                  -0.018395  ...    -0.125331  0.160029 -0.096865   \n",
              "Education             0.150039  ...    -0.099197 -0.289245 -0.107411   \n",
              "Income                0.152922  ...    -0.195289 -0.387299 -0.217659   \n",
              "Diabetes_binary      -0.082044  ...     0.045360  0.407130  0.092612   \n",
              "\n",
              "                      PhysHlth  DiffWalk       Sex       Age  Education  \\\n",
              "HighBP                0.170594  0.233808  0.042522  0.338744  -0.146916   \n",
              "HighChol              0.143679  0.163677  0.017362  0.242178  -0.083378   \n",
              "CholCheck             0.032952  0.045078 -0.012597  0.096109  -0.007685   \n",
              "BMI                   0.162341  0.242992 -0.000057 -0.043518  -0.105073   \n",
              "Smoker                0.113456  0.116286  0.113016  0.108172  -0.133464   \n",
              "Stroke                0.162759  0.190998  0.001387  0.123750  -0.076382   \n",
              "HeartDiseaseorAttack  0.198494  0.236190  0.097965  0.220956  -0.100573   \n",
              "PhysActivity         -0.239389 -0.279045  0.056067 -0.102783   0.194089   \n",
              "Fruits               -0.044264 -0.052373 -0.090690  0.058032   0.099870   \n",
              "Veggies              -0.069110 -0.085216 -0.052097 -0.018395   0.150039   \n",
              "HvyAlcoholConsump    -0.038640 -0.053504  0.011840 -0.062859   0.033503   \n",
              "AnyHealthcare        -0.002904  0.014864 -0.008788  0.144653   0.099047   \n",
              "NoDocbcCost           0.148831  0.125478 -0.047845 -0.125331  -0.099197   \n",
              "GenHlth               0.552195  0.478556 -0.020582  0.160029  -0.289245   \n",
              "MentHlth              0.378340  0.253435 -0.088719 -0.096865  -0.107411   \n",
              "PhysHlth              1.000000  0.488833 -0.047683  0.087515  -0.159820   \n",
              "DiffWalk              0.488833  1.000000 -0.086044  0.195489  -0.202161   \n",
              "Sex                  -0.047683 -0.086044  1.000000 -0.001228   0.044141   \n",
              "Age                   0.087515  0.195489 -0.001228  1.000000  -0.107129   \n",
              "Education            -0.159820 -0.202161  0.044141 -0.107129   1.000000   \n",
              "Income               -0.280098 -0.342881  0.161548 -0.133540   0.462435   \n",
              "Diabetes_binary       0.212632  0.268147  0.046437  0.274125  -0.171768   \n",
              "\n",
              "                        Income  Diabetes_binary  \n",
              "HighBP               -0.189234         0.376526  \n",
              "HighChol             -0.109895         0.286841  \n",
              "CholCheck             0.001493         0.111563  \n",
              "BMI                  -0.128645         0.285256  \n",
              "Smoker               -0.104473         0.088019  \n",
              "Stroke               -0.135411         0.122279  \n",
              "HeartDiseaseorAttack -0.148558         0.209694  \n",
              "PhysActivity          0.200856        -0.158864  \n",
              "Fruits                0.077209        -0.055899  \n",
              "Veggies               0.152922        -0.082044  \n",
              "HvyAlcoholConsump     0.067077        -0.099096  \n",
              "AnyHealthcare         0.132417         0.026085  \n",
              "NoDocbcCost          -0.195289         0.045360  \n",
              "GenHlth              -0.387299         0.407130  \n",
              "MentHlth             -0.217659         0.092612  \n",
              "PhysHlth             -0.280098         0.212632  \n",
              "DiffWalk             -0.342881         0.268147  \n",
              "Sex                   0.161548         0.046437  \n",
              "Age                  -0.133540         0.274125  \n",
              "Education             0.462435        -0.171768  \n",
              "Income                1.000000        -0.228165  \n",
              "Diabetes_binary      -0.228165         1.000000  \n",
              "\n",
              "[22 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b5e8e6e-e3ba-4d95-a222-7f57dafb9133\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HighBP</th>\n",
              "      <th>HighChol</th>\n",
              "      <th>CholCheck</th>\n",
              "      <th>BMI</th>\n",
              "      <th>Smoker</th>\n",
              "      <th>Stroke</th>\n",
              "      <th>HeartDiseaseorAttack</th>\n",
              "      <th>PhysActivity</th>\n",
              "      <th>Fruits</th>\n",
              "      <th>Veggies</th>\n",
              "      <th>...</th>\n",
              "      <th>NoDocbcCost</th>\n",
              "      <th>GenHlth</th>\n",
              "      <th>MentHlth</th>\n",
              "      <th>PhysHlth</th>\n",
              "      <th>DiffWalk</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>Education</th>\n",
              "      <th>Income</th>\n",
              "      <th>Diabetes_binary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>HighBP</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.311236</td>\n",
              "      <td>0.099884</td>\n",
              "      <td>0.236281</td>\n",
              "      <td>0.089950</td>\n",
              "      <td>0.126744</td>\n",
              "      <td>0.208756</td>\n",
              "      <td>-0.135254</td>\n",
              "      <td>-0.040702</td>\n",
              "      <td>-0.070827</td>\n",
              "      <td>...</td>\n",
              "      <td>0.028051</td>\n",
              "      <td>0.321972</td>\n",
              "      <td>0.065890</td>\n",
              "      <td>0.170594</td>\n",
              "      <td>0.233808</td>\n",
              "      <td>0.042522</td>\n",
              "      <td>0.338744</td>\n",
              "      <td>-0.146916</td>\n",
              "      <td>-0.189234</td>\n",
              "      <td>0.376526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HighChol</th>\n",
              "      <td>0.311236</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.083554</td>\n",
              "      <td>0.127300</td>\n",
              "      <td>0.095690</td>\n",
              "      <td>0.094960</td>\n",
              "      <td>0.179297</td>\n",
              "      <td>-0.093568</td>\n",
              "      <td>-0.047099</td>\n",
              "      <td>-0.045464</td>\n",
              "      <td>...</td>\n",
              "      <td>0.033963</td>\n",
              "      <td>0.240876</td>\n",
              "      <td>0.087067</td>\n",
              "      <td>0.143679</td>\n",
              "      <td>0.163677</td>\n",
              "      <td>0.017362</td>\n",
              "      <td>0.242178</td>\n",
              "      <td>-0.083378</td>\n",
              "      <td>-0.109895</td>\n",
              "      <td>0.286841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CholCheck</th>\n",
              "      <td>0.099884</td>\n",
              "      <td>0.083554</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.042715</td>\n",
              "      <td>0.000212</td>\n",
              "      <td>0.021334</td>\n",
              "      <td>0.041997</td>\n",
              "      <td>0.001372</td>\n",
              "      <td>0.019675</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048852</td>\n",
              "      <td>0.055604</td>\n",
              "      <td>-0.006995</td>\n",
              "      <td>0.032952</td>\n",
              "      <td>0.045078</td>\n",
              "      <td>-0.012597</td>\n",
              "      <td>0.096109</td>\n",
              "      <td>-0.007685</td>\n",
              "      <td>0.001493</td>\n",
              "      <td>0.111563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BMI</th>\n",
              "      <td>0.236281</td>\n",
              "      <td>0.127300</td>\n",
              "      <td>0.042715</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.016177</td>\n",
              "      <td>0.023881</td>\n",
              "      <td>0.060535</td>\n",
              "      <td>-0.170975</td>\n",
              "      <td>-0.088202</td>\n",
              "      <td>-0.062403</td>\n",
              "      <td>...</td>\n",
              "      <td>0.068475</td>\n",
              "      <td>0.267767</td>\n",
              "      <td>0.108843</td>\n",
              "      <td>0.162341</td>\n",
              "      <td>0.242992</td>\n",
              "      <td>-0.000057</td>\n",
              "      <td>-0.043518</td>\n",
              "      <td>-0.105073</td>\n",
              "      <td>-0.128645</td>\n",
              "      <td>0.285256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Smoker</th>\n",
              "      <td>0.089950</td>\n",
              "      <td>0.095690</td>\n",
              "      <td>0.000212</td>\n",
              "      <td>0.016177</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.056371</td>\n",
              "      <td>0.121253</td>\n",
              "      <td>-0.078780</td>\n",
              "      <td>-0.073219</td>\n",
              "      <td>-0.026060</td>\n",
              "      <td>...</td>\n",
              "      <td>0.035422</td>\n",
              "      <td>0.148567</td>\n",
              "      <td>0.089474</td>\n",
              "      <td>0.113456</td>\n",
              "      <td>0.116286</td>\n",
              "      <td>0.113016</td>\n",
              "      <td>0.108172</td>\n",
              "      <td>-0.133464</td>\n",
              "      <td>-0.104473</td>\n",
              "      <td>0.088019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Stroke</th>\n",
              "      <td>0.126744</td>\n",
              "      <td>0.094960</td>\n",
              "      <td>0.021334</td>\n",
              "      <td>0.023881</td>\n",
              "      <td>0.056371</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.222207</td>\n",
              "      <td>-0.076980</td>\n",
              "      <td>-0.016357</td>\n",
              "      <td>-0.046955</td>\n",
              "      <td>...</td>\n",
              "      <td>0.035878</td>\n",
              "      <td>0.188516</td>\n",
              "      <td>0.081412</td>\n",
              "      <td>0.162759</td>\n",
              "      <td>0.190998</td>\n",
              "      <td>0.001387</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>-0.076382</td>\n",
              "      <td>-0.135411</td>\n",
              "      <td>0.122279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HeartDiseaseorAttack</th>\n",
              "      <td>0.208756</td>\n",
              "      <td>0.179297</td>\n",
              "      <td>0.041997</td>\n",
              "      <td>0.060535</td>\n",
              "      <td>0.121253</td>\n",
              "      <td>0.222207</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.099269</td>\n",
              "      <td>-0.021003</td>\n",
              "      <td>-0.037118</td>\n",
              "      <td>...</td>\n",
              "      <td>0.034954</td>\n",
              "      <td>0.279887</td>\n",
              "      <td>0.076207</td>\n",
              "      <td>0.198494</td>\n",
              "      <td>0.236190</td>\n",
              "      <td>0.097965</td>\n",
              "      <td>0.220956</td>\n",
              "      <td>-0.100573</td>\n",
              "      <td>-0.148558</td>\n",
              "      <td>0.209694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PhysActivity</th>\n",
              "      <td>-0.135254</td>\n",
              "      <td>-0.093568</td>\n",
              "      <td>0.001372</td>\n",
              "      <td>-0.170975</td>\n",
              "      <td>-0.078780</td>\n",
              "      <td>-0.076980</td>\n",
              "      <td>-0.099269</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.131511</td>\n",
              "      <td>0.150867</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.062051</td>\n",
              "      <td>-0.278534</td>\n",
              "      <td>-0.134311</td>\n",
              "      <td>-0.239389</td>\n",
              "      <td>-0.279045</td>\n",
              "      <td>0.056067</td>\n",
              "      <td>-0.102783</td>\n",
              "      <td>0.194089</td>\n",
              "      <td>0.200856</td>\n",
              "      <td>-0.158864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Fruits</th>\n",
              "      <td>-0.040702</td>\n",
              "      <td>-0.047099</td>\n",
              "      <td>0.019675</td>\n",
              "      <td>-0.088202</td>\n",
              "      <td>-0.073219</td>\n",
              "      <td>-0.016357</td>\n",
              "      <td>-0.021003</td>\n",
              "      <td>0.131511</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.241294</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.042598</td>\n",
              "      <td>-0.099497</td>\n",
              "      <td>-0.062810</td>\n",
              "      <td>-0.044264</td>\n",
              "      <td>-0.052373</td>\n",
              "      <td>-0.090690</td>\n",
              "      <td>0.058032</td>\n",
              "      <td>0.099870</td>\n",
              "      <td>0.077209</td>\n",
              "      <td>-0.055899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Veggies</th>\n",
              "      <td>-0.070827</td>\n",
              "      <td>-0.045464</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>-0.062403</td>\n",
              "      <td>-0.026060</td>\n",
              "      <td>-0.046955</td>\n",
              "      <td>-0.037118</td>\n",
              "      <td>0.150867</td>\n",
              "      <td>0.241294</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.035863</td>\n",
              "      <td>-0.117172</td>\n",
              "      <td>-0.052749</td>\n",
              "      <td>-0.069110</td>\n",
              "      <td>-0.085216</td>\n",
              "      <td>-0.052097</td>\n",
              "      <td>-0.018395</td>\n",
              "      <td>0.150039</td>\n",
              "      <td>0.152922</td>\n",
              "      <td>-0.082044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HvyAlcoholConsump</th>\n",
              "      <td>-0.035925</td>\n",
              "      <td>-0.030222</td>\n",
              "      <td>-0.026800</td>\n",
              "      <td>-0.063586</td>\n",
              "      <td>0.085625</td>\n",
              "      <td>-0.027784</td>\n",
              "      <td>-0.040163</td>\n",
              "      <td>0.018412</td>\n",
              "      <td>-0.028663</td>\n",
              "      <td>0.020708</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000768</td>\n",
              "      <td>-0.061421</td>\n",
              "      <td>0.014268</td>\n",
              "      <td>-0.038640</td>\n",
              "      <td>-0.053504</td>\n",
              "      <td>0.011840</td>\n",
              "      <td>-0.062859</td>\n",
              "      <td>0.033503</td>\n",
              "      <td>0.067077</td>\n",
              "      <td>-0.099096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AnyHealthcare</th>\n",
              "      <td>0.038252</td>\n",
              "      <td>0.036827</td>\n",
              "      <td>0.114613</td>\n",
              "      <td>-0.013365</td>\n",
              "      <td>-0.011749</td>\n",
              "      <td>0.008870</td>\n",
              "      <td>0.021352</td>\n",
              "      <td>0.022745</td>\n",
              "      <td>0.026888</td>\n",
              "      <td>0.026171</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.221377</td>\n",
              "      <td>-0.030046</td>\n",
              "      <td>-0.054014</td>\n",
              "      <td>-0.002904</td>\n",
              "      <td>0.014864</td>\n",
              "      <td>-0.008788</td>\n",
              "      <td>0.144653</td>\n",
              "      <td>0.099047</td>\n",
              "      <td>0.132417</td>\n",
              "      <td>0.026085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NoDocbcCost</th>\n",
              "      <td>0.028051</td>\n",
              "      <td>0.033963</td>\n",
              "      <td>-0.048852</td>\n",
              "      <td>0.068475</td>\n",
              "      <td>0.035422</td>\n",
              "      <td>0.035878</td>\n",
              "      <td>0.034954</td>\n",
              "      <td>-0.062051</td>\n",
              "      <td>-0.042598</td>\n",
              "      <td>-0.035863</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.166084</td>\n",
              "      <td>0.192966</td>\n",
              "      <td>0.148831</td>\n",
              "      <td>0.125478</td>\n",
              "      <td>-0.047845</td>\n",
              "      <td>-0.125331</td>\n",
              "      <td>-0.099197</td>\n",
              "      <td>-0.195289</td>\n",
              "      <td>0.045360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GenHlth</th>\n",
              "      <td>0.321972</td>\n",
              "      <td>0.240876</td>\n",
              "      <td>0.055604</td>\n",
              "      <td>0.267767</td>\n",
              "      <td>0.148567</td>\n",
              "      <td>0.188516</td>\n",
              "      <td>0.279887</td>\n",
              "      <td>-0.278534</td>\n",
              "      <td>-0.099497</td>\n",
              "      <td>-0.117172</td>\n",
              "      <td>...</td>\n",
              "      <td>0.166084</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.321608</td>\n",
              "      <td>0.552195</td>\n",
              "      <td>0.478556</td>\n",
              "      <td>-0.020582</td>\n",
              "      <td>0.160029</td>\n",
              "      <td>-0.289245</td>\n",
              "      <td>-0.387299</td>\n",
              "      <td>0.407130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MentHlth</th>\n",
              "      <td>0.065890</td>\n",
              "      <td>0.087067</td>\n",
              "      <td>-0.006995</td>\n",
              "      <td>0.108843</td>\n",
              "      <td>0.089474</td>\n",
              "      <td>0.081412</td>\n",
              "      <td>0.076207</td>\n",
              "      <td>-0.134311</td>\n",
              "      <td>-0.062810</td>\n",
              "      <td>-0.052749</td>\n",
              "      <td>...</td>\n",
              "      <td>0.192966</td>\n",
              "      <td>0.321608</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.378340</td>\n",
              "      <td>0.253435</td>\n",
              "      <td>-0.088719</td>\n",
              "      <td>-0.096865</td>\n",
              "      <td>-0.107411</td>\n",
              "      <td>-0.217659</td>\n",
              "      <td>0.092612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PhysHlth</th>\n",
              "      <td>0.170594</td>\n",
              "      <td>0.143679</td>\n",
              "      <td>0.032952</td>\n",
              "      <td>0.162341</td>\n",
              "      <td>0.113456</td>\n",
              "      <td>0.162759</td>\n",
              "      <td>0.198494</td>\n",
              "      <td>-0.239389</td>\n",
              "      <td>-0.044264</td>\n",
              "      <td>-0.069110</td>\n",
              "      <td>...</td>\n",
              "      <td>0.148831</td>\n",
              "      <td>0.552195</td>\n",
              "      <td>0.378340</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.488833</td>\n",
              "      <td>-0.047683</td>\n",
              "      <td>0.087515</td>\n",
              "      <td>-0.159820</td>\n",
              "      <td>-0.280098</td>\n",
              "      <td>0.212632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DiffWalk</th>\n",
              "      <td>0.233808</td>\n",
              "      <td>0.163677</td>\n",
              "      <td>0.045078</td>\n",
              "      <td>0.242992</td>\n",
              "      <td>0.116286</td>\n",
              "      <td>0.190998</td>\n",
              "      <td>0.236190</td>\n",
              "      <td>-0.279045</td>\n",
              "      <td>-0.052373</td>\n",
              "      <td>-0.085216</td>\n",
              "      <td>...</td>\n",
              "      <td>0.125478</td>\n",
              "      <td>0.478556</td>\n",
              "      <td>0.253435</td>\n",
              "      <td>0.488833</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.086044</td>\n",
              "      <td>0.195489</td>\n",
              "      <td>-0.202161</td>\n",
              "      <td>-0.342881</td>\n",
              "      <td>0.268147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sex</th>\n",
              "      <td>0.042522</td>\n",
              "      <td>0.017362</td>\n",
              "      <td>-0.012597</td>\n",
              "      <td>-0.000057</td>\n",
              "      <td>0.113016</td>\n",
              "      <td>0.001387</td>\n",
              "      <td>0.097965</td>\n",
              "      <td>0.056067</td>\n",
              "      <td>-0.090690</td>\n",
              "      <td>-0.052097</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.047845</td>\n",
              "      <td>-0.020582</td>\n",
              "      <td>-0.088719</td>\n",
              "      <td>-0.047683</td>\n",
              "      <td>-0.086044</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.001228</td>\n",
              "      <td>0.044141</td>\n",
              "      <td>0.161548</td>\n",
              "      <td>0.046437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Age</th>\n",
              "      <td>0.338744</td>\n",
              "      <td>0.242178</td>\n",
              "      <td>0.096109</td>\n",
              "      <td>-0.043518</td>\n",
              "      <td>0.108172</td>\n",
              "      <td>0.123750</td>\n",
              "      <td>0.220956</td>\n",
              "      <td>-0.102783</td>\n",
              "      <td>0.058032</td>\n",
              "      <td>-0.018395</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.125331</td>\n",
              "      <td>0.160029</td>\n",
              "      <td>-0.096865</td>\n",
              "      <td>0.087515</td>\n",
              "      <td>0.195489</td>\n",
              "      <td>-0.001228</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.107129</td>\n",
              "      <td>-0.133540</td>\n",
              "      <td>0.274125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Education</th>\n",
              "      <td>-0.146916</td>\n",
              "      <td>-0.083378</td>\n",
              "      <td>-0.007685</td>\n",
              "      <td>-0.105073</td>\n",
              "      <td>-0.133464</td>\n",
              "      <td>-0.076382</td>\n",
              "      <td>-0.100573</td>\n",
              "      <td>0.194089</td>\n",
              "      <td>0.099870</td>\n",
              "      <td>0.150039</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.099197</td>\n",
              "      <td>-0.289245</td>\n",
              "      <td>-0.107411</td>\n",
              "      <td>-0.159820</td>\n",
              "      <td>-0.202161</td>\n",
              "      <td>0.044141</td>\n",
              "      <td>-0.107129</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.462435</td>\n",
              "      <td>-0.171768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Income</th>\n",
              "      <td>-0.189234</td>\n",
              "      <td>-0.109895</td>\n",
              "      <td>0.001493</td>\n",
              "      <td>-0.128645</td>\n",
              "      <td>-0.104473</td>\n",
              "      <td>-0.135411</td>\n",
              "      <td>-0.148558</td>\n",
              "      <td>0.200856</td>\n",
              "      <td>0.077209</td>\n",
              "      <td>0.152922</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195289</td>\n",
              "      <td>-0.387299</td>\n",
              "      <td>-0.217659</td>\n",
              "      <td>-0.280098</td>\n",
              "      <td>-0.342881</td>\n",
              "      <td>0.161548</td>\n",
              "      <td>-0.133540</td>\n",
              "      <td>0.462435</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.228165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Diabetes_binary</th>\n",
              "      <td>0.376526</td>\n",
              "      <td>0.286841</td>\n",
              "      <td>0.111563</td>\n",
              "      <td>0.285256</td>\n",
              "      <td>0.088019</td>\n",
              "      <td>0.122279</td>\n",
              "      <td>0.209694</td>\n",
              "      <td>-0.158864</td>\n",
              "      <td>-0.055899</td>\n",
              "      <td>-0.082044</td>\n",
              "      <td>...</td>\n",
              "      <td>0.045360</td>\n",
              "      <td>0.407130</td>\n",
              "      <td>0.092612</td>\n",
              "      <td>0.212632</td>\n",
              "      <td>0.268147</td>\n",
              "      <td>0.046437</td>\n",
              "      <td>0.274125</td>\n",
              "      <td>-0.171768</td>\n",
              "      <td>-0.228165</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b5e8e6e-e3ba-4d95-a222-7f57dafb9133')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6b5e8e6e-e3ba-4d95-a222-7f57dafb9133 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6b5e8e6e-e3ba-4d95-a222-7f57dafb9133');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "# Correlation contingency table\n",
        "cdc_corr = cdc_df.corr()\n",
        "cdc_corr"
      ],
      "id": "ca754f69-d709-4a98-b447-2af55408b071"
    },
    {
      "cell_type": "code",
      "source": [
        "print(cdc_corr.iloc[-1,:].sort_values(ascending=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fS4k-WjEWRt",
        "outputId": "ca57a795-0059-4882-b471-6848b7ceb966"
      },
      "id": "8fS4k-WjEWRt",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diabetes_binary         1.000000\n",
            "GenHlth                 0.407130\n",
            "HighBP                  0.376526\n",
            "HighChol                0.286841\n",
            "BMI                     0.285256\n",
            "Age                     0.274125\n",
            "DiffWalk                0.268147\n",
            "PhysHlth                0.212632\n",
            "HeartDiseaseorAttack    0.209694\n",
            "Stroke                  0.122279\n",
            "CholCheck               0.111563\n",
            "MentHlth                0.092612\n",
            "Smoker                  0.088019\n",
            "Sex                     0.046437\n",
            "NoDocbcCost             0.045360\n",
            "AnyHealthcare           0.026085\n",
            "Fruits                 -0.055899\n",
            "Veggies                -0.082044\n",
            "HvyAlcoholConsump      -0.099096\n",
            "PhysActivity           -0.158864\n",
            "Education              -0.171768\n",
            "Income                 -0.228165\n",
            "Name: Diabetes_binary, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "569b078f-5ad3-45ce-abf7-1c186f4a265c"
      },
      "source": [
        "Moreover, we evaluate the Weighted Average of the correlation of predictors:\n",
        "\n",
        "$\\text{Weighted Average of Correlation}$ $=$ $\\frac{\\text{Sum of Correlation of Predictors}}{\\text{Number of Predictors}}$ $=$ $0.098\n",
        "$\n"
      ],
      "id": "569b078f-5ad3-45ce-abf7-1c186f4a265c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4560cb5-3cc9-40d1-9fae-fea9ae136813",
        "outputId": "55c99b5f-841d-4905-9479-d831f46e97ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted Average: 0.09766407302066364\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the Weighted Average of the correlation of predictors\n",
        "\n",
        "print(\"Weighted Average:\", sum(cdc_corr.iloc[-1,:-1])/21)"
      ],
      "id": "b4560cb5-3cc9-40d1-9fae-fea9ae136813"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59bd63c5-07ad-48fb-81b4-d529545bcbf5"
      },
      "source": [
        "-----\n",
        "\n",
        "###  1.1 KNN with PCA\n",
        "\n",
        "Since it's a considerably large dataset with 253680 tuples and 21 predictors. Although it's not high-dimensional but it's obviously time consuming when training the model on a personal computer.\n",
        "\n",
        "So in order to address this issue, we should implement Dimensionality Reduction based on Principal Components Analysis (PCA).\n",
        "\n",
        "To begin with, we have to plot the Explained Variance vs Principal Component Diagraph to determine how many dimensions do we need to keep.\n",
        "\n"
      ],
      "id": "59bd63c5-07ad-48fb-81b4-d529545bcbf5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "b10b73f5-42ba-439c-b71c-99f471e81117",
        "outputId": "36856180-574d-4ed8-9e3f-a0d76b382de7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Scale the dataset; This is very important before you apply PCA\n",
        "\n",
        "train_x, test_x, train_y, test_y = data_split(cdc_df)\n",
        "train_x_scaled, test_x_scaled = data_scaling(train_x, test_x)\n",
        "train_x_scaled, val_x_scaled, train_y, val_y = train_test_split(train_x_scaled,train_y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Instantiate PCA\n",
        "pca = PCA()\n",
        "\n",
        "# Determine transformed features\n",
        "X_train_pca = pca.fit_transform(train_x_scaled)\n",
        "\n",
        "# Determine explained variance using explained_variance_ratio_ attribute\n",
        "exp_var_pca = pca.explained_variance_ratio_\n",
        "\n",
        "\n",
        "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
        "# for visualizing the variance explained by each principal component.\n",
        "\n",
        "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
        "\n",
        "# Create the visualization plot\n",
        "\n",
        "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
        "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "b10b73f5-42ba-439c-b71c-99f471e81117"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3054d2e8-8349-4bd9-9142-df3378bf0a97"
      },
      "source": [
        "The more components you include the more variance you explain and the less information you loose but there's a **trade off**.\n",
        "\n",
        "Ideally, we should choose the number of components to include in the model by adding the explained variance ratio of each component until it reaches a total of around **0.8** to avoid overfitting.\n",
        "\n",
        "Since we do not have powerful computational resources. We have to select 5 dimensions for the trade-off."
      ],
      "id": "3054d2e8-8349-4bd9-9142-df3378bf0a97"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a247513d-8e98-4283-ba05-b851aed13f90",
        "outputId": "4537fb90-ae41-4266-e515-d8b466fab781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k: 1 Training Score: 0.9954246054551081 Validation Score:  0.6672265935814694\n",
            "k: 2 Training Score: 0.8308430219707351 Validation Score:  0.6426487490053929\n",
            "k: 3 Training Score: 0.8291631669687458 Validation Score:  0.6846432676155955\n",
            "k: 5 Training Score: 0.7901065381724945 Validation Score:  0.6988771991866325\n",
            "k: 7 Training Score: 0.7760930109190575 Validation Score:  0.7055963221642648\n",
            "k: 9 Training Score: 0.7664117413023297 Validation Score:  0.7086022456016268\n",
            "k: 15 Training Score: 0.7536581053003846 Validation Score:  0.7164706922464857\n",
            "k: 31 Training Score: 0.7450599000928341 Validation Score:  0.7269030147643887\n",
            "k: 51 Training Score: 0.7399540250209982 Validation Score:  0.7287596145345239\n",
            "k: 71 Training Score: 0.7386720304142169 Validation Score:  0.7270798337901159\n",
            "k: 91 Training Score: 0.7381636532425622 Validation Score:  0.7277871098930245\n",
            "The best k is 51 and the best val score is 0.7288\n",
            "Best Model Validation Score: 0.7288 \n",
            " Training Score: 0.7400 \n",
            " Test Score: 0.7431\n",
            "Testing: Sensitivity/Recall: 0.8038331454340474 Specificity: 0.6818117279568365\n"
          ]
        }
      ],
      "source": [
        "pca = PCA(n_components = 5).fit(train_x_scaled)\n",
        "\n",
        "X_train_pca = pca.transform(train_x_scaled)\n",
        "\n",
        "X_test_pca = pca.transform(test_x_scaled)\n",
        "\n",
        "X_val_pca = pca.transform(val_x_scaled)\n",
        "\n",
        "best_k = -1\n",
        "best_score = -1\n",
        "for k in [1,2,3,5,7,9,15,31,51,71,91]:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)    # just change the n_neighbors parameter\n",
        "    knn_model = knn.fit(X_train_pca, train_y) # scaled X, un-scaled y\n",
        "    train_score = knn.score(X_train_pca, train_y)\n",
        "    val_score = knn.score(X_val_pca, val_y)\n",
        "\n",
        "    print(\"k:\", k, \"Training Score:\", train_score, \"Validation Score: \", val_score)\n",
        "    # find the best k\n",
        "        # find the best k\n",
        "    if best_score <= val_score:\n",
        "        best_score = val_score\n",
        "        best_model = knn_model\n",
        "        best_k = k\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(test_y, best_model.predict(X_test_pca)).ravel()\n",
        "print(f'The best k is {best_k} and the best val score is {best_score:.4f}')\n",
        "\n",
        "\n",
        "# Print the validation, training and testing scores for the best model  \n",
        "print(\"Best Model Validation Score: {:.4f}\".format(best_model.score(X_val_pca, val_y)), \n",
        "\"\\n Training Score: {:.4f}\".format(best_model.score(X_train_pca, train_y)),\n",
        "\"\\n Test Score: {:.4f}\".format(best_model.score(X_test_pca, test_y)))\n",
        "print(\"Testing: Sensitivity/Recall:\", tp/(tp+fn), \"Specificity:\", tn/(tn+fp))\n"
      ],
      "id": "a247513d-8e98-4283-ba05-b851aed13f90"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbd8a3bd-536b-47d2-8c57-60bc85c0fab1"
      },
      "source": [
        "####  Validation without PCA (took around 5000s)\n",
        "\n",
        "Luckily, we can see the trade-off between explained variance and information we keeped based on PCA does not cause to many losses compare with the traditional KNN (21 predictors), as long as we select the optimal K."
      ],
      "id": "dbd8a3bd-536b-47d2-8c57-60bc85c0fab1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### Visualization via PCA to 2-dim"
      ],
      "metadata": {
        "id": "OXccx-NSofWh"
      },
      "id": "OXccx-NSofWh"
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 2).fit(train_x_scaled)\n",
        "\n",
        "Xt = pca.transform(train_x_scaled)\n",
        "\n",
        "plot = plt.scatter(Xt[:300,0], Xt[:300,1], c=train_y[:300])\n",
        "plt.legend(handles=plot.legend_elements()[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "NWATvXs6ofgg",
        "outputId": "14ac3b8f-2457-4b93-a036-2a954fc98932"
      },
      "id": "NWATvXs6ofgg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 2).fit(train_x_scaled)\n",
        "\n",
        "Xt = pca.transform(train_x_scaled)\n",
        "\n",
        "plot = plt.scatter(Xt[:,0], Xt[:,1], c=train_y)\n",
        "plt.legend(handles=plot.legend_elements()[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "48I9qhrQqtKV",
        "outputId": "d25267ee-f8b1-4f71-ba5d-fcf51dc878d6"
      },
      "id": "48I9qhrQqtKV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f347b1e-e01c-4974-b8f9-34474f918899"
      },
      "source": [
        "-----\n",
        "\n",
        "###  1.2 Logistic Regression\n",
        "#### 1.2.1 Without scaling the data"
      ],
      "id": "7f347b1e-e01c-4974-b8f9-34474f918899"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeaa4158-45e8-4ba7-99db-33748e2b62ff",
        "outputId": "d217779e-f74f-4a34-8ca9-bfa050202e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.7446819797358231\n",
            "Testing Accuracy: 0.7482848857769291\n",
            "Testing: Sensitivity/Recall: 0.7727850784230607 Specificity: 0.7237326536391957\n"
          ]
        }
      ],
      "source": [
        "lg_classifier(cdc_df)"
      ],
      "id": "aeaa4158-45e8-4ba7-99db-33748e2b62ff"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.2 With scaling the data"
      ],
      "metadata": {
        "id": "f2VDjRyLDC95"
      },
      "id": "f2VDjRyLDC95"
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, test_x, train_y, test_y = data_split(cdc_df)\n",
        "train_x_scaled, test_x_scaled = data_scaling(train_x, test_x)\n",
        "\n",
        "mdl = LogisticRegressionCV(cv=5, multi_class='ovr', random_state=0).fit(train_x_scaled, train_y)\n",
        "tn, fp, fn, tp = confusion_matrix(test_y, mdl.predict(test_x_scaled)).ravel()\n",
        "\n",
        "\n",
        "print(\"Training Accuracy:\", mdl.score(train_x_scaled, train_y))\n",
        "print(\"Testing Accuracy:\", mdl.score(test_x_scaled, test_y))\n",
        "print(\"Testing: Sensitivity/Recall:\", tp/(tp+fn), \"Specificity:\", tn/(tn+fp))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8P-d3JHDG7H",
        "outputId": "c5f2ef6a-691b-4254-b004-d346f28c303d"
      },
      "id": "I8P-d3JHDG7H",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.7448234399589765\n",
            "Testing Accuracy: 0.7481434330575005\n",
            "Testing: Sensitivity/Recall: 0.7725024727992087 Specificity: 0.7237326536391957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lbp3Py0pZMt"
      },
      "source": [
        "-----\n",
        "\n",
        "###  1.3 Building a prototype classifier based on BPNN\n",
        "#### 1.3.1 NN Tuning via Grid Search\n",
        "\n",
        "\n",
        "We have following hyperparameters to be determined:\n",
        "\n",
        "![hyperpara](/img/hyperpara.png)\n",
        "\n",
        "**1. Principles for determining the number of hidden layers** \n",
        "\n",
        "![hidden](/img/hidden.jpg)\n",
        "\n",
        "**2. Principles for determining the number of neurons of hidden layers** \n",
        "\n",
        "We have following empirical formulas:\n",
        "\n",
        "1. $\\text{Hidden Neurons}$ $=$ $\\frac{\\text{Num of Training Data}}{(2 \\text{~} 10) {\\kern 3pt}*{\\kern 3pt} \\text{(Input Neurons + Output Neurons)}}$\n",
        "\n",
        "2. $\\text{Hidden Neurons}$ $=$ $\\sqrt{\\text{Input Neurons * Output Neurons}}$\n",
        "\n",
        "3. \n",
        "  - The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
        "  - The number of hidden neurons should be 2/3 of the size of the input layer plus 2/3 of the size of the output layer.\n",
        "  - The number of hidden neurons should be less than twice the size of the input layer.\n"
      ],
      "id": "-Lbp3Py0pZMt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare training and testing data\n",
        "\n",
        "train_x, test_x, train_y, test_y = data_split(cdc_df)\n",
        "\n",
        "train_x = np.float32(train_x)\n",
        "test_x = np.float32(test_x)\n",
        "\n",
        "# ------- Normalize the predictors or not ----------\n",
        "# train_x = train_x/train_x.max()\n",
        "# test_x = test_x/test_x.max()\n",
        "# --------------------------------------------------------\n",
        "\n",
        "def gridNN(layers, activation, optimizer):\n",
        "  mdl = Sequential()\n",
        "  for i, neuron in enumerate(layers):\n",
        "    mdl.add(Dense(neuron, activation = activation))\n",
        "  mdl.add(Dense(1, activation='sigmoid'))\n",
        "  mdl.compile(loss='binary_crossentropy', optimizer = optimizer, metrics=['accuracy'])\n",
        "  return mdl\n",
        "\n",
        "\n",
        "mdl = KerasClassifier(build_fn = gridNN)\n",
        "\n",
        "# Define Hyperparameters\n",
        "\n",
        "layers = [[2],[4],[6],[8],[16],[32],[64],[4,2],[8,4],[16,8],[32,16],[64,32],[64,32,16],[32,16,8],[16,8,4],[8,4,2]]\n",
        "activations = ['sigmoid', 'relu', 'softmax']\n",
        "optimizer = ['SGD', 'Adam']\n",
        "batch = [8, 16, 32, 64, 128, 256, 512, 1024]\n",
        "epoch = [10, 50, 100, 200, 500, 1000]\n",
        "\n",
        "para_grid = dict(layers=layers, activation=activations, optimizer=optimizer, batch_size=batch, epochs=epoch)\n",
        "\n",
        "grid = GridSearchCV(estimator = mdl, param_grid = para_grid)\n",
        "\n",
        "print(\"Training acc:\", grid.fit(train_x,train_y).best_score_, grid.fit(train_x,train_y).best_params_)\n",
        "\n"
      ],
      "metadata": {
        "id": "CxyWq4A8geAD"
      },
      "id": "CxyWq4A8geAD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above Grid Search code basically explore all hyperparameters and it's considerably time consuming.\n",
        "\n",
        "Hence, we decided to decomposite it intto small steps.\n",
        "\n",
        "**1. Finding numbers of hidden layers and neurons**\n",
        "\n",
        "We first assume that \n",
        "\n",
        "| Hyperparameters    | Value    |\n",
        "| :------------- | :------------- |\n",
        "| activations| 'relu' |\n",
        "| optimizer| 'Adam' |\n",
        "| batch| 1024 |\n",
        "| epoch| 200 |\n",
        "| loss| 'binary_crossentropy' |\n",
        "| scaling or not | No |\n",
        "| normalize or not | No |\n",
        "| learning rate| default |\n",
        "| momentum| default |\n",
        "| num of output class| 1 |\n"
      ],
      "metadata": {
        "id": "4OZyMvmK9kyY"
      },
      "id": "4OZyMvmK9kyY"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare training and testing data\n",
        "\n",
        "train_x, test_x, train_y, test_y = data_split(cdc_df)\n",
        "\n",
        "train_x = np.float32(train_x)\n",
        "test_x = np.float32(test_x)\n",
        "\n",
        "def gridNN(layers):\n",
        "  mdl = Sequential()\n",
        "  for i, neuron in enumerate(layers):\n",
        "    mdl.add(Dense(neuron, activation = 'relu'))\n",
        "  mdl.add(Dense(1, activation='sigmoid'))\n",
        "  mdl.compile(loss='binary_crossentropy', optimizer = 'Adam', metrics=['accuracy'])\n",
        "  return mdl\n",
        "\n",
        "\n",
        "mdl = KerasClassifier(build_fn = gridNN)\n",
        "\n",
        "# Define Hyperparameters\n",
        "\n",
        "layers = [[2],[4],[6],[8],[16],[32],[64],[4,2],[8,4],[16,8],[32,16],[64,32],[64,32,16],[32,16,8],[16,8,4],[8,4,2]]\n",
        "\n",
        "para_grid = dict(layers=layers, batch_size=[1024], epochs=[200])\n",
        "\n",
        "grid = GridSearchCV(estimator = mdl, param_grid = para_grid, cv =2)\n",
        "\n",
        "print(\"Training acc:\", grid.fit(train_x,train_y).best_score_, grid.fit(train_x,train_y).best_params_)\n"
      ],
      "metadata": {
        "id": "2fl9pHyRUcyA"
      },
      "id": "2fl9pHyRUcyA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "at the end we get best hidden layers are `[16, 8]`\n",
        "\n",
        "**2. Finding batch size and epochs**\n",
        "\n",
        "We first assume that \n",
        "\n",
        "| Hyperparameters    | Value    |\n",
        "| :------------- | :------------- |\n",
        "| activations| 'relu' |\n",
        "| optimizer| 'Adam' |\n",
        "| loss| 'binary_crossentropy' |\n",
        "| scaling or not | No |\n",
        "| normalize or not | No |\n",
        "| learning rate| default |\n",
        "| momentum| default |\n",
        "| num of output class| 1 |"
      ],
      "metadata": {
        "id": "4CarOyFe3OMF"
      },
      "id": "4CarOyFe3OMF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare training and testing data\n",
        "\n",
        "train_x, test_x, train_y, test_y = data_split(cdc_df)\n",
        "\n",
        "train_x = np.float32(train_x)\n",
        "test_x = np.float32(test_x)\n",
        "\n",
        "def gridNN():\n",
        "  mdl = Sequential()\n",
        "  mdl.add(Dense(16, activation = 'relu'))\n",
        "  mdl.add(Dense(8, activation = 'relu'))\n",
        "  mdl.add(Dense(1, activation='sigmoid'))\n",
        "  mdl.compile(loss='binary_crossentropy', optimizer = 'Adam', metrics=['accuracy'])\n",
        "  return mdl\n",
        "\n",
        "\n",
        "mdl = KerasClassifier(build_fn = gridNN)\n",
        "\n",
        "# Define Hyperparameters\n",
        "\n",
        "batch = [8, 16, 32, 64, 128, 256, 512, 1024]\n",
        "epoch = [10, 50, 100, 200, 500, 1000]\n",
        "\n",
        "para_grid = dict(batch_size=batch, epochs=epoch)\n",
        "\n",
        "grid = GridSearchCV(estimator = mdl, param_grid = para_grid, cv=2)\n",
        "\n",
        "print(\"Training acc:\", grid.fit(train_x,train_y).best_score_, grid.fit(train_x,train_y).best_params_)"
      ],
      "metadata": {
        "id": "VaYwNwe03aoB"
      },
      "id": "VaYwNwe03aoB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "at the end we get best epochs are `1000` and batch size is `512`\n",
        "\n",
        "**3. Finding best activation, optimizer and loss function**\n",
        "\n",
        "We first assume that \n",
        "\n",
        "| Hyperparameters    | Value    |\n",
        "| :------------- | :------------- |\n",
        "| scaling or not | No |\n",
        "| normalize or not | No |\n",
        "| learning rate| default |\n",
        "| momentum| default |\n",
        "| num of output class| 1 |"
      ],
      "metadata": {
        "id": "pqnH_HAJN3s0"
      },
      "id": "pqnH_HAJN3s0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare training and testing data\n",
        "\n",
        "train_x, test_x, train_y, test_y = data_split(cdc_df)\n",
        "\n",
        "train_x = np.float32(train_x)\n",
        "test_x = np.float32(test_x)\n",
        "\n",
        "# ------- Normalize the predictors or not ----------\n",
        "# train_x = train_x/train_x.max()\n",
        "# test_x = test_x/test_x.max()\n",
        "# --------------------------------------------------------\n",
        "\n",
        "def gridNN(activation, optimizer, loss):\n",
        "  mdl = Sequential()\n",
        "  mdl.add(Dense(16, activation = activation))\n",
        "  mdl.add(Dense(8, activation = activation))\n",
        "  mdl.add(Dense(1, activation=activation))\n",
        "  mdl.compile(loss=loss, optimizer = optimizer, metrics=['accuracy'])\n",
        "  return mdl\n",
        "\n",
        "\n",
        "mdl = KerasClassifier(build_fn = gridNN)\n",
        "\n",
        "# Define Hyperparameters\n",
        "\n",
        "activations = ['sigmoid', 'relu', 'softmax']\n",
        "optimizer = ['SGD', 'Adam']\n",
        "loss = ['binary_crossentropy']\n",
        "\n",
        "para_grid = dict(activation=activations, optimizer=optimizer, batch_size=[512], epochs=[1000], loss = loss)\n",
        "\n",
        "grid = GridSearchCV(estimator = mdl, param_grid = para_grid, cv=2)\n",
        "\n",
        "print(\"Training acc:\", grid.fit(train_x,train_y).best_score_, grid.fit(train_x,train_y).best_params_)"
      ],
      "metadata": {
        "id": "2whXPnhOfTVG"
      },
      "id": "2whXPnhOfTVG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "at the end we found the best activation is `sigmoid`, optimizer is `Adam` and lossfunction is `'binary_crossentropy'`.\n",
        "\n",
        "**4. Determine other hyperparameters**\n",
        "\n",
        "We have the respective outcome below:\n",
        "\n",
        "| Hyperparameters    | Value    | Training acc    | Testing acc    |\n",
        "| :------------- | :------------- | :------------- | :------------- |\n",
        "| No operations | -- |0.7558|0.7517|\n",
        "| scaling or not | Yes |0.7553|0.7540|\n",
        "| normalize or not | Yes |0.7463|0.7518|\n",
        "| learning rate (0.0001,0.0003,0.001,0.003,0.01,0.03)| 0.0001 |0.7530|0.7539|\n",
        "| momentum| 0.9 |0.6367|0.6499|\n",
        "| num of output class|2 |0.7562|0.7529|\n",
        "\n",
        "With these hyperparameters, we can finally build our BPNN.\n",
        "\n",
        "![bpnn](/img/bpnn.png)\n",
        "\n",
        "\n",
        "\n",
        "#### 1.3.2 BPNN via Keras\n",
        "\n",
        "Please note that NNs defined via Keras can automatically fit the shape of input. Hence, we only need to define the hidden layers and output layers. \n",
        "\n",
        "Besides, the outcome might be different with or without adding an activation on the output layer.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Uc8fmD0_fxNM"
      },
      "id": "Uc8fmD0_fxNM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare training and testing data\n",
        "\n",
        "train_x, test_x, train_y, test_y = data_split(cdc_df)\n",
        "\n",
        "train_x_scaled, test_x_scaled = data_scaling(train_x, test_x)\n",
        "\n",
        "train_x = np.float32(train_x_scaled)\n",
        "test_x = np.float32(test_x_scaled)\n",
        "\n",
        "\n",
        "#train_x = train_x/train_x.max()\n",
        "#test_x = test_x/test_x.max()\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(16, activation='sigmoid'),\n",
        "    keras.layers.Dense(8, activation='sigmoid'),\n",
        "    keras.layers.Dense(2, activation='sigmoid')\n",
        "])\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "model.compile(optimizer=opt, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_x, train_y, epochs=1000, batch_size=512, validation_data=(test_x, test_y))\n",
        "\n",
        "_, accuracy = model.evaluate(train_x, train_y)\n",
        "print('Training Accuracy:', accuracy)\n",
        "# evaluate the keras model\n",
        "_, accuracy = model.evaluate(test_x, test_y)\n",
        "print('Testing Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF5nNN0HtB2H",
        "outputId": "07d43939-785e-46d2-b8f4-53df2ef84f86"
      },
      "id": "IF5nNN0HtB2H",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "111/111 [==============================] - 1s 4ms/step - loss: 0.6993 - accuracy: 0.4906 - val_loss: 0.6934 - val_accuracy: 0.5119\n",
            "Epoch 2/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.6900 - accuracy: 0.5415 - val_loss: 0.6848 - val_accuracy: 0.5896\n",
            "Epoch 3/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.6817 - accuracy: 0.6050 - val_loss: 0.6763 - val_accuracy: 0.6380\n",
            "Epoch 4/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.6734 - accuracy: 0.6437 - val_loss: 0.6677 - val_accuracy: 0.6631\n",
            "Epoch 5/1000\n",
            "111/111 [==============================] - 1s 5ms/step - loss: 0.6648 - accuracy: 0.6626 - val_loss: 0.6587 - val_accuracy: 0.6767\n",
            "Epoch 6/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.6559 - accuracy: 0.6749 - val_loss: 0.6494 - val_accuracy: 0.6871\n",
            "Epoch 7/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.6467 - accuracy: 0.6846 - val_loss: 0.6397 - val_accuracy: 0.6951\n",
            "Epoch 8/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.6373 - accuracy: 0.6899 - val_loss: 0.6299 - val_accuracy: 0.7021\n",
            "Epoch 9/1000\n",
            "111/111 [==============================] - 1s 7ms/step - loss: 0.6277 - accuracy: 0.6960 - val_loss: 0.6199 - val_accuracy: 0.7046\n",
            "Epoch 10/1000\n",
            "111/111 [==============================] - 1s 5ms/step - loss: 0.6181 - accuracy: 0.6992 - val_loss: 0.6101 - val_accuracy: 0.7086\n",
            "Epoch 11/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.6087 - accuracy: 0.7037 - val_loss: 0.6006 - val_accuracy: 0.7134\n",
            "Epoch 12/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5997 - accuracy: 0.7070 - val_loss: 0.5915 - val_accuracy: 0.7154\n",
            "Epoch 13/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.5911 - accuracy: 0.7112 - val_loss: 0.5830 - val_accuracy: 0.7195\n",
            "Epoch 14/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5831 - accuracy: 0.7143 - val_loss: 0.5750 - val_accuracy: 0.7231\n",
            "Epoch 15/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5757 - accuracy: 0.7174 - val_loss: 0.5678 - val_accuracy: 0.7252\n",
            "Epoch 16/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5690 - accuracy: 0.7202 - val_loss: 0.5612 - val_accuracy: 0.7284\n",
            "Epoch 17/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5628 - accuracy: 0.7235 - val_loss: 0.5552 - val_accuracy: 0.7305\n",
            "Epoch 18/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5573 - accuracy: 0.7256 - val_loss: 0.5499 - val_accuracy: 0.7336\n",
            "Epoch 19/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5523 - accuracy: 0.7286 - val_loss: 0.5451 - val_accuracy: 0.7348\n",
            "Epoch 20/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5478 - accuracy: 0.7306 - val_loss: 0.5409 - val_accuracy: 0.7365\n",
            "Epoch 21/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5438 - accuracy: 0.7323 - val_loss: 0.5371 - val_accuracy: 0.7382\n",
            "Epoch 22/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5402 - accuracy: 0.7344 - val_loss: 0.5337 - val_accuracy: 0.7394\n",
            "Epoch 23/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5370 - accuracy: 0.7362 - val_loss: 0.5308 - val_accuracy: 0.7414\n",
            "Epoch 24/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7369 - val_loss: 0.5283 - val_accuracy: 0.7418\n",
            "Epoch 25/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5317 - accuracy: 0.7388 - val_loss: 0.5260 - val_accuracy: 0.7426\n",
            "Epoch 26/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5296 - accuracy: 0.7396 - val_loss: 0.5241 - val_accuracy: 0.7438\n",
            "Epoch 27/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5277 - accuracy: 0.7404 - val_loss: 0.5225 - val_accuracy: 0.7454\n",
            "Epoch 28/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7418 - val_loss: 0.5211 - val_accuracy: 0.7455\n",
            "Epoch 29/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5246 - accuracy: 0.7419 - val_loss: 0.5198 - val_accuracy: 0.7464\n",
            "Epoch 30/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5234 - accuracy: 0.7422 - val_loss: 0.5188 - val_accuracy: 0.7475\n",
            "Epoch 31/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5223 - accuracy: 0.7427 - val_loss: 0.5179 - val_accuracy: 0.7462\n",
            "Epoch 32/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5213 - accuracy: 0.7428 - val_loss: 0.5171 - val_accuracy: 0.7462\n",
            "Epoch 33/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5205 - accuracy: 0.7434 - val_loss: 0.5164 - val_accuracy: 0.7467\n",
            "Epoch 34/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5198 - accuracy: 0.7434 - val_loss: 0.5159 - val_accuracy: 0.7476\n",
            "Epoch 35/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5191 - accuracy: 0.7434 - val_loss: 0.5153 - val_accuracy: 0.7481\n",
            "Epoch 36/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5185 - accuracy: 0.7438 - val_loss: 0.5149 - val_accuracy: 0.7487\n",
            "Epoch 37/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5180 - accuracy: 0.7440 - val_loss: 0.5145 - val_accuracy: 0.7490\n",
            "Epoch 38/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5176 - accuracy: 0.7441 - val_loss: 0.5142 - val_accuracy: 0.7484\n",
            "Epoch 39/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5172 - accuracy: 0.7439 - val_loss: 0.5138 - val_accuracy: 0.7491\n",
            "Epoch 40/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5168 - accuracy: 0.7443 - val_loss: 0.5136 - val_accuracy: 0.7491\n",
            "Epoch 41/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5165 - accuracy: 0.7446 - val_loss: 0.5133 - val_accuracy: 0.7495\n",
            "Epoch 42/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5162 - accuracy: 0.7452 - val_loss: 0.5131 - val_accuracy: 0.7487\n",
            "Epoch 43/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5160 - accuracy: 0.7453 - val_loss: 0.5129 - val_accuracy: 0.7492\n",
            "Epoch 44/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5157 - accuracy: 0.7456 - val_loss: 0.5127 - val_accuracy: 0.7491\n",
            "Epoch 45/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5155 - accuracy: 0.7456 - val_loss: 0.5126 - val_accuracy: 0.7493\n",
            "Epoch 46/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5153 - accuracy: 0.7462 - val_loss: 0.5124 - val_accuracy: 0.7490\n",
            "Epoch 47/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5152 - accuracy: 0.7461 - val_loss: 0.5123 - val_accuracy: 0.7496\n",
            "Epoch 48/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5150 - accuracy: 0.7458 - val_loss: 0.5122 - val_accuracy: 0.7501\n",
            "Epoch 49/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5149 - accuracy: 0.7462 - val_loss: 0.5120 - val_accuracy: 0.7501\n",
            "Epoch 50/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5147 - accuracy: 0.7459 - val_loss: 0.5119 - val_accuracy: 0.7502\n",
            "Epoch 51/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5146 - accuracy: 0.7464 - val_loss: 0.5118 - val_accuracy: 0.7505\n",
            "Epoch 52/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5145 - accuracy: 0.7464 - val_loss: 0.5117 - val_accuracy: 0.7501\n",
            "Epoch 53/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5144 - accuracy: 0.7464 - val_loss: 0.5116 - val_accuracy: 0.7504\n",
            "Epoch 54/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5143 - accuracy: 0.7466 - val_loss: 0.5116 - val_accuracy: 0.7506\n",
            "Epoch 55/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5142 - accuracy: 0.7463 - val_loss: 0.5115 - val_accuracy: 0.7505\n",
            "Epoch 56/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5141 - accuracy: 0.7467 - val_loss: 0.5114 - val_accuracy: 0.7507\n",
            "Epoch 57/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5140 - accuracy: 0.7464 - val_loss: 0.5113 - val_accuracy: 0.7510\n",
            "Epoch 58/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5139 - accuracy: 0.7463 - val_loss: 0.5113 - val_accuracy: 0.7509\n",
            "Epoch 59/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5138 - accuracy: 0.7466 - val_loss: 0.5112 - val_accuracy: 0.7512\n",
            "Epoch 60/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5138 - accuracy: 0.7465 - val_loss: 0.5111 - val_accuracy: 0.7510\n",
            "Epoch 61/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5137 - accuracy: 0.7463 - val_loss: 0.5111 - val_accuracy: 0.7512\n",
            "Epoch 62/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5136 - accuracy: 0.7462 - val_loss: 0.5110 - val_accuracy: 0.7508\n",
            "Epoch 63/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5135 - accuracy: 0.7461 - val_loss: 0.5109 - val_accuracy: 0.7510\n",
            "Epoch 64/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5135 - accuracy: 0.7463 - val_loss: 0.5109 - val_accuracy: 0.7512\n",
            "Epoch 65/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7464 - val_loss: 0.5108 - val_accuracy: 0.7513\n",
            "Epoch 66/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7460 - val_loss: 0.5108 - val_accuracy: 0.7510\n",
            "Epoch 67/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5133 - accuracy: 0.7464 - val_loss: 0.5107 - val_accuracy: 0.7511\n",
            "Epoch 68/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5132 - accuracy: 0.7463 - val_loss: 0.5106 - val_accuracy: 0.7513\n",
            "Epoch 69/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5132 - accuracy: 0.7462 - val_loss: 0.5106 - val_accuracy: 0.7514\n",
            "Epoch 70/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5131 - accuracy: 0.7465 - val_loss: 0.5105 - val_accuracy: 0.7514\n",
            "Epoch 71/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5131 - accuracy: 0.7465 - val_loss: 0.5105 - val_accuracy: 0.7516\n",
            "Epoch 72/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.7462 - val_loss: 0.5104 - val_accuracy: 0.7515\n",
            "Epoch 73/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.7463 - val_loss: 0.5104 - val_accuracy: 0.7515\n",
            "Epoch 74/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5129 - accuracy: 0.7465 - val_loss: 0.5103 - val_accuracy: 0.7519\n",
            "Epoch 75/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5129 - accuracy: 0.7463 - val_loss: 0.5103 - val_accuracy: 0.7510\n",
            "Epoch 76/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5128 - accuracy: 0.7461 - val_loss: 0.5102 - val_accuracy: 0.7513\n",
            "Epoch 77/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5128 - accuracy: 0.7464 - val_loss: 0.5102 - val_accuracy: 0.7515\n",
            "Epoch 78/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5127 - accuracy: 0.7462 - val_loss: 0.5101 - val_accuracy: 0.7510\n",
            "Epoch 79/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5127 - accuracy: 0.7465 - val_loss: 0.5101 - val_accuracy: 0.7512\n",
            "Epoch 80/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7464 - val_loss: 0.5101 - val_accuracy: 0.7512\n",
            "Epoch 81/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7463 - val_loss: 0.5100 - val_accuracy: 0.7511\n",
            "Epoch 82/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5125 - accuracy: 0.7464 - val_loss: 0.5099 - val_accuracy: 0.7509\n",
            "Epoch 83/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5125 - accuracy: 0.7465 - val_loss: 0.5099 - val_accuracy: 0.7510\n",
            "Epoch 84/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5124 - accuracy: 0.7464 - val_loss: 0.5099 - val_accuracy: 0.7510\n",
            "Epoch 85/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5124 - accuracy: 0.7467 - val_loss: 0.5098 - val_accuracy: 0.7510\n",
            "Epoch 86/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5123 - accuracy: 0.7466 - val_loss: 0.5098 - val_accuracy: 0.7503\n",
            "Epoch 87/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5123 - accuracy: 0.7466 - val_loss: 0.5097 - val_accuracy: 0.7506\n",
            "Epoch 88/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5123 - accuracy: 0.7464 - val_loss: 0.5097 - val_accuracy: 0.7503\n",
            "Epoch 89/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.7465 - val_loss: 0.5097 - val_accuracy: 0.7504\n",
            "Epoch 90/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.7464 - val_loss: 0.5096 - val_accuracy: 0.7510\n",
            "Epoch 91/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7466 - val_loss: 0.5096 - val_accuracy: 0.7510\n",
            "Epoch 92/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7466 - val_loss: 0.5095 - val_accuracy: 0.7506\n",
            "Epoch 93/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5120 - accuracy: 0.7466 - val_loss: 0.5095 - val_accuracy: 0.7505\n",
            "Epoch 94/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5120 - accuracy: 0.7465 - val_loss: 0.5095 - val_accuracy: 0.7509\n",
            "Epoch 95/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5120 - accuracy: 0.7468 - val_loss: 0.5094 - val_accuracy: 0.7508\n",
            "Epoch 96/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7465 - val_loss: 0.5094 - val_accuracy: 0.7507\n",
            "Epoch 97/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7466 - val_loss: 0.5093 - val_accuracy: 0.7505\n",
            "Epoch 98/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7466 - val_loss: 0.5093 - val_accuracy: 0.7509\n",
            "Epoch 99/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.7463 - val_loss: 0.5093 - val_accuracy: 0.7507\n",
            "Epoch 100/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.7466 - val_loss: 0.5092 - val_accuracy: 0.7506\n",
            "Epoch 101/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5117 - accuracy: 0.7467 - val_loss: 0.5092 - val_accuracy: 0.7508\n",
            "Epoch 102/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5117 - accuracy: 0.7467 - val_loss: 0.5091 - val_accuracy: 0.7508\n",
            "Epoch 103/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5117 - accuracy: 0.7465 - val_loss: 0.5091 - val_accuracy: 0.7508\n",
            "Epoch 104/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5116 - accuracy: 0.7468 - val_loss: 0.5091 - val_accuracy: 0.7508\n",
            "Epoch 105/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5116 - accuracy: 0.7467 - val_loss: 0.5091 - val_accuracy: 0.7503\n",
            "Epoch 106/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7467 - val_loss: 0.5090 - val_accuracy: 0.7508\n",
            "Epoch 107/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7467 - val_loss: 0.5090 - val_accuracy: 0.7503\n",
            "Epoch 108/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7465 - val_loss: 0.5089 - val_accuracy: 0.7505\n",
            "Epoch 109/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7465 - val_loss: 0.5089 - val_accuracy: 0.7503\n",
            "Epoch 110/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7466 - val_loss: 0.5089 - val_accuracy: 0.7507\n",
            "Epoch 111/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7465 - val_loss: 0.5088 - val_accuracy: 0.7508\n",
            "Epoch 112/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5113 - accuracy: 0.7466 - val_loss: 0.5088 - val_accuracy: 0.7509\n",
            "Epoch 113/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5113 - accuracy: 0.7466 - val_loss: 0.5088 - val_accuracy: 0.7508\n",
            "Epoch 114/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5113 - accuracy: 0.7467 - val_loss: 0.5087 - val_accuracy: 0.7507\n",
            "Epoch 115/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7467 - val_loss: 0.5087 - val_accuracy: 0.7507\n",
            "Epoch 116/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7465 - val_loss: 0.5087 - val_accuracy: 0.7508\n",
            "Epoch 117/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7467 - val_loss: 0.5086 - val_accuracy: 0.7508\n",
            "Epoch 118/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7467 - val_loss: 0.5086 - val_accuracy: 0.7507\n",
            "Epoch 119/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7466 - val_loss: 0.5086 - val_accuracy: 0.7515\n",
            "Epoch 120/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7468 - val_loss: 0.5085 - val_accuracy: 0.7514\n",
            "Epoch 121/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.7465 - val_loss: 0.5085 - val_accuracy: 0.7510\n",
            "Epoch 122/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.7466 - val_loss: 0.5085 - val_accuracy: 0.7513\n",
            "Epoch 123/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.7467 - val_loss: 0.5084 - val_accuracy: 0.7510\n",
            "Epoch 124/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7467 - val_loss: 0.5084 - val_accuracy: 0.7510\n",
            "Epoch 125/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7468 - val_loss: 0.5084 - val_accuracy: 0.7512\n",
            "Epoch 126/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7467 - val_loss: 0.5084 - val_accuracy: 0.7514\n",
            "Epoch 127/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7467 - val_loss: 0.5083 - val_accuracy: 0.7514\n",
            "Epoch 128/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7469 - val_loss: 0.5083 - val_accuracy: 0.7515\n",
            "Epoch 129/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5107 - accuracy: 0.7471 - val_loss: 0.5082 - val_accuracy: 0.7508\n",
            "Epoch 130/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5107 - accuracy: 0.7468 - val_loss: 0.5082 - val_accuracy: 0.7509\n",
            "Epoch 131/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5107 - accuracy: 0.7468 - val_loss: 0.5082 - val_accuracy: 0.7513\n",
            "Epoch 132/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7468 - val_loss: 0.5081 - val_accuracy: 0.7512\n",
            "Epoch 133/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5106 - accuracy: 0.7468 - val_loss: 0.5081 - val_accuracy: 0.7514\n",
            "Epoch 134/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7468 - val_loss: 0.5081 - val_accuracy: 0.7515\n",
            "Epoch 135/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7472 - val_loss: 0.5080 - val_accuracy: 0.7516\n",
            "Epoch 136/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7469 - val_loss: 0.5080 - val_accuracy: 0.7513\n",
            "Epoch 137/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5105 - accuracy: 0.7471 - val_loss: 0.5080 - val_accuracy: 0.7511\n",
            "Epoch 138/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7471 - val_loss: 0.5080 - val_accuracy: 0.7509\n",
            "Epoch 139/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7473 - val_loss: 0.5079 - val_accuracy: 0.7510\n",
            "Epoch 140/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7472 - val_loss: 0.5079 - val_accuracy: 0.7513\n",
            "Epoch 141/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7470 - val_loss: 0.5079 - val_accuracy: 0.7511\n",
            "Epoch 142/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7470 - val_loss: 0.5078 - val_accuracy: 0.7513\n",
            "Epoch 143/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7471 - val_loss: 0.5078 - val_accuracy: 0.7513\n",
            "Epoch 144/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7469 - val_loss: 0.5078 - val_accuracy: 0.7512\n",
            "Epoch 145/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7470 - val_loss: 0.5077 - val_accuracy: 0.7510\n",
            "Epoch 146/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7470 - val_loss: 0.5077 - val_accuracy: 0.7513\n",
            "Epoch 147/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7470 - val_loss: 0.5077 - val_accuracy: 0.7511\n",
            "Epoch 148/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7470 - val_loss: 0.5076 - val_accuracy: 0.7513\n",
            "Epoch 149/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7469 - val_loss: 0.5076 - val_accuracy: 0.7513\n",
            "Epoch 150/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7471 - val_loss: 0.5076 - val_accuracy: 0.7511\n",
            "Epoch 151/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7470 - val_loss: 0.5076 - val_accuracy: 0.7510\n",
            "Epoch 152/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7471 - val_loss: 0.5075 - val_accuracy: 0.7513\n",
            "Epoch 153/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7470 - val_loss: 0.5075 - val_accuracy: 0.7511\n",
            "Epoch 154/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7468 - val_loss: 0.5075 - val_accuracy: 0.7515\n",
            "Epoch 155/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7471 - val_loss: 0.5074 - val_accuracy: 0.7512\n",
            "Epoch 156/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7470 - val_loss: 0.5074 - val_accuracy: 0.7511\n",
            "Epoch 157/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7470 - val_loss: 0.5074 - val_accuracy: 0.7515\n",
            "Epoch 158/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7471 - val_loss: 0.5073 - val_accuracy: 0.7513\n",
            "Epoch 159/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7469 - val_loss: 0.5073 - val_accuracy: 0.7513\n",
            "Epoch 160/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7471 - val_loss: 0.5073 - val_accuracy: 0.7516\n",
            "Epoch 161/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7469 - val_loss: 0.5072 - val_accuracy: 0.7515\n",
            "Epoch 162/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7469 - val_loss: 0.5072 - val_accuracy: 0.7515\n",
            "Epoch 163/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7470 - val_loss: 0.5072 - val_accuracy: 0.7515\n",
            "Epoch 164/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7471 - val_loss: 0.5072 - val_accuracy: 0.7515\n",
            "Epoch 165/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7472 - val_loss: 0.5071 - val_accuracy: 0.7515\n",
            "Epoch 166/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7471 - val_loss: 0.5071 - val_accuracy: 0.7512\n",
            "Epoch 167/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7471 - val_loss: 0.5071 - val_accuracy: 0.7513\n",
            "Epoch 168/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7471 - val_loss: 0.5071 - val_accuracy: 0.7518\n",
            "Epoch 169/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7474 - val_loss: 0.5070 - val_accuracy: 0.7517\n",
            "Epoch 170/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7472 - val_loss: 0.5070 - val_accuracy: 0.7513\n",
            "Epoch 171/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7473 - val_loss: 0.5070 - val_accuracy: 0.7515\n",
            "Epoch 172/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7472 - val_loss: 0.5069 - val_accuracy: 0.7513\n",
            "Epoch 173/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7472 - val_loss: 0.5069 - val_accuracy: 0.7518\n",
            "Epoch 174/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7473 - val_loss: 0.5069 - val_accuracy: 0.7517\n",
            "Epoch 175/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7475 - val_loss: 0.5068 - val_accuracy: 0.7520\n",
            "Epoch 176/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7473 - val_loss: 0.5068 - val_accuracy: 0.7515\n",
            "Epoch 177/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7474 - val_loss: 0.5068 - val_accuracy: 0.7516\n",
            "Epoch 178/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7474 - val_loss: 0.5067 - val_accuracy: 0.7520\n",
            "Epoch 179/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7474 - val_loss: 0.5067 - val_accuracy: 0.7518\n",
            "Epoch 180/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7471 - val_loss: 0.5067 - val_accuracy: 0.7514\n",
            "Epoch 181/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7476 - val_loss: 0.5067 - val_accuracy: 0.7518\n",
            "Epoch 182/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7475 - val_loss: 0.5066 - val_accuracy: 0.7515\n",
            "Epoch 183/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7475 - val_loss: 0.5066 - val_accuracy: 0.7520\n",
            "Epoch 184/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7475 - val_loss: 0.5066 - val_accuracy: 0.7518\n",
            "Epoch 185/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7474 - val_loss: 0.5065 - val_accuracy: 0.7520\n",
            "Epoch 186/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7475 - val_loss: 0.5065 - val_accuracy: 0.7523\n",
            "Epoch 187/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7475 - val_loss: 0.5065 - val_accuracy: 0.7519\n",
            "Epoch 188/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5088 - accuracy: 0.7476 - val_loss: 0.5065 - val_accuracy: 0.7525\n",
            "Epoch 189/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7474 - val_loss: 0.5064 - val_accuracy: 0.7524\n",
            "Epoch 190/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7476 - val_loss: 0.5064 - val_accuracy: 0.7522\n",
            "Epoch 191/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7477 - val_loss: 0.5064 - val_accuracy: 0.7525\n",
            "Epoch 192/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7475 - val_loss: 0.5063 - val_accuracy: 0.7522\n",
            "Epoch 193/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7476 - val_loss: 0.5063 - val_accuracy: 0.7525\n",
            "Epoch 194/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7476 - val_loss: 0.5063 - val_accuracy: 0.7525\n",
            "Epoch 195/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7477 - val_loss: 0.5063 - val_accuracy: 0.7525\n",
            "Epoch 196/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7477 - val_loss: 0.5062 - val_accuracy: 0.7525\n",
            "Epoch 197/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7475 - val_loss: 0.5062 - val_accuracy: 0.7522\n",
            "Epoch 198/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7476 - val_loss: 0.5062 - val_accuracy: 0.7524\n",
            "Epoch 199/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7476 - val_loss: 0.5062 - val_accuracy: 0.7518\n",
            "Epoch 200/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7478 - val_loss: 0.5061 - val_accuracy: 0.7525\n",
            "Epoch 201/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5084 - accuracy: 0.7477 - val_loss: 0.5061 - val_accuracy: 0.7523\n",
            "Epoch 202/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7476 - val_loss: 0.5061 - val_accuracy: 0.7525\n",
            "Epoch 203/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7476 - val_loss: 0.5060 - val_accuracy: 0.7524\n",
            "Epoch 204/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7483 - val_loss: 0.5060 - val_accuracy: 0.7523\n",
            "Epoch 205/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7478 - val_loss: 0.5060 - val_accuracy: 0.7520\n",
            "Epoch 206/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5082 - accuracy: 0.7479 - val_loss: 0.5060 - val_accuracy: 0.7524\n",
            "Epoch 207/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5082 - accuracy: 0.7477 - val_loss: 0.5059 - val_accuracy: 0.7521\n",
            "Epoch 208/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5082 - accuracy: 0.7477 - val_loss: 0.5059 - val_accuracy: 0.7520\n",
            "Epoch 209/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5081 - accuracy: 0.7479 - val_loss: 0.5059 - val_accuracy: 0.7520\n",
            "Epoch 210/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.5081 - accuracy: 0.7477 - val_loss: 0.5059 - val_accuracy: 0.7527\n",
            "Epoch 211/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5081 - accuracy: 0.7481 - val_loss: 0.5058 - val_accuracy: 0.7521\n",
            "Epoch 212/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5080 - accuracy: 0.7481 - val_loss: 0.5058 - val_accuracy: 0.7519\n",
            "Epoch 213/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5080 - accuracy: 0.7479 - val_loss: 0.5058 - val_accuracy: 0.7520\n",
            "Epoch 214/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.5080 - accuracy: 0.7481 - val_loss: 0.5057 - val_accuracy: 0.7521\n",
            "Epoch 215/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5079 - accuracy: 0.7479 - val_loss: 0.5057 - val_accuracy: 0.7518\n",
            "Epoch 216/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5079 - accuracy: 0.7481 - val_loss: 0.5057 - val_accuracy: 0.7520\n",
            "Epoch 217/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5079 - accuracy: 0.7481 - val_loss: 0.5057 - val_accuracy: 0.7520\n",
            "Epoch 218/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5078 - accuracy: 0.7479 - val_loss: 0.5056 - val_accuracy: 0.7523\n",
            "Epoch 219/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5078 - accuracy: 0.7484 - val_loss: 0.5056 - val_accuracy: 0.7519\n",
            "Epoch 220/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5078 - accuracy: 0.7481 - val_loss: 0.5056 - val_accuracy: 0.7518\n",
            "Epoch 221/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5078 - accuracy: 0.7482 - val_loss: 0.5056 - val_accuracy: 0.7521\n",
            "Epoch 222/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5077 - accuracy: 0.7482 - val_loss: 0.5056 - val_accuracy: 0.7520\n",
            "Epoch 223/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5077 - accuracy: 0.7481 - val_loss: 0.5055 - val_accuracy: 0.7520\n",
            "Epoch 224/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5077 - accuracy: 0.7481 - val_loss: 0.5055 - val_accuracy: 0.7518\n",
            "Epoch 225/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5076 - accuracy: 0.7483 - val_loss: 0.5055 - val_accuracy: 0.7522\n",
            "Epoch 226/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5076 - accuracy: 0.7484 - val_loss: 0.5055 - val_accuracy: 0.7524\n",
            "Epoch 227/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5076 - accuracy: 0.7484 - val_loss: 0.5054 - val_accuracy: 0.7520\n",
            "Epoch 228/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5076 - accuracy: 0.7486 - val_loss: 0.5054 - val_accuracy: 0.7525\n",
            "Epoch 229/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5075 - accuracy: 0.7484 - val_loss: 0.5054 - val_accuracy: 0.7520\n",
            "Epoch 230/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5075 - accuracy: 0.7485 - val_loss: 0.5053 - val_accuracy: 0.7523\n",
            "Epoch 231/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5075 - accuracy: 0.7486 - val_loss: 0.5053 - val_accuracy: 0.7525\n",
            "Epoch 232/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5074 - accuracy: 0.7486 - val_loss: 0.5053 - val_accuracy: 0.7523\n",
            "Epoch 233/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5074 - accuracy: 0.7486 - val_loss: 0.5053 - val_accuracy: 0.7524\n",
            "Epoch 234/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5074 - accuracy: 0.7484 - val_loss: 0.5052 - val_accuracy: 0.7528\n",
            "Epoch 235/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5073 - accuracy: 0.7488 - val_loss: 0.5052 - val_accuracy: 0.7527\n",
            "Epoch 236/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.7486 - val_loss: 0.5052 - val_accuracy: 0.7525\n",
            "Epoch 237/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.7485 - val_loss: 0.5052 - val_accuracy: 0.7525\n",
            "Epoch 238/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5073 - accuracy: 0.7486 - val_loss: 0.5052 - val_accuracy: 0.7526\n",
            "Epoch 239/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.7489 - val_loss: 0.5052 - val_accuracy: 0.7527\n",
            "Epoch 240/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5072 - accuracy: 0.7487 - val_loss: 0.5051 - val_accuracy: 0.7530\n",
            "Epoch 241/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5072 - accuracy: 0.7490 - val_loss: 0.5051 - val_accuracy: 0.7529\n",
            "Epoch 242/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5072 - accuracy: 0.7489 - val_loss: 0.5051 - val_accuracy: 0.7529\n",
            "Epoch 243/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5071 - accuracy: 0.7488 - val_loss: 0.5050 - val_accuracy: 0.7530\n",
            "Epoch 244/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5071 - accuracy: 0.7492 - val_loss: 0.5050 - val_accuracy: 0.7527\n",
            "Epoch 245/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5071 - accuracy: 0.7491 - val_loss: 0.5050 - val_accuracy: 0.7527\n",
            "Epoch 246/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5071 - accuracy: 0.7491 - val_loss: 0.5050 - val_accuracy: 0.7527\n",
            "Epoch 247/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5070 - accuracy: 0.7490 - val_loss: 0.5050 - val_accuracy: 0.7529\n",
            "Epoch 248/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5070 - accuracy: 0.7492 - val_loss: 0.5049 - val_accuracy: 0.7530\n",
            "Epoch 249/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5070 - accuracy: 0.7490 - val_loss: 0.5049 - val_accuracy: 0.7531\n",
            "Epoch 250/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5070 - accuracy: 0.7493 - val_loss: 0.5049 - val_accuracy: 0.7531\n",
            "Epoch 251/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5069 - accuracy: 0.7491 - val_loss: 0.5049 - val_accuracy: 0.7532\n",
            "Epoch 252/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5069 - accuracy: 0.7493 - val_loss: 0.5048 - val_accuracy: 0.7530\n",
            "Epoch 253/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5069 - accuracy: 0.7492 - val_loss: 0.5048 - val_accuracy: 0.7532\n",
            "Epoch 254/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5069 - accuracy: 0.7490 - val_loss: 0.5048 - val_accuracy: 0.7528\n",
            "Epoch 255/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5068 - accuracy: 0.7494 - val_loss: 0.5048 - val_accuracy: 0.7527\n",
            "Epoch 256/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5068 - accuracy: 0.7493 - val_loss: 0.5048 - val_accuracy: 0.7526\n",
            "Epoch 257/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5068 - accuracy: 0.7493 - val_loss: 0.5047 - val_accuracy: 0.7530\n",
            "Epoch 258/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5068 - accuracy: 0.7494 - val_loss: 0.5047 - val_accuracy: 0.7528\n",
            "Epoch 259/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5067 - accuracy: 0.7492 - val_loss: 0.5047 - val_accuracy: 0.7531\n",
            "Epoch 260/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5067 - accuracy: 0.7495 - val_loss: 0.5047 - val_accuracy: 0.7534\n",
            "Epoch 261/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5067 - accuracy: 0.7494 - val_loss: 0.5047 - val_accuracy: 0.7537\n",
            "Epoch 262/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5067 - accuracy: 0.7494 - val_loss: 0.5047 - val_accuracy: 0.7539\n",
            "Epoch 263/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5066 - accuracy: 0.7496 - val_loss: 0.5046 - val_accuracy: 0.7537\n",
            "Epoch 264/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5066 - accuracy: 0.7495 - val_loss: 0.5046 - val_accuracy: 0.7539\n",
            "Epoch 265/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5066 - accuracy: 0.7495 - val_loss: 0.5046 - val_accuracy: 0.7537\n",
            "Epoch 266/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5066 - accuracy: 0.7495 - val_loss: 0.5046 - val_accuracy: 0.7537\n",
            "Epoch 267/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7497 - val_loss: 0.5046 - val_accuracy: 0.7540\n",
            "Epoch 268/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5065 - accuracy: 0.7494 - val_loss: 0.5045 - val_accuracy: 0.7539\n",
            "Epoch 269/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7497 - val_loss: 0.5045 - val_accuracy: 0.7537\n",
            "Epoch 270/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7498 - val_loss: 0.5045 - val_accuracy: 0.7540\n",
            "Epoch 271/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7495 - val_loss: 0.5045 - val_accuracy: 0.7542\n",
            "Epoch 272/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5064 - accuracy: 0.7498 - val_loss: 0.5044 - val_accuracy: 0.7539\n",
            "Epoch 273/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5064 - accuracy: 0.7496 - val_loss: 0.5045 - val_accuracy: 0.7537\n",
            "Epoch 274/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5064 - accuracy: 0.7497 - val_loss: 0.5044 - val_accuracy: 0.7541\n",
            "Epoch 275/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7496 - val_loss: 0.5044 - val_accuracy: 0.7537\n",
            "Epoch 276/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5063 - accuracy: 0.7494 - val_loss: 0.5044 - val_accuracy: 0.7537\n",
            "Epoch 277/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5063 - accuracy: 0.7497 - val_loss: 0.5044 - val_accuracy: 0.7539\n",
            "Epoch 278/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7495 - val_loss: 0.5044 - val_accuracy: 0.7538\n",
            "Epoch 279/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7497 - val_loss: 0.5043 - val_accuracy: 0.7538\n",
            "Epoch 280/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5062 - accuracy: 0.7494 - val_loss: 0.5043 - val_accuracy: 0.7539\n",
            "Epoch 281/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5062 - accuracy: 0.7495 - val_loss: 0.5043 - val_accuracy: 0.7539\n",
            "Epoch 282/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5062 - accuracy: 0.7495 - val_loss: 0.5043 - val_accuracy: 0.7538\n",
            "Epoch 283/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5062 - accuracy: 0.7494 - val_loss: 0.5043 - val_accuracy: 0.7538\n",
            "Epoch 284/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5061 - accuracy: 0.7494 - val_loss: 0.5042 - val_accuracy: 0.7539\n",
            "Epoch 285/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5061 - accuracy: 0.7495 - val_loss: 0.5042 - val_accuracy: 0.7537\n",
            "Epoch 286/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5061 - accuracy: 0.7495 - val_loss: 0.5042 - val_accuracy: 0.7537\n",
            "Epoch 287/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5061 - accuracy: 0.7496 - val_loss: 0.5042 - val_accuracy: 0.7539\n",
            "Epoch 288/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5061 - accuracy: 0.7496 - val_loss: 0.5042 - val_accuracy: 0.7542\n",
            "Epoch 289/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5060 - accuracy: 0.7498 - val_loss: 0.5042 - val_accuracy: 0.7539\n",
            "Epoch 290/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5060 - accuracy: 0.7496 - val_loss: 0.5042 - val_accuracy: 0.7542\n",
            "Epoch 291/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5060 - accuracy: 0.7496 - val_loss: 0.5042 - val_accuracy: 0.7539\n",
            "Epoch 292/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5060 - accuracy: 0.7494 - val_loss: 0.5041 - val_accuracy: 0.7539\n",
            "Epoch 293/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5059 - accuracy: 0.7498 - val_loss: 0.5041 - val_accuracy: 0.7537\n",
            "Epoch 294/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5059 - accuracy: 0.7497 - val_loss: 0.5041 - val_accuracy: 0.7539\n",
            "Epoch 295/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5059 - accuracy: 0.7496 - val_loss: 0.5041 - val_accuracy: 0.7538\n",
            "Epoch 296/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5059 - accuracy: 0.7496 - val_loss: 0.5041 - val_accuracy: 0.7542\n",
            "Epoch 297/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5059 - accuracy: 0.7494 - val_loss: 0.5040 - val_accuracy: 0.7539\n",
            "Epoch 298/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5058 - accuracy: 0.7500 - val_loss: 0.5040 - val_accuracy: 0.7534\n",
            "Epoch 299/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5058 - accuracy: 0.7498 - val_loss: 0.5040 - val_accuracy: 0.7540\n",
            "Epoch 300/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5058 - accuracy: 0.7498 - val_loss: 0.5040 - val_accuracy: 0.7542\n",
            "Epoch 301/1000\n",
            "111/111 [==============================] - 1s 9ms/step - loss: 0.5058 - accuracy: 0.7494 - val_loss: 0.5040 - val_accuracy: 0.7535\n",
            "Epoch 302/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.5058 - accuracy: 0.7496 - val_loss: 0.5040 - val_accuracy: 0.7541\n",
            "Epoch 303/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5057 - accuracy: 0.7498 - val_loss: 0.5040 - val_accuracy: 0.7539\n",
            "Epoch 304/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5057 - accuracy: 0.7497 - val_loss: 0.5039 - val_accuracy: 0.7537\n",
            "Epoch 305/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5057 - accuracy: 0.7496 - val_loss: 0.5039 - val_accuracy: 0.7537\n",
            "Epoch 306/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5057 - accuracy: 0.7495 - val_loss: 0.5039 - val_accuracy: 0.7545\n",
            "Epoch 307/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5057 - accuracy: 0.7499 - val_loss: 0.5039 - val_accuracy: 0.7540\n",
            "Epoch 308/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5056 - accuracy: 0.7498 - val_loss: 0.5039 - val_accuracy: 0.7539\n",
            "Epoch 309/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5056 - accuracy: 0.7497 - val_loss: 0.5039 - val_accuracy: 0.7537\n",
            "Epoch 310/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5056 - accuracy: 0.7500 - val_loss: 0.5039 - val_accuracy: 0.7545\n",
            "Epoch 311/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5056 - accuracy: 0.7498 - val_loss: 0.5038 - val_accuracy: 0.7544\n",
            "Epoch 312/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5056 - accuracy: 0.7501 - val_loss: 0.5038 - val_accuracy: 0.7536\n",
            "Epoch 313/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5055 - accuracy: 0.7500 - val_loss: 0.5038 - val_accuracy: 0.7539\n",
            "Epoch 314/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5055 - accuracy: 0.7499 - val_loss: 0.5038 - val_accuracy: 0.7545\n",
            "Epoch 315/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5055 - accuracy: 0.7498 - val_loss: 0.5038 - val_accuracy: 0.7542\n",
            "Epoch 316/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5055 - accuracy: 0.7500 - val_loss: 0.5038 - val_accuracy: 0.7543\n",
            "Epoch 317/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5055 - accuracy: 0.7501 - val_loss: 0.5038 - val_accuracy: 0.7539\n",
            "Epoch 318/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5055 - accuracy: 0.7500 - val_loss: 0.5038 - val_accuracy: 0.7543\n",
            "Epoch 319/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5054 - accuracy: 0.7500 - val_loss: 0.5037 - val_accuracy: 0.7537\n",
            "Epoch 320/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5054 - accuracy: 0.7500 - val_loss: 0.5037 - val_accuracy: 0.7542\n",
            "Epoch 321/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5054 - accuracy: 0.7500 - val_loss: 0.5037 - val_accuracy: 0.7539\n",
            "Epoch 322/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5054 - accuracy: 0.7498 - val_loss: 0.5037 - val_accuracy: 0.7542\n",
            "Epoch 323/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5054 - accuracy: 0.7501 - val_loss: 0.5037 - val_accuracy: 0.7547\n",
            "Epoch 324/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5053 - accuracy: 0.7497 - val_loss: 0.5037 - val_accuracy: 0.7529\n",
            "Epoch 325/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5053 - accuracy: 0.7497 - val_loss: 0.5037 - val_accuracy: 0.7540\n",
            "Epoch 326/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5053 - accuracy: 0.7500 - val_loss: 0.5037 - val_accuracy: 0.7541\n",
            "Epoch 327/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5053 - accuracy: 0.7500 - val_loss: 0.5037 - val_accuracy: 0.7544\n",
            "Epoch 328/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5053 - accuracy: 0.7500 - val_loss: 0.5036 - val_accuracy: 0.7539\n",
            "Epoch 329/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5052 - accuracy: 0.7500 - val_loss: 0.5036 - val_accuracy: 0.7538\n",
            "Epoch 330/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5052 - accuracy: 0.7500 - val_loss: 0.5036 - val_accuracy: 0.7538\n",
            "Epoch 331/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5052 - accuracy: 0.7499 - val_loss: 0.5036 - val_accuracy: 0.7539\n",
            "Epoch 332/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5052 - accuracy: 0.7500 - val_loss: 0.5036 - val_accuracy: 0.7539\n",
            "Epoch 333/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5052 - accuracy: 0.7499 - val_loss: 0.5036 - val_accuracy: 0.7541\n",
            "Epoch 334/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5052 - accuracy: 0.7500 - val_loss: 0.5036 - val_accuracy: 0.7539\n",
            "Epoch 335/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5052 - accuracy: 0.7501 - val_loss: 0.5036 - val_accuracy: 0.7539\n",
            "Epoch 336/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5051 - accuracy: 0.7499 - val_loss: 0.5036 - val_accuracy: 0.7542\n",
            "Epoch 337/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5051 - accuracy: 0.7499 - val_loss: 0.5035 - val_accuracy: 0.7540\n",
            "Epoch 338/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5051 - accuracy: 0.7502 - val_loss: 0.5035 - val_accuracy: 0.7538\n",
            "Epoch 339/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5051 - accuracy: 0.7502 - val_loss: 0.5035 - val_accuracy: 0.7537\n",
            "Epoch 340/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5051 - accuracy: 0.7503 - val_loss: 0.5035 - val_accuracy: 0.7537\n",
            "Epoch 341/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5051 - accuracy: 0.7499 - val_loss: 0.5035 - val_accuracy: 0.7537\n",
            "Epoch 342/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5050 - accuracy: 0.7503 - val_loss: 0.5035 - val_accuracy: 0.7534\n",
            "Epoch 343/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5050 - accuracy: 0.7503 - val_loss: 0.5035 - val_accuracy: 0.7536\n",
            "Epoch 344/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5050 - accuracy: 0.7501 - val_loss: 0.5035 - val_accuracy: 0.7536\n",
            "Epoch 345/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5050 - accuracy: 0.7503 - val_loss: 0.5035 - val_accuracy: 0.7538\n",
            "Epoch 346/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5050 - accuracy: 0.7504 - val_loss: 0.5034 - val_accuracy: 0.7538\n",
            "Epoch 347/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5050 - accuracy: 0.7502 - val_loss: 0.5034 - val_accuracy: 0.7539\n",
            "Epoch 348/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5049 - accuracy: 0.7502 - val_loss: 0.5034 - val_accuracy: 0.7538\n",
            "Epoch 349/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5049 - accuracy: 0.7503 - val_loss: 0.5034 - val_accuracy: 0.7539\n",
            "Epoch 350/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5049 - accuracy: 0.7503 - val_loss: 0.5034 - val_accuracy: 0.7539\n",
            "Epoch 351/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5049 - accuracy: 0.7505 - val_loss: 0.5034 - val_accuracy: 0.7538\n",
            "Epoch 352/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5049 - accuracy: 0.7504 - val_loss: 0.5034 - val_accuracy: 0.7539\n",
            "Epoch 353/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5049 - accuracy: 0.7502 - val_loss: 0.5034 - val_accuracy: 0.7537\n",
            "Epoch 354/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5048 - accuracy: 0.7505 - val_loss: 0.5034 - val_accuracy: 0.7538\n",
            "Epoch 355/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.7506 - val_loss: 0.5034 - val_accuracy: 0.7533\n",
            "Epoch 356/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5048 - accuracy: 0.7503 - val_loss: 0.5034 - val_accuracy: 0.7538\n",
            "Epoch 357/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5048 - accuracy: 0.7503 - val_loss: 0.5033 - val_accuracy: 0.7536\n",
            "Epoch 358/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.7501 - val_loss: 0.5033 - val_accuracy: 0.7537\n",
            "Epoch 359/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.7504 - val_loss: 0.5033 - val_accuracy: 0.7534\n",
            "Epoch 360/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5047 - accuracy: 0.7505 - val_loss: 0.5033 - val_accuracy: 0.7535\n",
            "Epoch 361/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5047 - accuracy: 0.7506 - val_loss: 0.5033 - val_accuracy: 0.7539\n",
            "Epoch 362/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5047 - accuracy: 0.7503 - val_loss: 0.5033 - val_accuracy: 0.7534\n",
            "Epoch 363/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5047 - accuracy: 0.7503 - val_loss: 0.5033 - val_accuracy: 0.7534\n",
            "Epoch 364/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5047 - accuracy: 0.7506 - val_loss: 0.5033 - val_accuracy: 0.7537\n",
            "Epoch 365/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5047 - accuracy: 0.7506 - val_loss: 0.5033 - val_accuracy: 0.7532\n",
            "Epoch 366/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5047 - accuracy: 0.7507 - val_loss: 0.5033 - val_accuracy: 0.7537\n",
            "Epoch 367/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5047 - accuracy: 0.7502 - val_loss: 0.5033 - val_accuracy: 0.7532\n",
            "Epoch 368/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5046 - accuracy: 0.7505 - val_loss: 0.5033 - val_accuracy: 0.7535\n",
            "Epoch 369/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5046 - accuracy: 0.7505 - val_loss: 0.5032 - val_accuracy: 0.7537\n",
            "Epoch 370/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5046 - accuracy: 0.7503 - val_loss: 0.5033 - val_accuracy: 0.7535\n",
            "Epoch 371/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5046 - accuracy: 0.7505 - val_loss: 0.5032 - val_accuracy: 0.7532\n",
            "Epoch 372/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5046 - accuracy: 0.7506 - val_loss: 0.5032 - val_accuracy: 0.7536\n",
            "Epoch 373/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5046 - accuracy: 0.7507 - val_loss: 0.5032 - val_accuracy: 0.7535\n",
            "Epoch 374/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5045 - accuracy: 0.7507 - val_loss: 0.5032 - val_accuracy: 0.7537\n",
            "Epoch 375/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5045 - accuracy: 0.7509 - val_loss: 0.5032 - val_accuracy: 0.7532\n",
            "Epoch 376/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5045 - accuracy: 0.7508 - val_loss: 0.5032 - val_accuracy: 0.7534\n",
            "Epoch 377/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5045 - accuracy: 0.7506 - val_loss: 0.5032 - val_accuracy: 0.7537\n",
            "Epoch 378/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5045 - accuracy: 0.7509 - val_loss: 0.5032 - val_accuracy: 0.7542\n",
            "Epoch 379/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5045 - accuracy: 0.7507 - val_loss: 0.5032 - val_accuracy: 0.7534\n",
            "Epoch 380/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5045 - accuracy: 0.7511 - val_loss: 0.5032 - val_accuracy: 0.7537\n",
            "Epoch 381/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.7506 - val_loss: 0.5032 - val_accuracy: 0.7537\n",
            "Epoch 382/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5045 - accuracy: 0.7507 - val_loss: 0.5032 - val_accuracy: 0.7536\n",
            "Epoch 383/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5044 - accuracy: 0.7510 - val_loss: 0.5031 - val_accuracy: 0.7537\n",
            "Epoch 384/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.7507 - val_loss: 0.5031 - val_accuracy: 0.7538\n",
            "Epoch 385/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5044 - accuracy: 0.7506 - val_loss: 0.5031 - val_accuracy: 0.7534\n",
            "Epoch 386/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.7506 - val_loss: 0.5031 - val_accuracy: 0.7537\n",
            "Epoch 387/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.7510 - val_loss: 0.5031 - val_accuracy: 0.7537\n",
            "Epoch 388/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.7510 - val_loss: 0.5031 - val_accuracy: 0.7540\n",
            "Epoch 389/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5043 - accuracy: 0.7505 - val_loss: 0.5031 - val_accuracy: 0.7539\n",
            "Epoch 390/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5043 - accuracy: 0.7508 - val_loss: 0.5031 - val_accuracy: 0.7538\n",
            "Epoch 391/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5043 - accuracy: 0.7509 - val_loss: 0.5031 - val_accuracy: 0.7537\n",
            "Epoch 392/1000\n",
            "111/111 [==============================] - 1s 9ms/step - loss: 0.5043 - accuracy: 0.7506 - val_loss: 0.5031 - val_accuracy: 0.7534\n",
            "Epoch 393/1000\n",
            "111/111 [==============================] - 1s 5ms/step - loss: 0.5043 - accuracy: 0.7508 - val_loss: 0.5031 - val_accuracy: 0.7534\n",
            "Epoch 394/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5043 - accuracy: 0.7509 - val_loss: 0.5031 - val_accuracy: 0.7539\n",
            "Epoch 395/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5043 - accuracy: 0.7507 - val_loss: 0.5031 - val_accuracy: 0.7537\n",
            "Epoch 396/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5042 - accuracy: 0.7508 - val_loss: 0.5031 - val_accuracy: 0.7537\n",
            "Epoch 397/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5042 - accuracy: 0.7510 - val_loss: 0.5031 - val_accuracy: 0.7537\n",
            "Epoch 398/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5042 - accuracy: 0.7508 - val_loss: 0.5031 - val_accuracy: 0.7537\n",
            "Epoch 399/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5042 - accuracy: 0.7509 - val_loss: 0.5031 - val_accuracy: 0.7537\n",
            "Epoch 400/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5042 - accuracy: 0.7510 - val_loss: 0.5030 - val_accuracy: 0.7537\n",
            "Epoch 401/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5042 - accuracy: 0.7510 - val_loss: 0.5030 - val_accuracy: 0.7537\n",
            "Epoch 402/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5042 - accuracy: 0.7509 - val_loss: 0.5030 - val_accuracy: 0.7537\n",
            "Epoch 403/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5042 - accuracy: 0.7509 - val_loss: 0.5030 - val_accuracy: 0.7534\n",
            "Epoch 404/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5042 - accuracy: 0.7510 - val_loss: 0.5030 - val_accuracy: 0.7537\n",
            "Epoch 405/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5041 - accuracy: 0.7509 - val_loss: 0.5030 - val_accuracy: 0.7537\n",
            "Epoch 406/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5041 - accuracy: 0.7510 - val_loss: 0.5030 - val_accuracy: 0.7539\n",
            "Epoch 407/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5041 - accuracy: 0.7511 - val_loss: 0.5030 - val_accuracy: 0.7540\n",
            "Epoch 408/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5041 - accuracy: 0.7510 - val_loss: 0.5030 - val_accuracy: 0.7539\n",
            "Epoch 409/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5041 - accuracy: 0.7508 - val_loss: 0.5030 - val_accuracy: 0.7537\n",
            "Epoch 410/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5041 - accuracy: 0.7509 - val_loss: 0.5030 - val_accuracy: 0.7537\n",
            "Epoch 411/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5041 - accuracy: 0.7509 - val_loss: 0.5030 - val_accuracy: 0.7540\n",
            "Epoch 412/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5041 - accuracy: 0.7512 - val_loss: 0.5030 - val_accuracy: 0.7539\n",
            "Epoch 413/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5041 - accuracy: 0.7510 - val_loss: 0.5030 - val_accuracy: 0.7542\n",
            "Epoch 414/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5040 - accuracy: 0.7511 - val_loss: 0.5030 - val_accuracy: 0.7538\n",
            "Epoch 415/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5040 - accuracy: 0.7507 - val_loss: 0.5030 - val_accuracy: 0.7543\n",
            "Epoch 416/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5040 - accuracy: 0.7510 - val_loss: 0.5030 - val_accuracy: 0.7537\n",
            "Epoch 417/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5040 - accuracy: 0.7511 - val_loss: 0.5029 - val_accuracy: 0.7540\n",
            "Epoch 418/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5040 - accuracy: 0.7510 - val_loss: 0.5029 - val_accuracy: 0.7538\n",
            "Epoch 419/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5040 - accuracy: 0.7511 - val_loss: 0.5029 - val_accuracy: 0.7542\n",
            "Epoch 420/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5040 - accuracy: 0.7510 - val_loss: 0.5029 - val_accuracy: 0.7542\n",
            "Epoch 421/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5039 - accuracy: 0.7512 - val_loss: 0.5029 - val_accuracy: 0.7538\n",
            "Epoch 422/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5039 - accuracy: 0.7510 - val_loss: 0.5029 - val_accuracy: 0.7537\n",
            "Epoch 423/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5039 - accuracy: 0.7512 - val_loss: 0.5029 - val_accuracy: 0.7542\n",
            "Epoch 424/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5039 - accuracy: 0.7508 - val_loss: 0.5029 - val_accuracy: 0.7540\n",
            "Epoch 425/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5039 - accuracy: 0.7512 - val_loss: 0.5029 - val_accuracy: 0.7542\n",
            "Epoch 426/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5039 - accuracy: 0.7511 - val_loss: 0.5029 - val_accuracy: 0.7541\n",
            "Epoch 427/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5039 - accuracy: 0.7512 - val_loss: 0.5029 - val_accuracy: 0.7540\n",
            "Epoch 428/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5039 - accuracy: 0.7512 - val_loss: 0.5029 - val_accuracy: 0.7542\n",
            "Epoch 429/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5039 - accuracy: 0.7512 - val_loss: 0.5029 - val_accuracy: 0.7543\n",
            "Epoch 430/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5038 - accuracy: 0.7510 - val_loss: 0.5029 - val_accuracy: 0.7542\n",
            "Epoch 431/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5038 - accuracy: 0.7511 - val_loss: 0.5029 - val_accuracy: 0.7543\n",
            "Epoch 432/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5038 - accuracy: 0.7510 - val_loss: 0.5029 - val_accuracy: 0.7542\n",
            "Epoch 433/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5038 - accuracy: 0.7511 - val_loss: 0.5029 - val_accuracy: 0.7543\n",
            "Epoch 434/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5038 - accuracy: 0.7511 - val_loss: 0.5029 - val_accuracy: 0.7542\n",
            "Epoch 435/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5038 - accuracy: 0.7513 - val_loss: 0.5028 - val_accuracy: 0.7542\n",
            "Epoch 436/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5038 - accuracy: 0.7513 - val_loss: 0.5029 - val_accuracy: 0.7542\n",
            "Epoch 437/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5038 - accuracy: 0.7510 - val_loss: 0.5028 - val_accuracy: 0.7543\n",
            "Epoch 438/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5037 - accuracy: 0.7512 - val_loss: 0.5028 - val_accuracy: 0.7539\n",
            "Epoch 439/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5038 - accuracy: 0.7511 - val_loss: 0.5028 - val_accuracy: 0.7544\n",
            "Epoch 440/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5037 - accuracy: 0.7512 - val_loss: 0.5028 - val_accuracy: 0.7546\n",
            "Epoch 441/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5037 - accuracy: 0.7510 - val_loss: 0.5028 - val_accuracy: 0.7547\n",
            "Epoch 442/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5037 - accuracy: 0.7511 - val_loss: 0.5028 - val_accuracy: 0.7543\n",
            "Epoch 443/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5037 - accuracy: 0.7511 - val_loss: 0.5028 - val_accuracy: 0.7544\n",
            "Epoch 444/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5037 - accuracy: 0.7512 - val_loss: 0.5028 - val_accuracy: 0.7544\n",
            "Epoch 445/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5037 - accuracy: 0.7510 - val_loss: 0.5028 - val_accuracy: 0.7544\n",
            "Epoch 446/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5037 - accuracy: 0.7511 - val_loss: 0.5028 - val_accuracy: 0.7544\n",
            "Epoch 447/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5037 - accuracy: 0.7512 - val_loss: 0.5028 - val_accuracy: 0.7542\n",
            "Epoch 448/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5037 - accuracy: 0.7511 - val_loss: 0.5028 - val_accuracy: 0.7540\n",
            "Epoch 449/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5036 - accuracy: 0.7511 - val_loss: 0.5028 - val_accuracy: 0.7541\n",
            "Epoch 450/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5036 - accuracy: 0.7511 - val_loss: 0.5028 - val_accuracy: 0.7543\n",
            "Epoch 451/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5036 - accuracy: 0.7511 - val_loss: 0.5028 - val_accuracy: 0.7542\n",
            "Epoch 452/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5036 - accuracy: 0.7513 - val_loss: 0.5028 - val_accuracy: 0.7542\n",
            "Epoch 453/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5036 - accuracy: 0.7514 - val_loss: 0.5028 - val_accuracy: 0.7543\n",
            "Epoch 454/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5036 - accuracy: 0.7511 - val_loss: 0.5028 - val_accuracy: 0.7548\n",
            "Epoch 455/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5036 - accuracy: 0.7510 - val_loss: 0.5028 - val_accuracy: 0.7545\n",
            "Epoch 456/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5036 - accuracy: 0.7512 - val_loss: 0.5028 - val_accuracy: 0.7548\n",
            "Epoch 457/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5036 - accuracy: 0.7510 - val_loss: 0.5027 - val_accuracy: 0.7542\n",
            "Epoch 458/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5035 - accuracy: 0.7510 - val_loss: 0.5028 - val_accuracy: 0.7546\n",
            "Epoch 459/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5036 - accuracy: 0.7511 - val_loss: 0.5027 - val_accuracy: 0.7542\n",
            "Epoch 460/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5035 - accuracy: 0.7512 - val_loss: 0.5027 - val_accuracy: 0.7542\n",
            "Epoch 461/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5035 - accuracy: 0.7512 - val_loss: 0.5028 - val_accuracy: 0.7547\n",
            "Epoch 462/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5035 - accuracy: 0.7510 - val_loss: 0.5027 - val_accuracy: 0.7546\n",
            "Epoch 463/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5035 - accuracy: 0.7514 - val_loss: 0.5027 - val_accuracy: 0.7545\n",
            "Epoch 464/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5035 - accuracy: 0.7511 - val_loss: 0.5027 - val_accuracy: 0.7542\n",
            "Epoch 465/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5035 - accuracy: 0.7513 - val_loss: 0.5027 - val_accuracy: 0.7542\n",
            "Epoch 466/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5035 - accuracy: 0.7511 - val_loss: 0.5027 - val_accuracy: 0.7540\n",
            "Epoch 467/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5035 - accuracy: 0.7511 - val_loss: 0.5027 - val_accuracy: 0.7542\n",
            "Epoch 468/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5035 - accuracy: 0.7512 - val_loss: 0.5027 - val_accuracy: 0.7543\n",
            "Epoch 469/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.7512 - val_loss: 0.5027 - val_accuracy: 0.7539\n",
            "Epoch 470/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5034 - accuracy: 0.7512 - val_loss: 0.5027 - val_accuracy: 0.7541\n",
            "Epoch 471/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.7514 - val_loss: 0.5027 - val_accuracy: 0.7542\n",
            "Epoch 472/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5034 - accuracy: 0.7512 - val_loss: 0.5027 - val_accuracy: 0.7542\n",
            "Epoch 473/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.7511 - val_loss: 0.5027 - val_accuracy: 0.7542\n",
            "Epoch 474/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5034 - accuracy: 0.7514 - val_loss: 0.5027 - val_accuracy: 0.7543\n",
            "Epoch 475/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.7513 - val_loss: 0.5027 - val_accuracy: 0.7542\n",
            "Epoch 476/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5034 - accuracy: 0.7510 - val_loss: 0.5026 - val_accuracy: 0.7544\n",
            "Epoch 477/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.7512 - val_loss: 0.5026 - val_accuracy: 0.7544\n",
            "Epoch 478/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5034 - accuracy: 0.7514 - val_loss: 0.5027 - val_accuracy: 0.7542\n",
            "Epoch 479/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.7513 - val_loss: 0.5027 - val_accuracy: 0.7540\n",
            "Epoch 480/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5033 - accuracy: 0.7512 - val_loss: 0.5027 - val_accuracy: 0.7542\n",
            "Epoch 481/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5033 - accuracy: 0.7514 - val_loss: 0.5026 - val_accuracy: 0.7542\n",
            "Epoch 482/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5033 - accuracy: 0.7514 - val_loss: 0.5026 - val_accuracy: 0.7541\n",
            "Epoch 483/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5033 - accuracy: 0.7515 - val_loss: 0.5027 - val_accuracy: 0.7544\n",
            "Epoch 484/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5033 - accuracy: 0.7511 - val_loss: 0.5026 - val_accuracy: 0.7542\n",
            "Epoch 485/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5033 - accuracy: 0.7512 - val_loss: 0.5026 - val_accuracy: 0.7541\n",
            "Epoch 486/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5033 - accuracy: 0.7514 - val_loss: 0.5026 - val_accuracy: 0.7541\n",
            "Epoch 487/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5033 - accuracy: 0.7513 - val_loss: 0.5026 - val_accuracy: 0.7539\n",
            "Epoch 488/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5033 - accuracy: 0.7514 - val_loss: 0.5026 - val_accuracy: 0.7544\n",
            "Epoch 489/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5033 - accuracy: 0.7511 - val_loss: 0.5026 - val_accuracy: 0.7537\n",
            "Epoch 490/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5033 - accuracy: 0.7510 - val_loss: 0.5026 - val_accuracy: 0.7540\n",
            "Epoch 491/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5032 - accuracy: 0.7511 - val_loss: 0.5026 - val_accuracy: 0.7541\n",
            "Epoch 492/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5032 - accuracy: 0.7514 - val_loss: 0.5026 - val_accuracy: 0.7544\n",
            "Epoch 493/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5032 - accuracy: 0.7514 - val_loss: 0.5026 - val_accuracy: 0.7542\n",
            "Epoch 494/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5032 - accuracy: 0.7512 - val_loss: 0.5026 - val_accuracy: 0.7539\n",
            "Epoch 495/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5032 - accuracy: 0.7512 - val_loss: 0.5026 - val_accuracy: 0.7543\n",
            "Epoch 496/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5032 - accuracy: 0.7515 - val_loss: 0.5026 - val_accuracy: 0.7539\n",
            "Epoch 497/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5032 - accuracy: 0.7513 - val_loss: 0.5026 - val_accuracy: 0.7543\n",
            "Epoch 498/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5032 - accuracy: 0.7518 - val_loss: 0.5026 - val_accuracy: 0.7541\n",
            "Epoch 499/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5032 - accuracy: 0.7512 - val_loss: 0.5026 - val_accuracy: 0.7542\n",
            "Epoch 500/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5032 - accuracy: 0.7515 - val_loss: 0.5026 - val_accuracy: 0.7547\n",
            "Epoch 501/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5032 - accuracy: 0.7515 - val_loss: 0.5026 - val_accuracy: 0.7537\n",
            "Epoch 502/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5032 - accuracy: 0.7515 - val_loss: 0.5026 - val_accuracy: 0.7547\n",
            "Epoch 503/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5031 - accuracy: 0.7514 - val_loss: 0.5026 - val_accuracy: 0.7549\n",
            "Epoch 504/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5032 - accuracy: 0.7514 - val_loss: 0.5026 - val_accuracy: 0.7545\n",
            "Epoch 505/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5031 - accuracy: 0.7513 - val_loss: 0.5026 - val_accuracy: 0.7540\n",
            "Epoch 506/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5031 - accuracy: 0.7512 - val_loss: 0.5026 - val_accuracy: 0.7539\n",
            "Epoch 507/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5031 - accuracy: 0.7516 - val_loss: 0.5026 - val_accuracy: 0.7539\n",
            "Epoch 508/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5031 - accuracy: 0.7515 - val_loss: 0.5026 - val_accuracy: 0.7547\n",
            "Epoch 509/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5031 - accuracy: 0.7513 - val_loss: 0.5026 - val_accuracy: 0.7547\n",
            "Epoch 510/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5031 - accuracy: 0.7514 - val_loss: 0.5025 - val_accuracy: 0.7540\n",
            "Epoch 511/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5031 - accuracy: 0.7516 - val_loss: 0.5025 - val_accuracy: 0.7537\n",
            "Epoch 512/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5031 - accuracy: 0.7515 - val_loss: 0.5025 - val_accuracy: 0.7540\n",
            "Epoch 513/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5031 - accuracy: 0.7514 - val_loss: 0.5025 - val_accuracy: 0.7540\n",
            "Epoch 514/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5031 - accuracy: 0.7515 - val_loss: 0.5025 - val_accuracy: 0.7541\n",
            "Epoch 515/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5030 - accuracy: 0.7513 - val_loss: 0.5025 - val_accuracy: 0.7539\n",
            "Epoch 516/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.7516 - val_loss: 0.5025 - val_accuracy: 0.7539\n",
            "Epoch 517/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.7517 - val_loss: 0.5025 - val_accuracy: 0.7540\n",
            "Epoch 518/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.7517 - val_loss: 0.5025 - val_accuracy: 0.7539\n",
            "Epoch 519/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.7515 - val_loss: 0.5025 - val_accuracy: 0.7544\n",
            "Epoch 520/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5030 - accuracy: 0.7515 - val_loss: 0.5025 - val_accuracy: 0.7549\n",
            "Epoch 521/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.7516 - val_loss: 0.5025 - val_accuracy: 0.7541\n",
            "Epoch 522/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.7515 - val_loss: 0.5025 - val_accuracy: 0.7542\n",
            "Epoch 523/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.7514 - val_loss: 0.5025 - val_accuracy: 0.7539\n",
            "Epoch 524/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.7515 - val_loss: 0.5025 - val_accuracy: 0.7542\n",
            "Epoch 525/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.7518 - val_loss: 0.5025 - val_accuracy: 0.7542\n",
            "Epoch 526/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.7518 - val_loss: 0.5025 - val_accuracy: 0.7543\n",
            "Epoch 527/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.7516 - val_loss: 0.5025 - val_accuracy: 0.7541\n",
            "Epoch 528/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.7515 - val_loss: 0.5025 - val_accuracy: 0.7541\n",
            "Epoch 529/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.7516 - val_loss: 0.5025 - val_accuracy: 0.7542\n",
            "Epoch 530/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.7516 - val_loss: 0.5025 - val_accuracy: 0.7544\n",
            "Epoch 531/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5029 - accuracy: 0.7516 - val_loss: 0.5025 - val_accuracy: 0.7542\n",
            "Epoch 532/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.7516 - val_loss: 0.5025 - val_accuracy: 0.7543\n",
            "Epoch 533/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.7517 - val_loss: 0.5025 - val_accuracy: 0.7544\n",
            "Epoch 534/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.7518 - val_loss: 0.5025 - val_accuracy: 0.7543\n",
            "Epoch 535/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.7515 - val_loss: 0.5024 - val_accuracy: 0.7540\n",
            "Epoch 536/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.7516 - val_loss: 0.5024 - val_accuracy: 0.7542\n",
            "Epoch 537/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5029 - accuracy: 0.7515 - val_loss: 0.5025 - val_accuracy: 0.7551\n",
            "Epoch 538/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.7515 - val_loss: 0.5024 - val_accuracy: 0.7540\n",
            "Epoch 539/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.7516 - val_loss: 0.5024 - val_accuracy: 0.7544\n",
            "Epoch 540/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5028 - accuracy: 0.7514 - val_loss: 0.5024 - val_accuracy: 0.7542\n",
            "Epoch 541/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5028 - accuracy: 0.7517 - val_loss: 0.5025 - val_accuracy: 0.7554\n",
            "Epoch 542/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5028 - accuracy: 0.7516 - val_loss: 0.5024 - val_accuracy: 0.7547\n",
            "Epoch 543/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5028 - accuracy: 0.7519 - val_loss: 0.5024 - val_accuracy: 0.7547\n",
            "Epoch 544/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5028 - accuracy: 0.7517 - val_loss: 0.5024 - val_accuracy: 0.7543\n",
            "Epoch 545/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5028 - accuracy: 0.7516 - val_loss: 0.5024 - val_accuracy: 0.7547\n",
            "Epoch 546/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5028 - accuracy: 0.7517 - val_loss: 0.5024 - val_accuracy: 0.7544\n",
            "Epoch 547/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5028 - accuracy: 0.7517 - val_loss: 0.5024 - val_accuracy: 0.7544\n",
            "Epoch 548/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5028 - accuracy: 0.7515 - val_loss: 0.5024 - val_accuracy: 0.7543\n",
            "Epoch 549/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5028 - accuracy: 0.7515 - val_loss: 0.5024 - val_accuracy: 0.7548\n",
            "Epoch 550/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5028 - accuracy: 0.7519 - val_loss: 0.5024 - val_accuracy: 0.7544\n",
            "Epoch 551/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5028 - accuracy: 0.7517 - val_loss: 0.5024 - val_accuracy: 0.7542\n",
            "Epoch 552/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5028 - accuracy: 0.7517 - val_loss: 0.5024 - val_accuracy: 0.7547\n",
            "Epoch 553/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5028 - accuracy: 0.7517 - val_loss: 0.5024 - val_accuracy: 0.7555\n",
            "Epoch 554/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5027 - accuracy: 0.7515 - val_loss: 0.5024 - val_accuracy: 0.7544\n",
            "Epoch 555/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5027 - accuracy: 0.7519 - val_loss: 0.5024 - val_accuracy: 0.7544\n",
            "Epoch 556/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5028 - accuracy: 0.7515 - val_loss: 0.5024 - val_accuracy: 0.7545\n",
            "Epoch 557/1000\n",
            "111/111 [==============================] - 1s 5ms/step - loss: 0.5027 - accuracy: 0.7519 - val_loss: 0.5024 - val_accuracy: 0.7544\n",
            "Epoch 558/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.5027 - accuracy: 0.7517 - val_loss: 0.5024 - val_accuracy: 0.7550\n",
            "Epoch 559/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.5027 - accuracy: 0.7518 - val_loss: 0.5024 - val_accuracy: 0.7548\n",
            "Epoch 560/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5027 - accuracy: 0.7517 - val_loss: 0.5024 - val_accuracy: 0.7547\n",
            "Epoch 561/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5027 - accuracy: 0.7517 - val_loss: 0.5024 - val_accuracy: 0.7545\n",
            "Epoch 562/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5027 - accuracy: 0.7520 - val_loss: 0.5024 - val_accuracy: 0.7551\n",
            "Epoch 563/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5027 - accuracy: 0.7516 - val_loss: 0.5024 - val_accuracy: 0.7548\n",
            "Epoch 564/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5027 - accuracy: 0.7517 - val_loss: 0.5024 - val_accuracy: 0.7550\n",
            "Epoch 565/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5027 - accuracy: 0.7518 - val_loss: 0.5024 - val_accuracy: 0.7548\n",
            "Epoch 566/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5027 - accuracy: 0.7517 - val_loss: 0.5024 - val_accuracy: 0.7547\n",
            "Epoch 567/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5027 - accuracy: 0.7518 - val_loss: 0.5024 - val_accuracy: 0.7547\n",
            "Epoch 568/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5027 - accuracy: 0.7519 - val_loss: 0.5024 - val_accuracy: 0.7547\n",
            "Epoch 569/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.7515 - val_loss: 0.5024 - val_accuracy: 0.7547\n",
            "Epoch 570/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5026 - accuracy: 0.7518 - val_loss: 0.5024 - val_accuracy: 0.7546\n",
            "Epoch 571/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.7518 - val_loss: 0.5024 - val_accuracy: 0.7546\n",
            "Epoch 572/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.7520 - val_loss: 0.5024 - val_accuracy: 0.7548\n",
            "Epoch 573/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.7519 - val_loss: 0.5024 - val_accuracy: 0.7547\n",
            "Epoch 574/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.7518 - val_loss: 0.5024 - val_accuracy: 0.7547\n",
            "Epoch 575/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.7519 - val_loss: 0.5023 - val_accuracy: 0.7547\n",
            "Epoch 576/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5026 - accuracy: 0.7518 - val_loss: 0.5024 - val_accuracy: 0.7547\n",
            "Epoch 577/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.7519 - val_loss: 0.5023 - val_accuracy: 0.7550\n",
            "Epoch 578/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.7517 - val_loss: 0.5023 - val_accuracy: 0.7547\n",
            "Epoch 579/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5026 - accuracy: 0.7521 - val_loss: 0.5023 - val_accuracy: 0.7549\n",
            "Epoch 580/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.7517 - val_loss: 0.5023 - val_accuracy: 0.7549\n",
            "Epoch 581/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.7518 - val_loss: 0.5023 - val_accuracy: 0.7547\n",
            "Epoch 582/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.7516 - val_loss: 0.5023 - val_accuracy: 0.7546\n",
            "Epoch 583/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.7520 - val_loss: 0.5023 - val_accuracy: 0.7549\n",
            "Epoch 584/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.7516 - val_loss: 0.5023 - val_accuracy: 0.7551\n",
            "Epoch 585/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.7518 - val_loss: 0.5023 - val_accuracy: 0.7550\n",
            "Epoch 586/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.7516 - val_loss: 0.5023 - val_accuracy: 0.7551\n",
            "Epoch 587/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.7518 - val_loss: 0.5023 - val_accuracy: 0.7548\n",
            "Epoch 588/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.7518 - val_loss: 0.5023 - val_accuracy: 0.7551\n",
            "Epoch 589/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.7521 - val_loss: 0.5023 - val_accuracy: 0.7549\n",
            "Epoch 590/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.7521 - val_loss: 0.5023 - val_accuracy: 0.7553\n",
            "Epoch 591/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.7516 - val_loss: 0.5023 - val_accuracy: 0.7549\n",
            "Epoch 592/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.7520 - val_loss: 0.5023 - val_accuracy: 0.7550\n",
            "Epoch 593/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.7517 - val_loss: 0.5023 - val_accuracy: 0.7550\n",
            "Epoch 594/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.7521 - val_loss: 0.5023 - val_accuracy: 0.7550\n",
            "Epoch 595/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.7518 - val_loss: 0.5023 - val_accuracy: 0.7549\n",
            "Epoch 596/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.7519 - val_loss: 0.5023 - val_accuracy: 0.7549\n",
            "Epoch 597/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5025 - accuracy: 0.7518 - val_loss: 0.5023 - val_accuracy: 0.7547\n",
            "Epoch 598/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.7519 - val_loss: 0.5023 - val_accuracy: 0.7552\n",
            "Epoch 599/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.7518 - val_loss: 0.5023 - val_accuracy: 0.7551\n",
            "Epoch 600/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.7520 - val_loss: 0.5023 - val_accuracy: 0.7547\n",
            "Epoch 601/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.7518 - val_loss: 0.5023 - val_accuracy: 0.7553\n",
            "Epoch 602/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5024 - accuracy: 0.7520 - val_loss: 0.5023 - val_accuracy: 0.7547\n",
            "Epoch 603/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.7517 - val_loss: 0.5023 - val_accuracy: 0.7549\n",
            "Epoch 604/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.7518 - val_loss: 0.5023 - val_accuracy: 0.7550\n",
            "Epoch 605/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.7518 - val_loss: 0.5022 - val_accuracy: 0.7548\n",
            "Epoch 606/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.7521 - val_loss: 0.5022 - val_accuracy: 0.7552\n",
            "Epoch 607/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.7516 - val_loss: 0.5022 - val_accuracy: 0.7554\n",
            "Epoch 608/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.7519 - val_loss: 0.5022 - val_accuracy: 0.7551\n",
            "Epoch 609/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.7519 - val_loss: 0.5022 - val_accuracy: 0.7550\n",
            "Epoch 610/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.7522 - val_loss: 0.5022 - val_accuracy: 0.7552\n",
            "Epoch 611/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.7519 - val_loss: 0.5022 - val_accuracy: 0.7548\n",
            "Epoch 612/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.7523 - val_loss: 0.5022 - val_accuracy: 0.7553\n",
            "Epoch 613/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5024 - accuracy: 0.7516 - val_loss: 0.5023 - val_accuracy: 0.7547\n",
            "Epoch 614/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.7518 - val_loss: 0.5022 - val_accuracy: 0.7551\n",
            "Epoch 615/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.7518 - val_loss: 0.5022 - val_accuracy: 0.7548\n",
            "Epoch 616/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.7523 - val_loss: 0.5022 - val_accuracy: 0.7550\n",
            "Epoch 617/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7520 - val_loss: 0.5022 - val_accuracy: 0.7549\n",
            "Epoch 618/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7517 - val_loss: 0.5022 - val_accuracy: 0.7549\n",
            "Epoch 619/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7521 - val_loss: 0.5022 - val_accuracy: 0.7549\n",
            "Epoch 620/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7520 - val_loss: 0.5022 - val_accuracy: 0.7547\n",
            "Epoch 621/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7521 - val_loss: 0.5022 - val_accuracy: 0.7550\n",
            "Epoch 622/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5023 - accuracy: 0.7521 - val_loss: 0.5022 - val_accuracy: 0.7549\n",
            "Epoch 623/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7521 - val_loss: 0.5022 - val_accuracy: 0.7547\n",
            "Epoch 624/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7521 - val_loss: 0.5022 - val_accuracy: 0.7551\n",
            "Epoch 625/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7523 - val_loss: 0.5022 - val_accuracy: 0.7549\n",
            "Epoch 626/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5023 - accuracy: 0.7521 - val_loss: 0.5022 - val_accuracy: 0.7548\n",
            "Epoch 627/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7522 - val_loss: 0.5022 - val_accuracy: 0.7548\n",
            "Epoch 628/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7523 - val_loss: 0.5022 - val_accuracy: 0.7547\n",
            "Epoch 629/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7519 - val_loss: 0.5022 - val_accuracy: 0.7551\n",
            "Epoch 630/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7522 - val_loss: 0.5022 - val_accuracy: 0.7551\n",
            "Epoch 631/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7522 - val_loss: 0.5022 - val_accuracy: 0.7554\n",
            "Epoch 632/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7520 - val_loss: 0.5022 - val_accuracy: 0.7551\n",
            "Epoch 633/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7523 - val_loss: 0.5022 - val_accuracy: 0.7546\n",
            "Epoch 634/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7520 - val_loss: 0.5022 - val_accuracy: 0.7549\n",
            "Epoch 635/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7520 - val_loss: 0.5022 - val_accuracy: 0.7547\n",
            "Epoch 636/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5022 - accuracy: 0.7521 - val_loss: 0.5022 - val_accuracy: 0.7549\n",
            "Epoch 637/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5022 - accuracy: 0.7520 - val_loss: 0.5022 - val_accuracy: 0.7551\n",
            "Epoch 638/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7521 - val_loss: 0.5022 - val_accuracy: 0.7547\n",
            "Epoch 639/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7521 - val_loss: 0.5022 - val_accuracy: 0.7549\n",
            "Epoch 640/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7521 - val_loss: 0.5022 - val_accuracy: 0.7547\n",
            "Epoch 641/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7522 - val_loss: 0.5022 - val_accuracy: 0.7546\n",
            "Epoch 642/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7520 - val_loss: 0.5022 - val_accuracy: 0.7554\n",
            "Epoch 643/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7522 - val_loss: 0.5022 - val_accuracy: 0.7547\n",
            "Epoch 644/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7522 - val_loss: 0.5022 - val_accuracy: 0.7549\n",
            "Epoch 645/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7523 - val_loss: 0.5022 - val_accuracy: 0.7547\n",
            "Epoch 646/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7520 - val_loss: 0.5021 - val_accuracy: 0.7549\n",
            "Epoch 647/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7521 - val_loss: 0.5022 - val_accuracy: 0.7545\n",
            "Epoch 648/1000\n",
            "111/111 [==============================] - 1s 5ms/step - loss: 0.5022 - accuracy: 0.7519 - val_loss: 0.5021 - val_accuracy: 0.7545\n",
            "Epoch 649/1000\n",
            "111/111 [==============================] - 1s 10ms/step - loss: 0.5022 - accuracy: 0.7524 - val_loss: 0.5022 - val_accuracy: 0.7549\n",
            "Epoch 650/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7521 - val_loss: 0.5021 - val_accuracy: 0.7545\n",
            "Epoch 651/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7523 - val_loss: 0.5021 - val_accuracy: 0.7544\n",
            "Epoch 652/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.7523 - val_loss: 0.5021 - val_accuracy: 0.7550\n",
            "Epoch 653/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7523 - val_loss: 0.5021 - val_accuracy: 0.7547\n",
            "Epoch 654/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7523 - val_loss: 0.5021 - val_accuracy: 0.7547\n",
            "Epoch 655/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7520 - val_loss: 0.5021 - val_accuracy: 0.7544\n",
            "Epoch 656/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7524 - val_loss: 0.5021 - val_accuracy: 0.7546\n",
            "Epoch 657/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7520 - val_loss: 0.5022 - val_accuracy: 0.7550\n",
            "Epoch 658/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7522 - val_loss: 0.5021 - val_accuracy: 0.7550\n",
            "Epoch 659/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7523 - val_loss: 0.5021 - val_accuracy: 0.7547\n",
            "Epoch 660/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7520 - val_loss: 0.5021 - val_accuracy: 0.7550\n",
            "Epoch 661/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7521 - val_loss: 0.5021 - val_accuracy: 0.7549\n",
            "Epoch 662/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7521 - val_loss: 0.5021 - val_accuracy: 0.7545\n",
            "Epoch 663/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7521 - val_loss: 0.5021 - val_accuracy: 0.7544\n",
            "Epoch 664/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7523 - val_loss: 0.5021 - val_accuracy: 0.7547\n",
            "Epoch 665/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7522 - val_loss: 0.5021 - val_accuracy: 0.7545\n",
            "Epoch 666/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7523 - val_loss: 0.5021 - val_accuracy: 0.7550\n",
            "Epoch 667/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7523 - val_loss: 0.5021 - val_accuracy: 0.7546\n",
            "Epoch 668/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7525 - val_loss: 0.5021 - val_accuracy: 0.7546\n",
            "Epoch 669/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7522 - val_loss: 0.5021 - val_accuracy: 0.7545\n",
            "Epoch 670/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7520 - val_loss: 0.5021 - val_accuracy: 0.7547\n",
            "Epoch 671/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7522 - val_loss: 0.5021 - val_accuracy: 0.7546\n",
            "Epoch 672/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7522 - val_loss: 0.5021 - val_accuracy: 0.7549\n",
            "Epoch 673/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7522 - val_loss: 0.5021 - val_accuracy: 0.7545\n",
            "Epoch 674/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7523 - val_loss: 0.5021 - val_accuracy: 0.7549\n",
            "Epoch 675/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7522 - val_loss: 0.5021 - val_accuracy: 0.7549\n",
            "Epoch 676/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7524 - val_loss: 0.5021 - val_accuracy: 0.7544\n",
            "Epoch 677/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7523 - val_loss: 0.5021 - val_accuracy: 0.7549\n",
            "Epoch 678/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7523 - val_loss: 0.5021 - val_accuracy: 0.7547\n",
            "Epoch 679/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7522 - val_loss: 0.5021 - val_accuracy: 0.7551\n",
            "Epoch 680/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7523 - val_loss: 0.5021 - val_accuracy: 0.7544\n",
            "Epoch 681/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7523 - val_loss: 0.5021 - val_accuracy: 0.7549\n",
            "Epoch 682/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7522 - val_loss: 0.5021 - val_accuracy: 0.7547\n",
            "Epoch 683/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7521 - val_loss: 0.5021 - val_accuracy: 0.7545\n",
            "Epoch 684/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7524 - val_loss: 0.5021 - val_accuracy: 0.7551\n",
            "Epoch 685/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7524 - val_loss: 0.5021 - val_accuracy: 0.7547\n",
            "Epoch 686/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7523 - val_loss: 0.5021 - val_accuracy: 0.7547\n",
            "Epoch 687/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7525 - val_loss: 0.5021 - val_accuracy: 0.7547\n",
            "Epoch 688/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7521 - val_loss: 0.5021 - val_accuracy: 0.7548\n",
            "Epoch 689/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7522 - val_loss: 0.5021 - val_accuracy: 0.7545\n",
            "Epoch 690/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7524 - val_loss: 0.5021 - val_accuracy: 0.7544\n",
            "Epoch 691/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7526 - val_loss: 0.5021 - val_accuracy: 0.7543\n",
            "Epoch 692/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7523 - val_loss: 0.5021 - val_accuracy: 0.7544\n",
            "Epoch 693/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7521 - val_loss: 0.5020 - val_accuracy: 0.7545\n",
            "Epoch 694/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7520 - val_loss: 0.5020 - val_accuracy: 0.7547\n",
            "Epoch 695/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7523 - val_loss: 0.5021 - val_accuracy: 0.7547\n",
            "Epoch 696/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7520 - val_loss: 0.5021 - val_accuracy: 0.7549\n",
            "Epoch 697/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7519 - val_loss: 0.5021 - val_accuracy: 0.7549\n",
            "Epoch 698/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7523 - val_loss: 0.5020 - val_accuracy: 0.7544\n",
            "Epoch 699/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7524 - val_loss: 0.5020 - val_accuracy: 0.7545\n",
            "Epoch 700/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7521 - val_loss: 0.5020 - val_accuracy: 0.7547\n",
            "Epoch 701/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7521 - val_loss: 0.5020 - val_accuracy: 0.7549\n",
            "Epoch 702/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7522 - val_loss: 0.5020 - val_accuracy: 0.7547\n",
            "Epoch 703/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7522 - val_loss: 0.5020 - val_accuracy: 0.7549\n",
            "Epoch 704/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7520 - val_loss: 0.5021 - val_accuracy: 0.7552\n",
            "Epoch 705/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7523 - val_loss: 0.5020 - val_accuracy: 0.7544\n",
            "Epoch 706/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7522 - val_loss: 0.5020 - val_accuracy: 0.7544\n",
            "Epoch 707/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5019 - accuracy: 0.7522 - val_loss: 0.5020 - val_accuracy: 0.7544\n",
            "Epoch 708/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7523 - val_loss: 0.5020 - val_accuracy: 0.7546\n",
            "Epoch 709/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7522 - val_loss: 0.5020 - val_accuracy: 0.7545\n",
            "Epoch 710/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7523 - val_loss: 0.5020 - val_accuracy: 0.7547\n",
            "Epoch 711/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7524 - val_loss: 0.5020 - val_accuracy: 0.7551\n",
            "Epoch 712/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.7522 - val_loss: 0.5020 - val_accuracy: 0.7544\n",
            "Epoch 713/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7522 - val_loss: 0.5020 - val_accuracy: 0.7549\n",
            "Epoch 714/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7523 - val_loss: 0.5020 - val_accuracy: 0.7546\n",
            "Epoch 715/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7521 - val_loss: 0.5020 - val_accuracy: 0.7547\n",
            "Epoch 716/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7524 - val_loss: 0.5020 - val_accuracy: 0.7544\n",
            "Epoch 717/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7522 - val_loss: 0.5020 - val_accuracy: 0.7545\n",
            "Epoch 718/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7519 - val_loss: 0.5020 - val_accuracy: 0.7548\n",
            "Epoch 719/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7524 - val_loss: 0.5020 - val_accuracy: 0.7544\n",
            "Epoch 720/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7522 - val_loss: 0.5020 - val_accuracy: 0.7549\n",
            "Epoch 721/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7526 - val_loss: 0.5020 - val_accuracy: 0.7546\n",
            "Epoch 722/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7523 - val_loss: 0.5020 - val_accuracy: 0.7547\n",
            "Epoch 723/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7522 - val_loss: 0.5020 - val_accuracy: 0.7546\n",
            "Epoch 724/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7522 - val_loss: 0.5020 - val_accuracy: 0.7545\n",
            "Epoch 725/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7521 - val_loss: 0.5020 - val_accuracy: 0.7543\n",
            "Epoch 726/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7523 - val_loss: 0.5020 - val_accuracy: 0.7544\n",
            "Epoch 727/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7522 - val_loss: 0.5020 - val_accuracy: 0.7545\n",
            "Epoch 728/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7521 - val_loss: 0.5020 - val_accuracy: 0.7549\n",
            "Epoch 729/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7522 - val_loss: 0.5020 - val_accuracy: 0.7544\n",
            "Epoch 730/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7521 - val_loss: 0.5020 - val_accuracy: 0.7544\n",
            "Epoch 731/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5018 - accuracy: 0.7520 - val_loss: 0.5020 - val_accuracy: 0.7547\n",
            "Epoch 732/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7520 - val_loss: 0.5020 - val_accuracy: 0.7545\n",
            "Epoch 733/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7523 - val_loss: 0.5020 - val_accuracy: 0.7542\n",
            "Epoch 734/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7521 - val_loss: 0.5020 - val_accuracy: 0.7549\n",
            "Epoch 735/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7523 - val_loss: 0.5020 - val_accuracy: 0.7543\n",
            "Epoch 736/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7522 - val_loss: 0.5020 - val_accuracy: 0.7542\n",
            "Epoch 737/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7521 - val_loss: 0.5020 - val_accuracy: 0.7547\n",
            "Epoch 738/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7521 - val_loss: 0.5020 - val_accuracy: 0.7550\n",
            "Epoch 739/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7521 - val_loss: 0.5020 - val_accuracy: 0.7542\n",
            "Epoch 740/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7522 - val_loss: 0.5020 - val_accuracy: 0.7542\n",
            "Epoch 741/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7522 - val_loss: 0.5020 - val_accuracy: 0.7544\n",
            "Epoch 742/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7524 - val_loss: 0.5019 - val_accuracy: 0.7543\n",
            "Epoch 743/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7519 - val_loss: 0.5020 - val_accuracy: 0.7547\n",
            "Epoch 744/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7524 - val_loss: 0.5020 - val_accuracy: 0.7547\n",
            "Epoch 745/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7523 - val_loss: 0.5019 - val_accuracy: 0.7548\n",
            "Epoch 746/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7521 - val_loss: 0.5020 - val_accuracy: 0.7541\n",
            "Epoch 747/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7521 - val_loss: 0.5020 - val_accuracy: 0.7551\n",
            "Epoch 748/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7524 - val_loss: 0.5020 - val_accuracy: 0.7549\n",
            "Epoch 749/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7521 - val_loss: 0.5019 - val_accuracy: 0.7545\n",
            "Epoch 750/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7523 - val_loss: 0.5019 - val_accuracy: 0.7547\n",
            "Epoch 751/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7521 - val_loss: 0.5019 - val_accuracy: 0.7542\n",
            "Epoch 752/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7520 - val_loss: 0.5019 - val_accuracy: 0.7552\n",
            "Epoch 753/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7524 - val_loss: 0.5019 - val_accuracy: 0.7547\n",
            "Epoch 754/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7523 - val_loss: 0.5019 - val_accuracy: 0.7542\n",
            "Epoch 755/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7523 - val_loss: 0.5019 - val_accuracy: 0.7542\n",
            "Epoch 756/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7520 - val_loss: 0.5019 - val_accuracy: 0.7546\n",
            "Epoch 757/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7518 - val_loss: 0.5019 - val_accuracy: 0.7553\n",
            "Epoch 758/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7521 - val_loss: 0.5019 - val_accuracy: 0.7546\n",
            "Epoch 759/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7521 - val_loss: 0.5019 - val_accuracy: 0.7543\n",
            "Epoch 760/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7521 - val_loss: 0.5019 - val_accuracy: 0.7551\n",
            "Epoch 761/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7522 - val_loss: 0.5019 - val_accuracy: 0.7545\n",
            "Epoch 762/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7523 - val_loss: 0.5019 - val_accuracy: 0.7541\n",
            "Epoch 763/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7524 - val_loss: 0.5019 - val_accuracy: 0.7545\n",
            "Epoch 764/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7525 - val_loss: 0.5019 - val_accuracy: 0.7547\n",
            "Epoch 765/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7519 - val_loss: 0.5019 - val_accuracy: 0.7551\n",
            "Epoch 766/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7520 - val_loss: 0.5019 - val_accuracy: 0.7544\n",
            "Epoch 767/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7525 - val_loss: 0.5019 - val_accuracy: 0.7548\n",
            "Epoch 768/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7521 - val_loss: 0.5019 - val_accuracy: 0.7544\n",
            "Epoch 769/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7519 - val_loss: 0.5019 - val_accuracy: 0.7549\n",
            "Epoch 770/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7522 - val_loss: 0.5019 - val_accuracy: 0.7556\n",
            "Epoch 771/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7525 - val_loss: 0.5019 - val_accuracy: 0.7548\n",
            "Epoch 772/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7521 - val_loss: 0.5019 - val_accuracy: 0.7552\n",
            "Epoch 773/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7525 - val_loss: 0.5019 - val_accuracy: 0.7542\n",
            "Epoch 774/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7522 - val_loss: 0.5019 - val_accuracy: 0.7544\n",
            "Epoch 775/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7524 - val_loss: 0.5019 - val_accuracy: 0.7550\n",
            "Epoch 776/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7522 - val_loss: 0.5019 - val_accuracy: 0.7549\n",
            "Epoch 777/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7524 - val_loss: 0.5019 - val_accuracy: 0.7552\n",
            "Epoch 778/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7521 - val_loss: 0.5019 - val_accuracy: 0.7549\n",
            "Epoch 779/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7525 - val_loss: 0.5019 - val_accuracy: 0.7552\n",
            "Epoch 780/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7523 - val_loss: 0.5019 - val_accuracy: 0.7544\n",
            "Epoch 781/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7524 - val_loss: 0.5019 - val_accuracy: 0.7545\n",
            "Epoch 782/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7524 - val_loss: 0.5019 - val_accuracy: 0.7542\n",
            "Epoch 783/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7522 - val_loss: 0.5019 - val_accuracy: 0.7546\n",
            "Epoch 784/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7523 - val_loss: 0.5019 - val_accuracy: 0.7551\n",
            "Epoch 785/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7523 - val_loss: 0.5019 - val_accuracy: 0.7539\n",
            "Epoch 786/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7523 - val_loss: 0.5019 - val_accuracy: 0.7548\n",
            "Epoch 787/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7523 - val_loss: 0.5019 - val_accuracy: 0.7549\n",
            "Epoch 788/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7524 - val_loss: 0.5019 - val_accuracy: 0.7549\n",
            "Epoch 789/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7523 - val_loss: 0.5019 - val_accuracy: 0.7550\n",
            "Epoch 790/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7526 - val_loss: 0.5019 - val_accuracy: 0.7544\n",
            "Epoch 791/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7523 - val_loss: 0.5019 - val_accuracy: 0.7544\n",
            "Epoch 792/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7521 - val_loss: 0.5019 - val_accuracy: 0.7546\n",
            "Epoch 793/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7521 - val_loss: 0.5019 - val_accuracy: 0.7559\n",
            "Epoch 794/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7524 - val_loss: 0.5019 - val_accuracy: 0.7547\n",
            "Epoch 795/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7525 - val_loss: 0.5019 - val_accuracy: 0.7548\n",
            "Epoch 796/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7523 - val_loss: 0.5019 - val_accuracy: 0.7551\n",
            "Epoch 797/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7523 - val_loss: 0.5019 - val_accuracy: 0.7547\n",
            "Epoch 798/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7525 - val_loss: 0.5018 - val_accuracy: 0.7541\n",
            "Epoch 799/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7547\n",
            "Epoch 800/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7546\n",
            "Epoch 801/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7525 - val_loss: 0.5019 - val_accuracy: 0.7549\n",
            "Epoch 802/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7526 - val_loss: 0.5019 - val_accuracy: 0.7554\n",
            "Epoch 803/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7525 - val_loss: 0.5019 - val_accuracy: 0.7545\n",
            "Epoch 804/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7525 - val_loss: 0.5018 - val_accuracy: 0.7549\n",
            "Epoch 805/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7524 - val_loss: 0.5019 - val_accuracy: 0.7545\n",
            "Epoch 806/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7526 - val_loss: 0.5019 - val_accuracy: 0.7549\n",
            "Epoch 807/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7528 - val_loss: 0.5018 - val_accuracy: 0.7547\n",
            "Epoch 808/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7547\n",
            "Epoch 809/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7540\n",
            "Epoch 810/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7525 - val_loss: 0.5018 - val_accuracy: 0.7545\n",
            "Epoch 811/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7525 - val_loss: 0.5018 - val_accuracy: 0.7543\n",
            "Epoch 812/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7522 - val_loss: 0.5018 - val_accuracy: 0.7547\n",
            "Epoch 813/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7523 - val_loss: 0.5018 - val_accuracy: 0.7544\n",
            "Epoch 814/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7526 - val_loss: 0.5018 - val_accuracy: 0.7545\n",
            "Epoch 815/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7547\n",
            "Epoch 816/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7548\n",
            "Epoch 817/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7525 - val_loss: 0.5018 - val_accuracy: 0.7544\n",
            "Epoch 818/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7523 - val_loss: 0.5018 - val_accuracy: 0.7549\n",
            "Epoch 819/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7526 - val_loss: 0.5018 - val_accuracy: 0.7545\n",
            "Epoch 820/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7523 - val_loss: 0.5018 - val_accuracy: 0.7548\n",
            "Epoch 821/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7547\n",
            "Epoch 822/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7523 - val_loss: 0.5018 - val_accuracy: 0.7549\n",
            "Epoch 823/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7525 - val_loss: 0.5018 - val_accuracy: 0.7542\n",
            "Epoch 824/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7525 - val_loss: 0.5018 - val_accuracy: 0.7547\n",
            "Epoch 825/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7523 - val_loss: 0.5018 - val_accuracy: 0.7546\n",
            "Epoch 826/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7525 - val_loss: 0.5018 - val_accuracy: 0.7544\n",
            "Epoch 827/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7543\n",
            "Epoch 828/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7546\n",
            "Epoch 829/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7526 - val_loss: 0.5018 - val_accuracy: 0.7544\n",
            "Epoch 830/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7549\n",
            "Epoch 831/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7526 - val_loss: 0.5018 - val_accuracy: 0.7542\n",
            "Epoch 832/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7527 - val_loss: 0.5018 - val_accuracy: 0.7545\n",
            "Epoch 833/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7525 - val_loss: 0.5018 - val_accuracy: 0.7542\n",
            "Epoch 834/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7525 - val_loss: 0.5018 - val_accuracy: 0.7549\n",
            "Epoch 835/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7549\n",
            "Epoch 836/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7528 - val_loss: 0.5018 - val_accuracy: 0.7542\n",
            "Epoch 837/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7527 - val_loss: 0.5018 - val_accuracy: 0.7542\n",
            "Epoch 838/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7542\n",
            "Epoch 839/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7526 - val_loss: 0.5018 - val_accuracy: 0.7540\n",
            "Epoch 840/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7521 - val_loss: 0.5018 - val_accuracy: 0.7543\n",
            "Epoch 841/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7523 - val_loss: 0.5018 - val_accuracy: 0.7542\n",
            "Epoch 842/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7526 - val_loss: 0.5018 - val_accuracy: 0.7547\n",
            "Epoch 843/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7526 - val_loss: 0.5018 - val_accuracy: 0.7549\n",
            "Epoch 844/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7526 - val_loss: 0.5018 - val_accuracy: 0.7541\n",
            "Epoch 845/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7548\n",
            "Epoch 846/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7539\n",
            "Epoch 847/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7548\n",
            "Epoch 848/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7523 - val_loss: 0.5018 - val_accuracy: 0.7544\n",
            "Epoch 849/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7542\n",
            "Epoch 850/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7525 - val_loss: 0.5018 - val_accuracy: 0.7539\n",
            "Epoch 851/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7545\n",
            "Epoch 852/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7527 - val_loss: 0.5018 - val_accuracy: 0.7543\n",
            "Epoch 853/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7526 - val_loss: 0.5018 - val_accuracy: 0.7542\n",
            "Epoch 854/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7527 - val_loss: 0.5018 - val_accuracy: 0.7543\n",
            "Epoch 855/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7528 - val_loss: 0.5018 - val_accuracy: 0.7542\n",
            "Epoch 856/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7527 - val_loss: 0.5018 - val_accuracy: 0.7547\n",
            "Epoch 857/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7526 - val_loss: 0.5018 - val_accuracy: 0.7540\n",
            "Epoch 858/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7546\n",
            "Epoch 859/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7527 - val_loss: 0.5018 - val_accuracy: 0.7542\n",
            "Epoch 860/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7528 - val_loss: 0.5018 - val_accuracy: 0.7544\n",
            "Epoch 861/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7526 - val_loss: 0.5017 - val_accuracy: 0.7543\n",
            "Epoch 862/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7527 - val_loss: 0.5018 - val_accuracy: 0.7539\n",
            "Epoch 863/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7524 - val_loss: 0.5018 - val_accuracy: 0.7542\n",
            "Epoch 864/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7527 - val_loss: 0.5017 - val_accuracy: 0.7543\n",
            "Epoch 865/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7527 - val_loss: 0.5018 - val_accuracy: 0.7549\n",
            "Epoch 866/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7526 - val_loss: 0.5017 - val_accuracy: 0.7543\n",
            "Epoch 867/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7541\n",
            "Epoch 868/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.7527 - val_loss: 0.5018 - val_accuracy: 0.7537\n",
            "Epoch 869/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7527 - val_loss: 0.5018 - val_accuracy: 0.7537\n",
            "Epoch 870/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7526 - val_loss: 0.5017 - val_accuracy: 0.7540\n",
            "Epoch 871/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7527 - val_loss: 0.5017 - val_accuracy: 0.7540\n",
            "Epoch 872/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7527 - val_loss: 0.5017 - val_accuracy: 0.7535\n",
            "Epoch 873/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7542\n",
            "Epoch 874/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7524 - val_loss: 0.5017 - val_accuracy: 0.7541\n",
            "Epoch 875/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7524 - val_loss: 0.5017 - val_accuracy: 0.7544\n",
            "Epoch 876/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7530 - val_loss: 0.5017 - val_accuracy: 0.7539\n",
            "Epoch 877/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7527 - val_loss: 0.5017 - val_accuracy: 0.7542\n",
            "Epoch 878/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7526 - val_loss: 0.5017 - val_accuracy: 0.7536\n",
            "Epoch 879/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7530 - val_loss: 0.5017 - val_accuracy: 0.7540\n",
            "Epoch 880/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.7527 - val_loss: 0.5017 - val_accuracy: 0.7539\n",
            "Epoch 881/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.7527 - val_loss: 0.5017 - val_accuracy: 0.7536\n",
            "Epoch 882/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7539\n",
            "Epoch 883/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.7527 - val_loss: 0.5017 - val_accuracy: 0.7546\n",
            "Epoch 884/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7539\n",
            "Epoch 885/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.7527 - val_loss: 0.5017 - val_accuracy: 0.7537\n",
            "Epoch 886/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.7524 - val_loss: 0.5017 - val_accuracy: 0.7537\n",
            "Epoch 887/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.7524 - val_loss: 0.5017 - val_accuracy: 0.7540\n",
            "Epoch 888/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7539\n",
            "Epoch 889/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7535\n",
            "Epoch 890/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7526 - val_loss: 0.5017 - val_accuracy: 0.7542\n",
            "Epoch 891/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7527 - val_loss: 0.5017 - val_accuracy: 0.7551\n",
            "Epoch 892/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7527 - val_loss: 0.5017 - val_accuracy: 0.7542\n",
            "Epoch 893/1000\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 0.5011 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7539\n",
            "Epoch 894/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7526 - val_loss: 0.5017 - val_accuracy: 0.7538\n",
            "Epoch 895/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7526 - val_loss: 0.5017 - val_accuracy: 0.7551\n",
            "Epoch 896/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7527 - val_loss: 0.5017 - val_accuracy: 0.7550\n",
            "Epoch 897/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7535\n",
            "Epoch 898/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7527 - val_loss: 0.5017 - val_accuracy: 0.7544\n",
            "Epoch 899/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7527 - val_loss: 0.5017 - val_accuracy: 0.7539\n",
            "Epoch 900/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7526 - val_loss: 0.5017 - val_accuracy: 0.7535\n",
            "Epoch 901/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7527 - val_loss: 0.5017 - val_accuracy: 0.7537\n",
            "Epoch 902/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7540\n",
            "Epoch 903/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7544\n",
            "Epoch 904/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7524 - val_loss: 0.5017 - val_accuracy: 0.7554\n",
            "Epoch 905/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7530 - val_loss: 0.5017 - val_accuracy: 0.7542\n",
            "Epoch 906/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7526 - val_loss: 0.5017 - val_accuracy: 0.7549\n",
            "Epoch 907/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7529 - val_loss: 0.5017 - val_accuracy: 0.7542\n",
            "Epoch 908/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7542\n",
            "Epoch 909/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7534\n",
            "Epoch 910/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7539\n",
            "Epoch 911/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7531 - val_loss: 0.5017 - val_accuracy: 0.7536\n",
            "Epoch 912/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7529 - val_loss: 0.5017 - val_accuracy: 0.7538\n",
            "Epoch 913/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7527 - val_loss: 0.5017 - val_accuracy: 0.7554\n",
            "Epoch 914/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7549\n",
            "Epoch 915/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7552\n",
            "Epoch 916/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7530 - val_loss: 0.5017 - val_accuracy: 0.7537\n",
            "Epoch 917/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7537\n",
            "Epoch 918/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7530 - val_loss: 0.5017 - val_accuracy: 0.7550\n",
            "Epoch 919/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7530 - val_loss: 0.5017 - val_accuracy: 0.7538\n",
            "Epoch 920/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7553\n",
            "Epoch 921/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7549\n",
            "Epoch 922/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7530 - val_loss: 0.5017 - val_accuracy: 0.7551\n",
            "Epoch 923/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7526 - val_loss: 0.5017 - val_accuracy: 0.7544\n",
            "Epoch 924/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7527 - val_loss: 0.5017 - val_accuracy: 0.7541\n",
            "Epoch 925/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.7529 - val_loss: 0.5017 - val_accuracy: 0.7540\n",
            "Epoch 926/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7530 - val_loss: 0.5017 - val_accuracy: 0.7544\n",
            "Epoch 927/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7532 - val_loss: 0.5017 - val_accuracy: 0.7535\n",
            "Epoch 928/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7542\n",
            "Epoch 929/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7530 - val_loss: 0.5017 - val_accuracy: 0.7534\n",
            "Epoch 930/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7530 - val_loss: 0.5017 - val_accuracy: 0.7537\n",
            "Epoch 931/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7531 - val_loss: 0.5017 - val_accuracy: 0.7544\n",
            "Epoch 932/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7529 - val_loss: 0.5017 - val_accuracy: 0.7547\n",
            "Epoch 933/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7530 - val_loss: 0.5017 - val_accuracy: 0.7549\n",
            "Epoch 934/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7533 - val_loss: 0.5017 - val_accuracy: 0.7539\n",
            "Epoch 935/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7528 - val_loss: 0.5016 - val_accuracy: 0.7547\n",
            "Epoch 936/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7529 - val_loss: 0.5017 - val_accuracy: 0.7543\n",
            "Epoch 937/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7530 - val_loss: 0.5017 - val_accuracy: 0.7546\n",
            "Epoch 938/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7530 - val_loss: 0.5017 - val_accuracy: 0.7547\n",
            "Epoch 939/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7529 - val_loss: 0.5016 - val_accuracy: 0.7544\n",
            "Epoch 940/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7537\n",
            "Epoch 941/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7528 - val_loss: 0.5016 - val_accuracy: 0.7547\n",
            "Epoch 942/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7530 - val_loss: 0.5017 - val_accuracy: 0.7544\n",
            "Epoch 943/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7531 - val_loss: 0.5017 - val_accuracy: 0.7547\n",
            "Epoch 944/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7529 - val_loss: 0.5017 - val_accuracy: 0.7550\n",
            "Epoch 945/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7542\n",
            "Epoch 946/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7532 - val_loss: 0.5017 - val_accuracy: 0.7549\n",
            "Epoch 947/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7531 - val_loss: 0.5017 - val_accuracy: 0.7544\n",
            "Epoch 948/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7532 - val_loss: 0.5017 - val_accuracy: 0.7549\n",
            "Epoch 949/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7532 - val_loss: 0.5016 - val_accuracy: 0.7542\n",
            "Epoch 950/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7529 - val_loss: 0.5016 - val_accuracy: 0.7536\n",
            "Epoch 951/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7532 - val_loss: 0.5017 - val_accuracy: 0.7544\n",
            "Epoch 952/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7532 - val_loss: 0.5017 - val_accuracy: 0.7545\n",
            "Epoch 953/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7533 - val_loss: 0.5017 - val_accuracy: 0.7547\n",
            "Epoch 954/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7529 - val_loss: 0.5016 - val_accuracy: 0.7540\n",
            "Epoch 955/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7529 - val_loss: 0.5017 - val_accuracy: 0.7537\n",
            "Epoch 956/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7544\n",
            "Epoch 957/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7531 - val_loss: 0.5016 - val_accuracy: 0.7544\n",
            "Epoch 958/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7528 - val_loss: 0.5017 - val_accuracy: 0.7544\n",
            "Epoch 959/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7532 - val_loss: 0.5016 - val_accuracy: 0.7545\n",
            "Epoch 960/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7532 - val_loss: 0.5016 - val_accuracy: 0.7546\n",
            "Epoch 961/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7530 - val_loss: 0.5016 - val_accuracy: 0.7545\n",
            "Epoch 962/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7530 - val_loss: 0.5016 - val_accuracy: 0.7539\n",
            "Epoch 963/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7531 - val_loss: 0.5016 - val_accuracy: 0.7543\n",
            "Epoch 964/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7532 - val_loss: 0.5016 - val_accuracy: 0.7542\n",
            "Epoch 965/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7531 - val_loss: 0.5016 - val_accuracy: 0.7546\n",
            "Epoch 966/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7532 - val_loss: 0.5016 - val_accuracy: 0.7546\n",
            "Epoch 967/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7531 - val_loss: 0.5016 - val_accuracy: 0.7548\n",
            "Epoch 968/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7530 - val_loss: 0.5016 - val_accuracy: 0.7539\n",
            "Epoch 969/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7531 - val_loss: 0.5016 - val_accuracy: 0.7547\n",
            "Epoch 970/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7530 - val_loss: 0.5016 - val_accuracy: 0.7542\n",
            "Epoch 971/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7530 - val_loss: 0.5017 - val_accuracy: 0.7546\n",
            "Epoch 972/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7534 - val_loss: 0.5016 - val_accuracy: 0.7548\n",
            "Epoch 973/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7534 - val_loss: 0.5016 - val_accuracy: 0.7547\n",
            "Epoch 974/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7529 - val_loss: 0.5016 - val_accuracy: 0.7545\n",
            "Epoch 975/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7530 - val_loss: 0.5016 - val_accuracy: 0.7544\n",
            "Epoch 976/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7535 - val_loss: 0.5016 - val_accuracy: 0.7535\n",
            "Epoch 977/1000\n",
            "111/111 [==============================] - 1s 5ms/step - loss: 0.5008 - accuracy: 0.7529 - val_loss: 0.5016 - val_accuracy: 0.7549\n",
            "Epoch 978/1000\n",
            "111/111 [==============================] - 1s 10ms/step - loss: 0.5008 - accuracy: 0.7532 - val_loss: 0.5016 - val_accuracy: 0.7539\n",
            "Epoch 979/1000\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 0.5007 - accuracy: 0.7529 - val_loss: 0.5016 - val_accuracy: 0.7544\n",
            "Epoch 980/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7531 - val_loss: 0.5016 - val_accuracy: 0.7548\n",
            "Epoch 981/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7533 - val_loss: 0.5016 - val_accuracy: 0.7547\n",
            "Epoch 982/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7529 - val_loss: 0.5016 - val_accuracy: 0.7545\n",
            "Epoch 983/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7533 - val_loss: 0.5016 - val_accuracy: 0.7547\n",
            "Epoch 984/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7531 - val_loss: 0.5016 - val_accuracy: 0.7537\n",
            "Epoch 985/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7533 - val_loss: 0.5016 - val_accuracy: 0.7537\n",
            "Epoch 986/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7529 - val_loss: 0.5016 - val_accuracy: 0.7547\n",
            "Epoch 987/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7534 - val_loss: 0.5016 - val_accuracy: 0.7544\n",
            "Epoch 988/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7532 - val_loss: 0.5016 - val_accuracy: 0.7545\n",
            "Epoch 989/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7536 - val_loss: 0.5016 - val_accuracy: 0.7549\n",
            "Epoch 990/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7529 - val_loss: 0.5016 - val_accuracy: 0.7548\n",
            "Epoch 991/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7533 - val_loss: 0.5016 - val_accuracy: 0.7537\n",
            "Epoch 992/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7533 - val_loss: 0.5016 - val_accuracy: 0.7538\n",
            "Epoch 993/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7528 - val_loss: 0.5016 - val_accuracy: 0.7542\n",
            "Epoch 994/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7532 - val_loss: 0.5016 - val_accuracy: 0.7547\n",
            "Epoch 995/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7534 - val_loss: 0.5016 - val_accuracy: 0.7548\n",
            "Epoch 996/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7533 - val_loss: 0.5016 - val_accuracy: 0.7545\n",
            "Epoch 997/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7531 - val_loss: 0.5016 - val_accuracy: 0.7539\n",
            "Epoch 998/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7531 - val_loss: 0.5016 - val_accuracy: 0.7548\n",
            "Epoch 999/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7535 - val_loss: 0.5016 - val_accuracy: 0.7544\n",
            "Epoch 1000/1000\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.7533 - val_loss: 0.5016 - val_accuracy: 0.7537\n",
            "1768/1768 [==============================] - 3s 2ms/step - loss: 0.5006 - accuracy: 0.7527\n",
            "Training Accuracy: 0.7526921629905701\n",
            "442/442 [==============================] - 1s 2ms/step - loss: 0.5016 - accuracy: 0.7537\n",
            "Testing Accuracy: 0.753730833530426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3.3 BPNN via Pytorch"
      ],
      "metadata": {
        "id": "ZPtiChZRgIgA"
      },
      "id": "ZPtiChZRgIgA"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "DX3WVUUmp5KH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d65407-f4b8-4508-bb43-2ee2482aaf6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs [200/1000]\n",
            "Training: Losses: 0.5478307009, Accuracy: 0.748046875\n",
            "Testing: Losses: 0.5504439473, Accuracy: 0.7485677912157861\n",
            "Testing: Sensitivity/Recall: 0.8035890914229193 Specificity: 0.6934296233361654\n",
            "Epochs [400/1000]\n",
            "Training: Losses: 0.5454199910, Accuracy: 0.740234375\n",
            "Testing: Losses: 0.5479124784, Accuracy: 0.7501944974892142\n",
            "Testing: Sensitivity/Recall: 0.8028825773632895 Specificity: 0.6973945058057207\n",
            "Epochs [600/1000]\n",
            "Training: Losses: 0.5530868769, Accuracy: 0.72265625\n",
            "Testing: Losses: 0.5449438095, Accuracy: 0.7527406464389278\n",
            "Testing: Sensitivity/Recall: 0.8004804295605482 Specificity: 0.7048994619088077\n",
            "Epochs [800/1000]\n",
            "Training: Losses: 0.5692577362, Accuracy: 0.724609375\n",
            "Testing: Losses: 0.5435369015, Accuracy: 0.7531650045972134\n",
            "Testing: Sensitivity/Recall: 0.7980782817578069 Specificity: 0.7081563296516568\n",
            "Epochs [1000/1000]\n",
            "Training: Losses: 0.5494416952, Accuracy: 0.74609375\n",
            "Testing: Losses: 0.5429655313, Accuracy: 0.7542966263526416\n",
            "Testing: Sensitivity/Recall: 0.7996326126889926 Specificity: 0.7088643443783631\n"
          ]
        }
      ],
      "source": [
        "# Prepare training and testing data\n",
        "train_x, test_x, train_y, test_y = data_split(cdc_df)\n",
        "\n",
        "\n",
        "train_x_scaled, test_x_scaled = data_scaling(train_x, test_x)\n",
        "\n",
        "\n",
        "train_x = torch.from_numpy(train_x_scaled).float()\n",
        "train_y = torch.squeeze(torch.from_numpy(train_y.to_numpy()).float())\n",
        "\n",
        "test_x = torch.from_numpy(test_x_scaled).float()\n",
        "test_y = torch.squeeze(torch.from_numpy(test_y.to_numpy()).float())\n",
        "\n",
        "# print(train_x.shape, train_y.shape)\n",
        "# print(test_x.shape, test_y.shape)\n",
        "\n",
        "\n",
        "# Building BPNN\n",
        "class bpnn(nn.Module):\n",
        "  # Initialize the neural network with three fully connected hidden layers\n",
        "  def __init__(self, input, hid1, hid2, numclases):\n",
        "    super(bpnn, self).__init__()\n",
        "    # an affine operation: y = Wx + b\n",
        "    self.fc1 = nn.Linear(input, hid1)\n",
        "    self.fc2 = nn.Linear(hid1, hid2)\n",
        "    self.fc3 = nn.Linear(hid2, numclass)\n",
        "  # feedforward\n",
        "  def forward(self, x):\n",
        "    x = F.sigmoid(self.fc1(x)) # Activation using sigmoid function for regularization\n",
        "    x = F.sigmoid(self.fc2(x))\n",
        "    x = F.sigmoid(self.fc3(x))\n",
        "    return x\n",
        "\n",
        "input = train_x.shape[1] \n",
        "hid1 = 16\n",
        "hid2 = 8\n",
        "numclass = 2\n",
        "epochs = 1000\n",
        "batch = 512\n",
        "\n",
        "modl = bpnn(input, hid1, hid2, numclass)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.BCELoss()\n",
        "optim = torch.optim.Adam(modl.parameters(), lr=0.0001)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  shuffle_indices = np.random.permutation(np.arange(train_y.shape[0]))\n",
        "  source = train_x[shuffle_indices]\n",
        "  target = train_y[shuffle_indices]\n",
        "\n",
        "  for batch_i in range(0, len(source)//batch):\n",
        "    start_i = batch_i * batch\n",
        "    source_batch = source[start_i:start_i + batch]\n",
        "    target_batch = target[start_i:start_i + batch]\n",
        "    train_pred = modl(source_batch)\n",
        "    train_pred = torch.squeeze(train_pred)\n",
        "    losses = criterion(train_pred, target_batch.type(torch.LongTensor))\n",
        "    optim.zero_grad()\n",
        "    losses.backward()\n",
        "    optim.step() # update weights and bias\n",
        "\n",
        "  if (epoch+1) % 200 == 0:\n",
        "    train_pred_np = torch.argmax(train_pred, axis=1).numpy()\n",
        "    train_acc = accuracy_score(target_batch, train_pred_np)\n",
        "\n",
        "    test_pred = modl(test_x)\n",
        "    test_pred = torch.squeeze(test_pred)\n",
        "\n",
        "    test_loss = criterion(test_pred, test_y.type(torch.LongTensor))\n",
        "\n",
        "    test_pred_np = torch.argmax(test_pred, axis=1).numpy()\n",
        "    test_acc = accuracy_score(test_y, test_pred_np)\n",
        "    tn, fp, fn, tp = confusion_matrix(test_y, test_pred_np).ravel()\n",
        "\n",
        "\n",
        "    print (f'Epochs [{epoch+1}/{epochs}]')\n",
        "    print (f'Training: Losses: {losses.item():.10f}, Accuracy: {train_acc}')\n",
        "    print (f'Testing: Losses: {test_loss.item():.10f}, Accuracy: {test_acc}')\n",
        "    print(\"Testing: Sensitivity/Recall:\", tp/(tp+fn), \"Specificity:\", tn/(tn+fp))\n",
        "\n",
        "\n"
      ],
      "id": "DX3WVUUmp5KH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3.4 Dropout to improve Accuracy\n",
        "\n",
        "Dropout drops a certain amount of data randomly, result in decreasing of Training Acc, in order to avoid overfitting."
      ],
      "metadata": {
        "id": "LXgXax2b8Vjd"
      },
      "id": "LXgXax2b8Vjd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare training and testing data\n",
        "train_x, test_x, train_y, test_y = data_split(cdc_df)\n",
        "\n",
        "\n",
        "train_x_scaled, test_x_scaled = data_scaling(train_x, test_x)\n",
        "\n",
        "\n",
        "train_x = torch.from_numpy(train_x_scaled).float()\n",
        "train_y = torch.squeeze(torch.from_numpy(train_y.to_numpy()).float())\n",
        "\n",
        "test_x = torch.from_numpy(test_x_scaled).float()\n",
        "test_y = torch.squeeze(torch.from_numpy(test_y.to_numpy()).float())\n",
        "\n",
        "# print(train_x.shape, train_y.shape)\n",
        "# print(test_x.shape, test_y.shape)\n",
        "\n",
        "\n",
        "# Building BPNN\n",
        "class bpnn(nn.Module):\n",
        "  # Initialize the neural network with three fully connected hidden layers\n",
        "  def __init__(self, input, hid1, hid2, numclases):\n",
        "    super(bpnn, self).__init__()\n",
        "    # an affine operation: y = Wx + b\n",
        "    self.fc1 = nn.Linear(input, hid1)\n",
        "    self.dropout1 = nn.Dropout(0.1)\n",
        "    self.fc2 = nn.Linear(hid1, hid2)\n",
        "    self.dropout2 = nn.Dropout(0.1)\n",
        "    self.fc3 = nn.Linear(hid2, numclass)\n",
        "  # feedforward\n",
        "  def forward(self, x):\n",
        "    x = F.sigmoid(self.fc1(x)) # Activation using sigmoid function for regularization\n",
        "    x = self.dropout1(x)\n",
        "    x = F.sigmoid(self.fc2(x))\n",
        "    x = self.dropout2(x)\n",
        "    x = F.sigmoid(self.fc3(x))\n",
        "    return x\n",
        "\n",
        "input = train_x.shape[1] \n",
        "hid1 = 64\n",
        "hid2 = 32\n",
        "numclass = 2\n",
        "epochs = 1000\n",
        "batch = 512\n",
        "\n",
        "modl = bpnn(input, hid1, hid2, numclass)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.BCELoss()\n",
        "optim = torch.optim.Adam(modl.parameters(), lr=0.0001)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  shuffle_indices = np.random.permutation(np.arange(train_y.shape[0]))\n",
        "  source = train_x[shuffle_indices]\n",
        "  target = train_y[shuffle_indices]\n",
        "\n",
        "  for batch_i in range(0, len(source)//batch):\n",
        "    start_i = batch_i * batch\n",
        "    source_batch = source[start_i:start_i + batch]\n",
        "    target_batch = target[start_i:start_i + batch]\n",
        "    train_pred = modl(source_batch)\n",
        "    train_pred = torch.squeeze(train_pred)\n",
        "    losses = criterion(train_pred, target_batch.type(torch.LongTensor))\n",
        "    optim.zero_grad()\n",
        "    losses.backward()\n",
        "    optim.step() # update weights and bias\n",
        "\n",
        "  if (epoch+1) % 200 == 0:\n",
        "    train_pred_np = torch.argmax(train_pred, axis=1).numpy()\n",
        "    train_acc = accuracy_score(target_batch, train_pred_np)\n",
        "\n",
        "    test_pred = modl(test_x)\n",
        "    test_pred = torch.squeeze(test_pred)\n",
        "\n",
        "    test_loss = criterion(test_pred, test_y.type(torch.LongTensor))\n",
        "\n",
        "    test_pred_np = torch.argmax(test_pred, axis=1).numpy()\n",
        "    test_acc = accuracy_score(test_y, test_pred_np)\n",
        "\n",
        "\n",
        "    print (f'Epochs [{epoch+1}/{epochs}]')\n",
        "    print (f'Training: Losses: {losses.item():.10f}, Accuracy: {train_acc}')\n",
        "    print (f'Testing: Losses: {test_loss.item():.10f}, Accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "znx-o59g8b0h"
      },
      "id": "znx-o59g8b0h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3.5 Batch Normalization\n",
        "\n",
        "Normalize the output of activation function"
      ],
      "metadata": {
        "id": "r8QjFKJZJxPg"
      },
      "id": "r8QjFKJZJxPg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare training and testing data\n",
        "train_x, test_x, train_y, test_y = data_split(cdc_df)\n",
        "\n",
        "\n",
        "train_x_scaled, test_x_scaled = data_scaling(train_x, test_x)\n",
        "\n",
        "\n",
        "train_x = torch.from_numpy(train_x_scaled).float()\n",
        "train_y = torch.squeeze(torch.from_numpy(train_y.to_numpy()).float())\n",
        "\n",
        "test_x = torch.from_numpy(test_x_scaled).float()\n",
        "test_y = torch.squeeze(torch.from_numpy(test_y.to_numpy()).float())\n",
        "\n",
        "# print(train_x.shape, train_y.shape)\n",
        "# print(test_x.shape, test_y.shape)\n",
        "\n",
        "\n",
        "# Building BPNN\n",
        "class bpnn(nn.Module):\n",
        "  # Initialize the neural network with three fully connected hidden layers\n",
        "  def __init__(self, input, hid1, hid2, numclases):\n",
        "    super(bpnn, self).__init__()\n",
        "    # an affine operation: y = Wx + b\n",
        "    self.fc1 = nn.Linear(input, hid1)\n",
        "    self.batch_norm1 = nn.BatchNorm1d(num_features=hid1)\n",
        "    self.fc2 = nn.Linear(hid1, hid2)\n",
        "    self.batch_norm2 = nn.BatchNorm1d(num_features=hid2)\n",
        "    self.fc3 = nn.Linear(hid2, numclass)\n",
        "  # feedforward\n",
        "  def forward(self, x):\n",
        "    x = F.sigmoid(self.fc1(x)) # Activation using sigmoid function for regularization\n",
        "    x = self.batch_norm1(x)\n",
        "    x = F.sigmoid(self.fc2(x))\n",
        "    x = self.batch_norm2(x)\n",
        "    x = F.sigmoid(self.fc3(x))\n",
        "    return x\n",
        "\n",
        "input = train_x.shape[1] \n",
        "hid1 = 16\n",
        "hid2 = 8\n",
        "numclass = 2\n",
        "epochs = 1000\n",
        "batch = 512\n",
        "\n",
        "modl = bpnn(input, hid1, hid2, numclass)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.BCELoss()\n",
        "optim = torch.optim.Adam(modl.parameters(), lr=0.0001)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  shuffle_indices = np.random.permutation(np.arange(train_y.shape[0]))\n",
        "  source = train_x[shuffle_indices]\n",
        "  target = train_y[shuffle_indices]\n",
        "\n",
        "  for batch_i in range(0, len(source)//batch):\n",
        "    start_i = batch_i * batch\n",
        "    source_batch = source[start_i:start_i + batch]\n",
        "    target_batch = target[start_i:start_i + batch]\n",
        "    train_pred = modl(source_batch)\n",
        "    train_pred = torch.squeeze(train_pred)\n",
        "    losses = criterion(train_pred, target_batch.type(torch.LongTensor))\n",
        "    optim.zero_grad()\n",
        "    losses.backward()\n",
        "    optim.step() # update weights and bias\n",
        "\n",
        "  if (epoch+1) % 200 == 0:\n",
        "    train_pred_np = torch.argmax(train_pred, axis=1).numpy()\n",
        "    train_acc = accuracy_score(target_batch, train_pred_np)\n",
        "\n",
        "    test_pred = modl(test_x)\n",
        "    test_pred = torch.squeeze(test_pred)\n",
        "\n",
        "    test_loss = criterion(test_pred, test_y.type(torch.LongTensor))\n",
        "\n",
        "    test_pred_np = torch.argmax(test_pred, axis=1).numpy()\n",
        "    test_acc = accuracy_score(test_y, test_pred_np)\n",
        "    tn, fp, fn, tp = confusion_matrix(test_y, test_pred_np).ravel()\n",
        "\n",
        "    print (f'Epochs [{epoch+1}/{epochs}]')\n",
        "    print (f'Training: Losses: {losses.item():.10f}, Accuracy: {train_acc}')\n",
        "    print (f'Testing: Losses: {test_loss.item():.10f}, Accuracy: {test_acc}')\n",
        "    print(\"Testing: Sensitivity/Recall:\", tp/(tp+fn), \"Specificity:\", tn/(tn+fp))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x89Aekik5z_b",
        "outputId": "8d647177-52f6-4cf2-b83a-56f6664f8749"
      },
      "id": "x89Aekik5z_b",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs [200/1000]\n",
            "Training: Losses: 0.5298256874, Accuracy: 0.7734375\n",
            "Testing: Losses: 0.5435692668, Accuracy: 0.7535186363957848\n",
            "Testing: Sensitivity/Recall: 0.8026033471606351 Specificity: 0.705512031337437\n",
            "Epochs [400/1000]\n",
            "Training: Losses: 0.5687990785, Accuracy: 0.720703125\n",
            "Testing: Losses: 0.5432581306, Accuracy: 0.7555697008274984\n",
            "Testing: Sensitivity/Recall: 0.8030324703189815 Specificity: 0.7091494124230554\n",
            "Epochs [600/1000]\n",
            "Training: Losses: 0.5346801877, Accuracy: 0.763671875\n",
            "Testing: Losses: 0.5432820916, Accuracy: 0.7542258999929273\n",
            "Testing: Sensitivity/Recall: 0.8026033471606351 Specificity: 0.7069110240626749\n",
            "Epochs [800/1000]\n",
            "Training: Losses: 0.5485797524, Accuracy: 0.7578125\n",
            "Testing: Losses: 0.5431126356, Accuracy: 0.755711153546927\n",
            "Testing: Sensitivity/Recall: 0.8033185524245459 Specificity: 0.7091494124230554\n",
            "Epochs [1000/1000]\n",
            "Training: Losses: 0.5243884921, Accuracy: 0.78515625\n",
            "Testing: Losses: 0.5431408286, Accuracy: 0.7562769644246411\n",
            "Testing: Sensitivity/Recall: 0.8013159776855958 Specificity: 0.7122271964185787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3.6 Data Cleaning\n",
        "\n",
        "We only use the features of which `|corr| >= 0.2` for training and testing.\n",
        "\n",
        "| Features    | Corr   |\n",
        "| :------------- | :------------- |\n",
        "|GenHlth        |         0.407130\n",
        "|HighBP         |         0.376526\n",
        "|HighChol         |       0.286841\n",
        "|BMI         |            0.285256\n",
        "|Age         |            0.274125\n",
        "|DiffWalk       |         0.268147\n",
        "|PhysHlth        |        0.212632\n",
        "|HeartDiseaseorAttack  |  0.209694\n",
        "|Income        |         -0.228165\n",
        "|Diabetes_binary    |     1.000000"
      ],
      "metadata": {
        "id": "ympHQtV3MNlG"
      },
      "id": "ympHQtV3MNlG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare training and testing data\n",
        "\n",
        "cdc_df = cdc_df[['GenHlth','HighBP','HighChol','BMI','Age','DiffWalk','PhysHlth','HeartDiseaseorAttack','Income','Diabetes_binary']]\n",
        "\n",
        "train_x, test_x, train_y, test_y = data_split(cdc_df)\n",
        "\n",
        "\n",
        "train_x_scaled, test_x_scaled = data_scaling(train_x, test_x)\n",
        "\n",
        "\n",
        "train_x = torch.from_numpy(train_x_scaled).float()\n",
        "train_y = torch.squeeze(torch.from_numpy(train_y.to_numpy()).float())\n",
        "\n",
        "test_x = torch.from_numpy(test_x_scaled).float()\n",
        "test_y = torch.squeeze(torch.from_numpy(test_y.to_numpy()).float())\n",
        "\n",
        "# print(train_x.shape, train_y.shape)\n",
        "# print(test_x.shape, test_y.shape)\n",
        "\n",
        "\n",
        "# Building BPNN\n",
        "class bpnn(nn.Module):\n",
        "  # Initialize the neural network with three fully connected hidden layers\n",
        "  def __init__(self, input, hid1, hid2, numclases):\n",
        "    super(bpnn, self).__init__()\n",
        "    # an affine operation: y = Wx + b\n",
        "    self.fc1 = nn.Linear(input, hid1)\n",
        "    self.fc2 = nn.Linear(hid1, hid2)\n",
        "    self.fc3 = nn.Linear(hid2, numclass)\n",
        "  # feedforward\n",
        "  def forward(self, x):\n",
        "    x = F.sigmoid(self.fc1(x)) # Activation using sigmoid function for regularization\n",
        "    x = F.sigmoid(self.fc2(x))\n",
        "    x = F.sigmoid(self.fc3(x))\n",
        "    return x\n",
        "\n",
        "input = train_x.shape[1] \n",
        "hid1 = 16\n",
        "hid2 = 8\n",
        "numclass = 2\n",
        "epochs = 1000\n",
        "batch = 512\n",
        "\n",
        "modl = bpnn(input, hid1, hid2, numclass)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.BCELoss()\n",
        "optim = torch.optim.Adam(modl.parameters(), lr=0.0003)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  shuffle_indices = np.random.permutation(np.arange(train_y.shape[0]))\n",
        "  source = train_x[shuffle_indices]\n",
        "  target = train_y[shuffle_indices]\n",
        "\n",
        "  for batch_i in range(0, len(source)//batch):\n",
        "    start_i = batch_i * batch\n",
        "    source_batch = source[start_i:start_i + batch]\n",
        "    target_batch = target[start_i:start_i + batch]\n",
        "    train_pred = modl(source_batch)\n",
        "    train_pred = torch.squeeze(train_pred)\n",
        "    losses = criterion(train_pred, target_batch.type(torch.LongTensor))\n",
        "    optim.zero_grad()\n",
        "    losses.backward()\n",
        "    optim.step() # update weights and bias\n",
        "\n",
        "  if (epoch+1) % 200 == 0:\n",
        "    train_pred_np = torch.argmax(train_pred, axis=1).numpy()\n",
        "    train_acc = accuracy_score(target_batch, train_pred_np)\n",
        "\n",
        "    test_pred = modl(test_x)\n",
        "    test_pred = torch.squeeze(test_pred)\n",
        "\n",
        "    test_loss = criterion(test_pred, test_y.type(torch.LongTensor))\n",
        "\n",
        "    test_pred_np = torch.argmax(test_pred, axis=1).numpy()\n",
        "    test_acc = accuracy_score(test_y, test_pred_np)\n",
        "    tn, fp, fn, tp = confusion_matrix(test_y, test_pred_np).ravel()\n",
        "\n",
        "\n",
        "    print (f'Epochs [{epoch+1}/{epochs}]')\n",
        "    print (f'Training: Losses: {losses.item():.10f}, Accuracy: {train_acc}')\n",
        "    print (f'Testing: Losses: {test_loss.item():.10f}, Accuracy: {test_acc}')\n",
        "    print(\"Testing: Sensitivity/Recall:\", tp/(tp+fn), \"Specificity:\", tn/(tn+fp))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnIQYoZDF6vm",
        "outputId": "e4a836b0-7a2d-4b9f-c252-c998df8d14b2"
      },
      "id": "tnIQYoZDF6vm",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs [200/1000]\n",
            "Training: Losses: 0.5322936773, Accuracy: 0.775390625\n",
            "Testing: Losses: 0.5474342704, Accuracy: 0.7489921493740718\n",
            "Testing: Sensitivity/Recall: 0.8016020597911601 Specificity: 0.6975377728035814\n",
            "Epochs [400/1000]\n",
            "Training: Losses: 0.5163312554, Accuracy: 0.79296875\n",
            "Testing: Losses: 0.5462947488, Accuracy: 0.7488506966546432\n",
            "Testing: Sensitivity/Recall: 0.800600772421685 Specificity: 0.6982372691662003\n",
            "Epochs [600/1000]\n",
            "Training: Losses: 0.5698907971, Accuracy: 0.716796875\n",
            "Testing: Losses: 0.5459094644, Accuracy: 0.7488506966546432\n",
            "Testing: Sensitivity/Recall: 0.8027463882134173 Specificity: 0.6961387800783436\n",
            "Epochs [800/1000]\n",
            "Training: Losses: 0.5471276641, Accuracy: 0.751953125\n",
            "Testing: Losses: 0.5457747579, Accuracy: 0.7479312539783577\n",
            "Testing: Sensitivity/Recall: 0.8011729366328136 Specificity: 0.6958589815332961\n",
            "Epochs [1000/1000]\n",
            "Training: Losses: 0.5404641032, Accuracy: 0.748046875\n",
            "Testing: Losses: 0.5458907485, Accuracy: 0.7490628757337859\n",
            "Testing: Sensitivity/Recall: 0.8046059218995851 Specificity: 0.6947397873531057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hence, we have the outcome comparsion**\n",
        "\n",
        "| Classifier    | Training Acc    | Testing Acc|Recall|Specificity|\n",
        "| :------------- | :------------- | :------------- | :------------- | :------------- |\n",
        "| KNN with PCA | 0.7400 | 0.7431| 0.8038|0.6818\n",
        "| Logistic Regression | 0.7448 | 0.7481 |0.7728|0.7237\n",
        "| BPNN| 0.7460 | 0.7542 |0.7996|0.7089\n",
        "| BPNN with Batch Normalization| 0.7852 | 0.7563 |0.8013|0.7122\n",
        "| BPNN with Data Cleaning| 0.7480 | 0.7490 |0.8046|0.6947"
      ],
      "metadata": {
        "id": "YK87I3nuLn5r"
      },
      "id": "YK87I3nuLn5r"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8497a0d1-8255-4980-ab6b-5a70211dd0da"
      },
      "source": [
        "-----\n",
        "## 2. NIDDK Pima Indians Diabetes Database\n",
        "\n",
        "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
        "\n",
        "####  Attribute Information\n",
        "| Column Name    | Expression    |\n",
        "| :------------- | :------------- |\n",
        "| Pregnancies | Number of times pregnant |\n",
        "| Glucose | Plasma glucose concentration a 2 hours in an oral glucose tolerance test |\n",
        "| BloodPressure | Diastolic blood pressure (mm Hg) |\n",
        "| SkinThickness | Triceps skin fold thickness (mm) |\n",
        "| Insulin | 2-Hour serum insulin (mu U/ml) |\n",
        "| BMI | Body mass index (weight in kg/(height in m)^2) |\n",
        "| DiabetesPedigreeFunction | Diabetes pedigree function |\n",
        "| Age | Age (years) |\n",
        "| Outcome | Class variable (0 or 1) |\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "8497a0d1-8255-4980-ab6b-5a70211dd0da"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "1481da9e-3597-47b0-81b5-1a1a6d1e1d37",
        "outputId": "1b5153f8-0fd1-4afa-d43d-c8f5444870a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Any null value: True\n",
            "Any NaN value: True\n",
            "Before Droping NaN Number of Rows: 768\n",
            "After Droping NaN Number of Rows: 768\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "763           10      101             76             48      180  32.9   \n",
              "764            2      122             70             27        0  36.8   \n",
              "765            5      121             72             23      112  26.2   \n",
              "766            1      126             60              0        0  30.1   \n",
              "767            1       93             70             31        0  30.4   \n",
              "\n",
              "     DiabetesPedigreeFunction  Age  Outcome  \n",
              "763                     0.171   63        0  \n",
              "764                     0.340   27        0  \n",
              "765                     0.245   30        0  \n",
              "766                     0.349   47        1  \n",
              "767                     0.315   23        0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-262056db-dad9-4736-a2c8-3e78b291a068\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10</td>\n",
              "      <td>101</td>\n",
              "      <td>76</td>\n",
              "      <td>48</td>\n",
              "      <td>180</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2</td>\n",
              "      <td>122</td>\n",
              "      <td>70</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5</td>\n",
              "      <td>121</td>\n",
              "      <td>72</td>\n",
              "      <td>23</td>\n",
              "      <td>112</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1</td>\n",
              "      <td>93</td>\n",
              "      <td>70</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-262056db-dad9-4736-a2c8-3e78b291a068')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-262056db-dad9-4736-a2c8-3e78b291a068 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-262056db-dad9-4736-a2c8-3e78b291a068');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Read 'NIDDK_Pima.csv'\n",
        "niddk_df = pd.read_csv(\"./data/NIDDK_Pima.csv\")\n",
        "\n",
        "# Drop NaN value\n",
        "print(\"Any null value:\", any(niddk_df.isnull()))\n",
        "print(\"Any NaN value:\", any(niddk_df.isna()))\n",
        "print(\"Before Droping NaN Number of Rows:\", len(niddk_df))\n",
        "\n",
        "niddk_df = niddk_df.dropna()\n",
        "print(\"After Droping NaN Number of Rows:\", len(niddk_df))\n",
        "\n",
        "niddk_df.tail()"
      ],
      "id": "1481da9e-3597-47b0-81b5-1a1a6d1e1d37"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22002461-d56c-449e-b302-fecfe5de9de2"
      },
      "outputs": [],
      "source": [
        "# Correlation contingency table\n",
        "niddk_corr = niddk_df.corr()\n",
        "niddk_corr"
      ],
      "id": "22002461-d56c-449e-b302-fecfe5de9de2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26d554fa-4f37-479c-a540-92ff6c6e432c"
      },
      "source": [
        "Moreover, we evaluate the Weighted Average of the correlation of predictors:\n",
        "\n",
        "$\\text{Weighted Average of Correlation}$ $=$ $\\frac{\\text{Sum of Correlation of Predictors}}{\\text{Number of Predictors}}$ $=$ $0.208\n",
        "$"
      ],
      "id": "26d554fa-4f37-479c-a540-92ff6c6e432c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4991edb-a6f3-4672-8f45-2a8be8f8ded2"
      },
      "outputs": [],
      "source": [
        "# Evaluate the Weighted Average of the correlation of predictors\n",
        "\n",
        "print(\"Weighted Average:\", sum(niddk_corr.iloc[-1,:-1])/8)"
      ],
      "id": "e4991edb-a6f3-4672-8f45-2a8be8f8ded2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e319cfb2-86dd-4b49-900d-654fc5889f5f"
      },
      "source": [
        "-----\n",
        "\n",
        "### 2.1 KNN"
      ],
      "id": "e319cfb2-86dd-4b49-900d-654fc5889f5f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "172b5d06-106e-4df4-acff-040c1064be52",
        "outputId": "ba335013-7191-4424-d64e-a7108be8cbe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k: 1 Training Score: 1.0 Validation Score:  0.7398373983739838\n",
            "k: 2 Training Score: 0.8126272912423625 Validation Score:  0.7317073170731707\n",
            "k: 3 Training Score: 0.8207739307535642 Validation Score:  0.7154471544715447\n",
            "k: 5 Training Score: 0.7922606924643585 Validation Score:  0.7398373983739838\n",
            "k: 7 Training Score: 0.7841140529531568 Validation Score:  0.7560975609756098\n",
            "k: 9 Training Score: 0.780040733197556 Validation Score:  0.7560975609756098\n",
            "k: 15 Training Score: 0.7556008146639511 Validation Score:  0.7560975609756098\n",
            "k: 31 Training Score: 0.7556008146639511 Validation Score:  0.7398373983739838\n",
            "k: 51 Training Score: 0.7494908350305499 Validation Score:  0.7398373983739838\n",
            "k: 491 Training Score: 0.6435845213849287 Validation Score:  0.6260162601626016\n",
            "The best k is 15 and the best val score is 0.7561\n",
            "Best Model Validation Score: 0.7561 \n",
            " Training Score: 0.7556 \n",
            " Test Score: 0.8247\n",
            "Testing: Sensitivity/Recall: 0.5957446808510638 Specificity: 0.9252336448598131\n"
          ]
        }
      ],
      "source": [
        "knn_classifier(niddk_df)"
      ],
      "id": "172b5d06-106e-4df4-acff-040c1064be52"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7fa4fc7-650d-42e0-bb97-a4ded0c1cb86"
      },
      "source": [
        "-----\n",
        "\n",
        "###  2.2 Logistic Regression"
      ],
      "id": "e7fa4fc7-650d-42e0-bb97-a4ded0c1cb86"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134b6ae9-75cf-4f9d-8d81-89fdb884ea54",
        "outputId": "ed19009d-50fb-42b6-e433-79ad89e34275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.7671009771986971\n",
            "Testing Accuracy: 0.8246753246753247\n",
            "Testing: Sensitivity/Recall: 0.6170212765957447 Specificity: 0.9158878504672897\n"
          ]
        }
      ],
      "source": [
        "lg_classifier(niddk_df)"
      ],
      "id": "134b6ae9-75cf-4f9d-8d81-89fdb884ea54"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm4UHIhHdSoy",
        "outputId": "55075313-8375-4b12-e534-a10c79d73765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs [5000/50000]\n",
            "Training: Losses: 0.4589080215, Accuracy: 0.8566775244299675\n",
            "Testing: Losses: 0.5663192868, Accuracy: 0.7402597402597403\n",
            "Epochs [10000/50000]\n",
            "Training: Losses: 0.4488089979, Accuracy: 0.8697068403908795\n",
            "Testing: Losses: 0.6016312242, Accuracy: 0.7077922077922078\n",
            "Epochs [15000/50000]\n",
            "Training: Losses: 0.4556134343, Accuracy: 0.8566775244299675\n",
            "Testing: Losses: 0.5701509118, Accuracy: 0.7402597402597403\n",
            "Epochs [20000/50000]\n",
            "Training: Losses: 0.4427391887, Accuracy: 0.8697068403908795\n",
            "Testing: Losses: 0.5791807771, Accuracy: 0.7337662337662337\n",
            "Epochs [25000/50000]\n",
            "Training: Losses: 0.4317446649, Accuracy: 0.8827361563517915\n",
            "Testing: Losses: 0.5812276602, Accuracy: 0.7337662337662337\n",
            "Epochs [30000/50000]\n",
            "Training: Losses: 0.4375318587, Accuracy: 0.8762214983713354\n",
            "Testing: Losses: 0.5775487423, Accuracy: 0.7337662337662337\n",
            "Epochs [35000/50000]\n",
            "Training: Losses: 0.4470010996, Accuracy: 0.8631921824104235\n",
            "Testing: Losses: 0.5668684840, Accuracy: 0.7467532467532467\n",
            "Epochs [40000/50000]\n",
            "Training: Losses: 0.4336825311, Accuracy: 0.8794788273615635\n",
            "Testing: Losses: 0.5666416883, Accuracy: 0.7467532467532467\n",
            "Epochs [45000/50000]\n",
            "Training: Losses: 0.4504430890, Accuracy: 0.8631921824104235\n",
            "Testing: Losses: 0.5811266303, Accuracy: 0.7337662337662337\n",
            "Epochs [50000/50000]\n",
            "Training: Losses: 0.4487298727, Accuracy: 0.8648208469055375\n",
            "Testing: Losses: 0.5579327941, Accuracy: 0.7532467532467533\n"
          ]
        }
      ],
      "source": [
        "# Prepare training and testing data\n",
        "train_x, test_x, train_y, test_y = data_split(niddk_df)\n",
        "\n",
        "train_x = torch.from_numpy(train_x.to_numpy()).float()\n",
        "train_y = torch.squeeze(torch.from_numpy(train_y.to_numpy()).float())\n",
        "\n",
        "test_x = torch.from_numpy(test_x.to_numpy()).float()\n",
        "test_y = torch.squeeze(torch.from_numpy(test_y.to_numpy()).float())\n",
        "\n",
        "# print(train_x.shape, train_y.shape)\n",
        "# print(test_x.shape, test_y.shape)\n",
        "\n",
        "\n",
        "# Building FCNN\n",
        "class bpnn(nn.Module):\n",
        "  # Initialize the neural network with three fully connected hidden layers\n",
        "  def __init__(self, input, hid1, hid2, hid3, numclases):\n",
        "    super(bpnn, self).__init__()\n",
        "    # an affine operation: y = Wx + b\n",
        "    self.fc1 = nn.Linear(input, hid1)\n",
        "    self.fc2 = nn.Linear(hid1, hid2)\n",
        "    self.fc3 = nn.Linear(hid2, hid3)\n",
        "    self.fc4 = nn.Linear(hid3, numclass)\n",
        "  # feedforward\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x)) # Activation using sigmoid function for regularization\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = F.softmax(self.fc4(x))\n",
        "    return x\n",
        "\n",
        "input = train_x.shape[1] \n",
        "hid1 = 10\n",
        "hid2 = 8\n",
        "hid3 = 5\n",
        "numclass = 2\n",
        "\n",
        "batch = 32\n",
        "\n",
        "modl = bpnn(input, hid1, hid2, hid3, numclass)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optim = torch.optim.Adam(modl.parameters(), lr=0.003)\n",
        "\n",
        "for epoch in range(50000):\n",
        "  train_pred = modl(train_x)\n",
        "  train_pred = torch.squeeze(train_pred)\n",
        "  losses = criterion(train_pred, train_y.type(torch.LongTensor))\n",
        "  optim.zero_grad()\n",
        "  losses.backward()\n",
        "  optim.step() # update weights and bias\n",
        "\n",
        "  if (epoch+1) % 5000 == 0:\n",
        "    train_pred_np = torch.argmax(train_pred, axis=1).numpy()\n",
        "    train_acc = accuracy_score(train_y, train_pred_np)\n",
        "\n",
        "    test_pred = modl(test_x)\n",
        "    test_pred = torch.squeeze(test_pred)\n",
        "\n",
        "    test_loss = criterion(test_pred, test_y.type(torch.LongTensor))\n",
        "\n",
        "    test_pred_np = torch.argmax(test_pred, axis=1).numpy()\n",
        "    test_acc = accuracy_score(test_y, test_pred_np)\n",
        "\n",
        "\n",
        "    print (f'Epochs [{epoch+1}/50000]')\n",
        "    print (f'Training: Losses: {losses.item():.10f}, Accuracy: {train_acc}')\n",
        "    print (f'Testing: Losses: {test_loss.item():.10f}, Accuracy: {test_acc}')"
      ],
      "id": "fm4UHIhHdSoy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3.2 Keras\n",
        "2.3.2.1 One hidden layers with 2 neurons in output layer"
      ],
      "metadata": {
        "id": "gjhGQgUGBNCb"
      },
      "id": "gjhGQgUGBNCb"
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, test_x, train_y, test_y = data_split(niddk_df)\n",
        "\n",
        "train_x = np.float32(train_x)\n",
        "test_x = np.float32(test_x)\n",
        "\n",
        "train_x = train_x/train_x.max()\n",
        "test_x = test_x/test_x.max()\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(2)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_x, train_y, epochs=200, batch_size=32,\n",
        "          validation_data=(test_x, test_y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7Mfjaw4BQRU",
        "outputId": "6e912dc4-7148-495b-9bb7-df7c701891c6"
      },
      "id": "D7Mfjaw4BQRU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "20/20 [==============================] - 1s 11ms/step - loss: 0.6779 - accuracy: 0.6401 - val_loss: 0.6466 - val_accuracy: 0.6948\n",
            "Epoch 2/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.6618 - accuracy: 0.6401 - val_loss: 0.6285 - val_accuracy: 0.6948\n",
            "Epoch 3/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.6544 - accuracy: 0.6401 - val_loss: 0.6199 - val_accuracy: 0.6948\n",
            "Epoch 4/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.6497 - accuracy: 0.6401 - val_loss: 0.6106 - val_accuracy: 0.6948\n",
            "Epoch 5/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.6451 - accuracy: 0.6384 - val_loss: 0.6117 - val_accuracy: 0.7143\n",
            "Epoch 6/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.6400 - accuracy: 0.6384 - val_loss: 0.6003 - val_accuracy: 0.7078\n",
            "Epoch 7/200\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.6339 - accuracy: 0.6433 - val_loss: 0.5976 - val_accuracy: 0.7208\n",
            "Epoch 8/200\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.6283 - accuracy: 0.6498 - val_loss: 0.5996 - val_accuracy: 0.7273\n",
            "Epoch 9/200\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.6206 - accuracy: 0.6466 - val_loss: 0.5900 - val_accuracy: 0.7338\n",
            "Epoch 10/200\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.6129 - accuracy: 0.6564 - val_loss: 0.5926 - val_accuracy: 0.7338\n",
            "Epoch 11/200\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.6054 - accuracy: 0.6580 - val_loss: 0.5799 - val_accuracy: 0.7403\n",
            "Epoch 12/200\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.5960 - accuracy: 0.6759 - val_loss: 0.6100 - val_accuracy: 0.7078\n",
            "Epoch 13/200\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.5898 - accuracy: 0.6954 - val_loss: 0.5725 - val_accuracy: 0.7338\n",
            "Epoch 14/200\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.5789 - accuracy: 0.6873 - val_loss: 0.6455 - val_accuracy: 0.6753\n",
            "Epoch 15/200\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.5695 - accuracy: 0.7052 - val_loss: 0.6309 - val_accuracy: 0.7013\n",
            "Epoch 16/200\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.5604 - accuracy: 0.7362 - val_loss: 0.6545 - val_accuracy: 0.6558\n",
            "Epoch 17/200\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.5517 - accuracy: 0.7459 - val_loss: 0.6760 - val_accuracy: 0.6169\n",
            "Epoch 18/200\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.5435 - accuracy: 0.7590 - val_loss: 0.7034 - val_accuracy: 0.5779\n",
            "Epoch 19/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.5367 - accuracy: 0.7524 - val_loss: 0.6927 - val_accuracy: 0.5974\n",
            "Epoch 20/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.5288 - accuracy: 0.7524 - val_loss: 0.8322 - val_accuracy: 0.4870\n",
            "Epoch 21/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.5265 - accuracy: 0.7476 - val_loss: 0.7580 - val_accuracy: 0.5455\n",
            "Epoch 22/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.5217 - accuracy: 0.7476 - val_loss: 0.7938 - val_accuracy: 0.5390\n",
            "Epoch 23/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.5204 - accuracy: 0.7427 - val_loss: 0.8417 - val_accuracy: 0.5260\n",
            "Epoch 24/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.5149 - accuracy: 0.7573 - val_loss: 0.8784 - val_accuracy: 0.5130\n",
            "Epoch 25/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.5094 - accuracy: 0.7606 - val_loss: 0.9653 - val_accuracy: 0.4610\n",
            "Epoch 26/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.5140 - accuracy: 0.7557 - val_loss: 0.9089 - val_accuracy: 0.5065\n",
            "Epoch 27/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.5099 - accuracy: 0.7476 - val_loss: 0.8175 - val_accuracy: 0.5519\n",
            "Epoch 28/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.5073 - accuracy: 0.7524 - val_loss: 0.9966 - val_accuracy: 0.4740\n",
            "Epoch 29/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.5077 - accuracy: 0.7541 - val_loss: 1.0042 - val_accuracy: 0.4675\n",
            "Epoch 30/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.5032 - accuracy: 0.7541 - val_loss: 1.0974 - val_accuracy: 0.4286\n",
            "Epoch 31/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.5092 - accuracy: 0.7443 - val_loss: 1.0739 - val_accuracy: 0.4545\n",
            "Epoch 32/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.5079 - accuracy: 0.7557 - val_loss: 1.0099 - val_accuracy: 0.4740\n",
            "Epoch 33/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.5022 - accuracy: 0.7541 - val_loss: 1.0092 - val_accuracy: 0.4740\n",
            "Epoch 34/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4995 - accuracy: 0.7524 - val_loss: 0.9782 - val_accuracy: 0.5000\n",
            "Epoch 35/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4980 - accuracy: 0.7524 - val_loss: 1.1145 - val_accuracy: 0.4286\n",
            "Epoch 36/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.5017 - accuracy: 0.7557 - val_loss: 1.0825 - val_accuracy: 0.4545\n",
            "Epoch 37/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4990 - accuracy: 0.7524 - val_loss: 1.1572 - val_accuracy: 0.4091\n",
            "Epoch 38/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4994 - accuracy: 0.7541 - val_loss: 1.0311 - val_accuracy: 0.4870\n",
            "Epoch 39/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4961 - accuracy: 0.7524 - val_loss: 1.0980 - val_accuracy: 0.4481\n",
            "Epoch 40/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4960 - accuracy: 0.7557 - val_loss: 1.0737 - val_accuracy: 0.4675\n",
            "Epoch 41/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.5002 - accuracy: 0.7524 - val_loss: 1.2059 - val_accuracy: 0.4026\n",
            "Epoch 42/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4963 - accuracy: 0.7606 - val_loss: 1.0227 - val_accuracy: 0.4935\n",
            "Epoch 43/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4996 - accuracy: 0.7573 - val_loss: 1.0661 - val_accuracy: 0.4805\n",
            "Epoch 44/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4964 - accuracy: 0.7590 - val_loss: 1.1660 - val_accuracy: 0.4286\n",
            "Epoch 45/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4941 - accuracy: 0.7541 - val_loss: 1.0967 - val_accuracy: 0.4610\n",
            "Epoch 46/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4963 - accuracy: 0.7443 - val_loss: 1.0359 - val_accuracy: 0.4870\n",
            "Epoch 47/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4964 - accuracy: 0.7638 - val_loss: 1.0460 - val_accuracy: 0.5000\n",
            "Epoch 48/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.5079 - accuracy: 0.7394 - val_loss: 1.0622 - val_accuracy: 0.4935\n",
            "Epoch 49/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4953 - accuracy: 0.7704 - val_loss: 1.2493 - val_accuracy: 0.3896\n",
            "Epoch 50/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4923 - accuracy: 0.7524 - val_loss: 1.0469 - val_accuracy: 0.5000\n",
            "Epoch 51/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4968 - accuracy: 0.7557 - val_loss: 1.0547 - val_accuracy: 0.4870\n",
            "Epoch 52/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4971 - accuracy: 0.7541 - val_loss: 1.2005 - val_accuracy: 0.4156\n",
            "Epoch 53/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4931 - accuracy: 0.7638 - val_loss: 1.0885 - val_accuracy: 0.4675\n",
            "Epoch 54/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.5010 - accuracy: 0.7573 - val_loss: 1.0960 - val_accuracy: 0.4740\n",
            "Epoch 55/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4924 - accuracy: 0.7524 - val_loss: 1.1018 - val_accuracy: 0.4675\n",
            "Epoch 56/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4918 - accuracy: 0.7590 - val_loss: 1.1838 - val_accuracy: 0.4286\n",
            "Epoch 57/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4915 - accuracy: 0.7476 - val_loss: 1.1593 - val_accuracy: 0.4286\n",
            "Epoch 58/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4890 - accuracy: 0.7622 - val_loss: 0.9744 - val_accuracy: 0.5195\n",
            "Epoch 59/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4945 - accuracy: 0.7557 - val_loss: 1.1264 - val_accuracy: 0.4481\n",
            "Epoch 60/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4900 - accuracy: 0.7573 - val_loss: 1.0778 - val_accuracy: 0.4870\n",
            "Epoch 61/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4891 - accuracy: 0.7557 - val_loss: 1.1825 - val_accuracy: 0.4221\n",
            "Epoch 62/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4909 - accuracy: 0.7590 - val_loss: 1.0490 - val_accuracy: 0.5065\n",
            "Epoch 63/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4882 - accuracy: 0.7704 - val_loss: 1.2607 - val_accuracy: 0.3961\n",
            "Epoch 64/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4924 - accuracy: 0.7557 - val_loss: 1.0382 - val_accuracy: 0.5000\n",
            "Epoch 65/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4917 - accuracy: 0.7590 - val_loss: 1.0902 - val_accuracy: 0.4870\n",
            "Epoch 66/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4887 - accuracy: 0.7508 - val_loss: 1.1191 - val_accuracy: 0.4545\n",
            "Epoch 67/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4903 - accuracy: 0.7606 - val_loss: 1.1871 - val_accuracy: 0.4221\n",
            "Epoch 68/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4900 - accuracy: 0.7541 - val_loss: 1.1210 - val_accuracy: 0.4610\n",
            "Epoch 69/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4926 - accuracy: 0.7459 - val_loss: 1.1035 - val_accuracy: 0.4740\n",
            "Epoch 70/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4899 - accuracy: 0.7704 - val_loss: 1.1822 - val_accuracy: 0.4286\n",
            "Epoch 71/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4881 - accuracy: 0.7590 - val_loss: 1.1546 - val_accuracy: 0.4481\n",
            "Epoch 72/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4876 - accuracy: 0.7541 - val_loss: 1.1781 - val_accuracy: 0.4286\n",
            "Epoch 73/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4913 - accuracy: 0.7622 - val_loss: 1.1529 - val_accuracy: 0.4481\n",
            "Epoch 74/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4860 - accuracy: 0.7573 - val_loss: 1.0909 - val_accuracy: 0.4870\n",
            "Epoch 75/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4897 - accuracy: 0.7622 - val_loss: 1.1286 - val_accuracy: 0.4545\n",
            "Epoch 76/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4869 - accuracy: 0.7655 - val_loss: 1.1251 - val_accuracy: 0.4675\n",
            "Epoch 77/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4883 - accuracy: 0.7638 - val_loss: 1.1847 - val_accuracy: 0.4286\n",
            "Epoch 78/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4894 - accuracy: 0.7590 - val_loss: 1.2152 - val_accuracy: 0.4026\n",
            "Epoch 79/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4872 - accuracy: 0.7638 - val_loss: 1.2025 - val_accuracy: 0.4156\n",
            "Epoch 80/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4861 - accuracy: 0.7671 - val_loss: 1.0942 - val_accuracy: 0.4870\n",
            "Epoch 81/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4871 - accuracy: 0.7573 - val_loss: 1.2085 - val_accuracy: 0.4156\n",
            "Epoch 82/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4851 - accuracy: 0.7573 - val_loss: 1.1743 - val_accuracy: 0.4416\n",
            "Epoch 83/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4856 - accuracy: 0.7671 - val_loss: 1.1751 - val_accuracy: 0.4416\n",
            "Epoch 84/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4828 - accuracy: 0.7671 - val_loss: 1.1342 - val_accuracy: 0.4545\n",
            "Epoch 85/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4928 - accuracy: 0.7606 - val_loss: 1.2463 - val_accuracy: 0.4026\n",
            "Epoch 86/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4850 - accuracy: 0.7638 - val_loss: 1.1842 - val_accuracy: 0.4286\n",
            "Epoch 87/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4863 - accuracy: 0.7524 - val_loss: 1.1122 - val_accuracy: 0.4805\n",
            "Epoch 88/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4820 - accuracy: 0.7687 - val_loss: 1.1207 - val_accuracy: 0.4740\n",
            "Epoch 89/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4832 - accuracy: 0.7622 - val_loss: 1.2386 - val_accuracy: 0.4026\n",
            "Epoch 90/200\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.4884 - accuracy: 0.7606 - val_loss: 1.2992 - val_accuracy: 0.3831\n",
            "Epoch 91/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4909 - accuracy: 0.7622 - val_loss: 1.2249 - val_accuracy: 0.4286\n",
            "Epoch 92/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4880 - accuracy: 0.7606 - val_loss: 1.0286 - val_accuracy: 0.5000\n",
            "Epoch 93/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4914 - accuracy: 0.7671 - val_loss: 1.0559 - val_accuracy: 0.5000\n",
            "Epoch 94/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4828 - accuracy: 0.7573 - val_loss: 1.1977 - val_accuracy: 0.4286\n",
            "Epoch 95/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4843 - accuracy: 0.7541 - val_loss: 1.0544 - val_accuracy: 0.5000\n",
            "Epoch 96/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4916 - accuracy: 0.7524 - val_loss: 1.0613 - val_accuracy: 0.5000\n",
            "Epoch 97/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4861 - accuracy: 0.7606 - val_loss: 1.0622 - val_accuracy: 0.5000\n",
            "Epoch 98/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4887 - accuracy: 0.7671 - val_loss: 1.2279 - val_accuracy: 0.4091\n",
            "Epoch 99/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4802 - accuracy: 0.7573 - val_loss: 1.2172 - val_accuracy: 0.4221\n",
            "Epoch 100/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4851 - accuracy: 0.7573 - val_loss: 1.0848 - val_accuracy: 0.4935\n",
            "Epoch 101/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4802 - accuracy: 0.7524 - val_loss: 1.2910 - val_accuracy: 0.3831\n",
            "Epoch 102/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4836 - accuracy: 0.7541 - val_loss: 1.1861 - val_accuracy: 0.4156\n",
            "Epoch 103/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4791 - accuracy: 0.7655 - val_loss: 1.1151 - val_accuracy: 0.4740\n",
            "Epoch 104/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4869 - accuracy: 0.7606 - val_loss: 1.3155 - val_accuracy: 0.3831\n",
            "Epoch 105/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4875 - accuracy: 0.7638 - val_loss: 1.2126 - val_accuracy: 0.4156\n",
            "Epoch 106/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4873 - accuracy: 0.7622 - val_loss: 0.9517 - val_accuracy: 0.5325\n",
            "Epoch 107/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4842 - accuracy: 0.7443 - val_loss: 1.3566 - val_accuracy: 0.3701\n",
            "Epoch 108/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4923 - accuracy: 0.7573 - val_loss: 1.1936 - val_accuracy: 0.4221\n",
            "Epoch 109/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4808 - accuracy: 0.7590 - val_loss: 1.1746 - val_accuracy: 0.4286\n",
            "Epoch 110/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4778 - accuracy: 0.7557 - val_loss: 1.1285 - val_accuracy: 0.4675\n",
            "Epoch 111/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4815 - accuracy: 0.7655 - val_loss: 1.2390 - val_accuracy: 0.3961\n",
            "Epoch 112/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4793 - accuracy: 0.7704 - val_loss: 1.0869 - val_accuracy: 0.4935\n",
            "Epoch 113/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4934 - accuracy: 0.7508 - val_loss: 0.9913 - val_accuracy: 0.5195\n",
            "Epoch 114/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4961 - accuracy: 0.7638 - val_loss: 1.2760 - val_accuracy: 0.3831\n",
            "Epoch 115/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4823 - accuracy: 0.7638 - val_loss: 1.2022 - val_accuracy: 0.4156\n",
            "Epoch 116/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4774 - accuracy: 0.7622 - val_loss: 1.2058 - val_accuracy: 0.4156\n",
            "Epoch 117/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4746 - accuracy: 0.7606 - val_loss: 1.1306 - val_accuracy: 0.4675\n",
            "Epoch 118/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4864 - accuracy: 0.7638 - val_loss: 1.3433 - val_accuracy: 0.3766\n",
            "Epoch 119/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4832 - accuracy: 0.7606 - val_loss: 1.1591 - val_accuracy: 0.4481\n",
            "Epoch 120/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4774 - accuracy: 0.7622 - val_loss: 1.1180 - val_accuracy: 0.4675\n",
            "Epoch 121/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4751 - accuracy: 0.7622 - val_loss: 1.2786 - val_accuracy: 0.3961\n",
            "Epoch 122/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4766 - accuracy: 0.7622 - val_loss: 1.0966 - val_accuracy: 0.4870\n",
            "Epoch 123/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4761 - accuracy: 0.7573 - val_loss: 1.2590 - val_accuracy: 0.3961\n",
            "Epoch 124/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4723 - accuracy: 0.7638 - val_loss: 1.0849 - val_accuracy: 0.4935\n",
            "Epoch 125/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4763 - accuracy: 0.7655 - val_loss: 1.2403 - val_accuracy: 0.4091\n",
            "Epoch 126/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4751 - accuracy: 0.7622 - val_loss: 1.1640 - val_accuracy: 0.4545\n",
            "Epoch 127/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4734 - accuracy: 0.7655 - val_loss: 1.2776 - val_accuracy: 0.3961\n",
            "Epoch 128/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4755 - accuracy: 0.7557 - val_loss: 1.2549 - val_accuracy: 0.4026\n",
            "Epoch 129/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4779 - accuracy: 0.7606 - val_loss: 1.3131 - val_accuracy: 0.3831\n",
            "Epoch 130/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4739 - accuracy: 0.7655 - val_loss: 1.2049 - val_accuracy: 0.4221\n",
            "Epoch 131/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4740 - accuracy: 0.7638 - val_loss: 1.1075 - val_accuracy: 0.4935\n",
            "Epoch 132/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4775 - accuracy: 0.7557 - val_loss: 1.2825 - val_accuracy: 0.4026\n",
            "Epoch 133/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4738 - accuracy: 0.7687 - val_loss: 1.2533 - val_accuracy: 0.4156\n",
            "Epoch 134/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4760 - accuracy: 0.7573 - val_loss: 1.1720 - val_accuracy: 0.4481\n",
            "Epoch 135/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4746 - accuracy: 0.7590 - val_loss: 1.1800 - val_accuracy: 0.4481\n",
            "Epoch 136/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4753 - accuracy: 0.7492 - val_loss: 1.3083 - val_accuracy: 0.3896\n",
            "Epoch 137/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4732 - accuracy: 0.7606 - val_loss: 1.1973 - val_accuracy: 0.4416\n",
            "Epoch 138/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4746 - accuracy: 0.7573 - val_loss: 1.2054 - val_accuracy: 0.4351\n",
            "Epoch 139/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4721 - accuracy: 0.7671 - val_loss: 1.2970 - val_accuracy: 0.4026\n",
            "Epoch 140/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4757 - accuracy: 0.7622 - val_loss: 1.3417 - val_accuracy: 0.3896\n",
            "Epoch 141/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4733 - accuracy: 0.7671 - val_loss: 1.1250 - val_accuracy: 0.4870\n",
            "Epoch 142/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4765 - accuracy: 0.7606 - val_loss: 1.2032 - val_accuracy: 0.4286\n",
            "Epoch 143/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4715 - accuracy: 0.7573 - val_loss: 1.2381 - val_accuracy: 0.4286\n",
            "Epoch 144/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4704 - accuracy: 0.7655 - val_loss: 1.1664 - val_accuracy: 0.4545\n",
            "Epoch 145/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4725 - accuracy: 0.7573 - val_loss: 1.2223 - val_accuracy: 0.4221\n",
            "Epoch 146/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4899 - accuracy: 0.7638 - val_loss: 1.2886 - val_accuracy: 0.4091\n",
            "Epoch 147/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4828 - accuracy: 0.7590 - val_loss: 1.3357 - val_accuracy: 0.3831\n",
            "Epoch 148/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4706 - accuracy: 0.7622 - val_loss: 1.2112 - val_accuracy: 0.4351\n",
            "Epoch 149/200\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.4717 - accuracy: 0.7655 - val_loss: 1.1992 - val_accuracy: 0.4221\n",
            "Epoch 150/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4710 - accuracy: 0.7541 - val_loss: 1.1967 - val_accuracy: 0.4156\n",
            "Epoch 151/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4715 - accuracy: 0.7606 - val_loss: 1.2575 - val_accuracy: 0.4156\n",
            "Epoch 152/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4725 - accuracy: 0.7736 - val_loss: 1.1100 - val_accuracy: 0.4740\n",
            "Epoch 153/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4719 - accuracy: 0.7573 - val_loss: 1.2014 - val_accuracy: 0.4351\n",
            "Epoch 154/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4690 - accuracy: 0.7557 - val_loss: 1.2538 - val_accuracy: 0.4091\n",
            "Epoch 155/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4697 - accuracy: 0.7687 - val_loss: 1.1862 - val_accuracy: 0.4416\n",
            "Epoch 156/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4675 - accuracy: 0.7557 - val_loss: 1.3265 - val_accuracy: 0.3831\n",
            "Epoch 157/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4693 - accuracy: 0.7606 - val_loss: 1.1765 - val_accuracy: 0.4481\n",
            "Epoch 158/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4695 - accuracy: 0.7638 - val_loss: 1.1963 - val_accuracy: 0.4351\n",
            "Epoch 159/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4684 - accuracy: 0.7720 - val_loss: 1.2292 - val_accuracy: 0.4221\n",
            "Epoch 160/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4680 - accuracy: 0.7704 - val_loss: 1.3354 - val_accuracy: 0.3896\n",
            "Epoch 161/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4728 - accuracy: 0.7720 - val_loss: 1.1579 - val_accuracy: 0.4610\n",
            "Epoch 162/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4684 - accuracy: 0.7704 - val_loss: 1.1689 - val_accuracy: 0.4545\n",
            "Epoch 163/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4683 - accuracy: 0.7704 - val_loss: 1.2121 - val_accuracy: 0.4351\n",
            "Epoch 164/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4683 - accuracy: 0.7622 - val_loss: 1.2836 - val_accuracy: 0.3961\n",
            "Epoch 165/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4736 - accuracy: 0.7638 - val_loss: 1.2214 - val_accuracy: 0.4221\n",
            "Epoch 166/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4694 - accuracy: 0.7687 - val_loss: 1.2012 - val_accuracy: 0.4286\n",
            "Epoch 167/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4678 - accuracy: 0.7687 - val_loss: 1.3767 - val_accuracy: 0.3766\n",
            "Epoch 168/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4775 - accuracy: 0.7655 - val_loss: 1.0946 - val_accuracy: 0.4870\n",
            "Epoch 169/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4667 - accuracy: 0.7655 - val_loss: 1.3020 - val_accuracy: 0.4026\n",
            "Epoch 170/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4650 - accuracy: 0.7720 - val_loss: 1.2064 - val_accuracy: 0.4286\n",
            "Epoch 171/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4722 - accuracy: 0.7704 - val_loss: 1.1394 - val_accuracy: 0.4675\n",
            "Epoch 172/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4695 - accuracy: 0.7671 - val_loss: 1.1937 - val_accuracy: 0.4286\n",
            "Epoch 173/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4689 - accuracy: 0.7590 - val_loss: 1.1427 - val_accuracy: 0.4545\n",
            "Epoch 174/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4715 - accuracy: 0.7655 - val_loss: 1.1819 - val_accuracy: 0.4481\n",
            "Epoch 175/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4672 - accuracy: 0.7671 - val_loss: 1.1579 - val_accuracy: 0.4545\n",
            "Epoch 176/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4647 - accuracy: 0.7622 - val_loss: 1.3756 - val_accuracy: 0.3896\n",
            "Epoch 177/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4817 - accuracy: 0.7606 - val_loss: 1.1196 - val_accuracy: 0.4675\n",
            "Epoch 178/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4662 - accuracy: 0.7622 - val_loss: 1.2782 - val_accuracy: 0.4091\n",
            "Epoch 179/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4692 - accuracy: 0.7704 - val_loss: 1.3096 - val_accuracy: 0.3961\n",
            "Epoch 180/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4643 - accuracy: 0.7704 - val_loss: 1.2521 - val_accuracy: 0.4156\n",
            "Epoch 181/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4658 - accuracy: 0.7720 - val_loss: 1.2483 - val_accuracy: 0.4221\n",
            "Epoch 182/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4711 - accuracy: 0.7590 - val_loss: 1.2417 - val_accuracy: 0.4221\n",
            "Epoch 183/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4646 - accuracy: 0.7655 - val_loss: 1.2550 - val_accuracy: 0.4221\n",
            "Epoch 184/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4664 - accuracy: 0.7671 - val_loss: 1.3306 - val_accuracy: 0.3896\n",
            "Epoch 185/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4662 - accuracy: 0.7638 - val_loss: 1.1871 - val_accuracy: 0.4351\n",
            "Epoch 186/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4624 - accuracy: 0.7704 - val_loss: 1.4345 - val_accuracy: 0.3766\n",
            "Epoch 187/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4751 - accuracy: 0.7655 - val_loss: 1.1896 - val_accuracy: 0.4351\n",
            "Epoch 188/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4630 - accuracy: 0.7638 - val_loss: 1.2478 - val_accuracy: 0.4221\n",
            "Epoch 189/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4653 - accuracy: 0.7638 - val_loss: 1.0790 - val_accuracy: 0.5000\n",
            "Epoch 190/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4744 - accuracy: 0.7671 - val_loss: 1.3859 - val_accuracy: 0.3896\n",
            "Epoch 191/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4686 - accuracy: 0.7687 - val_loss: 1.3432 - val_accuracy: 0.3896\n",
            "Epoch 192/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4662 - accuracy: 0.7638 - val_loss: 1.3244 - val_accuracy: 0.3896\n",
            "Epoch 193/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4622 - accuracy: 0.7720 - val_loss: 1.1473 - val_accuracy: 0.4610\n",
            "Epoch 194/200\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.4662 - accuracy: 0.7590 - val_loss: 1.2997 - val_accuracy: 0.4026\n",
            "Epoch 195/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4611 - accuracy: 0.7720 - val_loss: 1.2446 - val_accuracy: 0.4221\n",
            "Epoch 196/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4626 - accuracy: 0.7638 - val_loss: 1.3081 - val_accuracy: 0.4026\n",
            "Epoch 197/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4641 - accuracy: 0.7736 - val_loss: 1.0807 - val_accuracy: 0.5000\n",
            "Epoch 198/200\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.4716 - accuracy: 0.7687 - val_loss: 1.1622 - val_accuracy: 0.4481\n",
            "Epoch 199/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4622 - accuracy: 0.7687 - val_loss: 1.2504 - val_accuracy: 0.4221\n",
            "Epoch 200/200\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.4653 - accuracy: 0.7704 - val_loss: 1.3594 - val_accuracy: 0.3831\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe491a254d0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc9ffbe7-cfca-42b9-9d61-a2a61e5b02bf"
      },
      "source": [
        "-----\n",
        "## 3. Sylhet Hospital Diabetes Risk Prediction Dataset\n",
        "\n",
        "This has been collected using direct questionnaires from the patients of Sylhet Diabetes\n",
        "Hospital in Sylhet, Bangladesh and approved by a doctor.\n",
        "\n",
        "####  Attribute Information\n",
        "| Column Name    | Expression    |\n",
        "| :------------- | :------------- |\n",
        "| Age        | 20-65 |\n",
        "| Gender        | 1. Male, 2.Female |\n",
        "| Polyuria        | 1.Yes, 2.No. |\n",
        "| Polydipsia        | 1.Yes, 2.No |\n",
        "| sudden weight loss        | 1.Yes, 2.No. |\n",
        "| weakness        | 1.Yes, 2.No. |\n",
        "| Polyphagia        | 1.Yes, 2.No. |\n",
        "| Genital thrush        | 1.Yes, 2.No. |\n",
        "| visual blurring        | 1.Yes, 2.No. |\n",
        "| Itching        | 1.Yes, 2.No. |\n",
        "| Irritability        | 1.Yes, 2.No. |\n",
        "| delayed healing        | 1.Yes, 2.No. |\n",
        "| partial paresis        | 1.Yes, 2.No. |\n",
        "| muscle stiffness        | 1.Yes, 2.No. |\n",
        "| Alopecia       | 1.Yes, 2.No. |\n",
        "| Obesity        | 1.Yes, 2.No. |\n",
        "| Class        | 1.Positive, 2.Negative. |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "bc9ffbe7-cfca-42b9-9d61-a2a61e5b02bf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "23a25361-566a-4884-9e94-898958b20c6b",
        "outputId": "efa35664-f2ae-4b46-8181-50ee8bfee9f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Any null value: True\n",
            "Any NaN value: True\n",
            "Before Droping NaN Number of Rows: 520\n",
            "After Droping NaN Number of Rows: 520\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Age  Gender  Polyuria  Polydipsia  sudden weight loss  weakness  \\\n",
              "515   39       0         1           1                   1         0   \n",
              "516   48       0         1           1                   1         1   \n",
              "517   58       0         1           1                   1         1   \n",
              "518   32       0         0           0                   0         1   \n",
              "519   42       1         0           0                   0         0   \n",
              "\n",
              "     Polyphagia  Genital thrush  visual blurring  Itching  Irritability  \\\n",
              "515           1               0                0        1             0   \n",
              "516           1               0                0        1             1   \n",
              "517           1               0                1        0             0   \n",
              "518           0               0                1        1             0   \n",
              "519           0               0                0        0             0   \n",
              "\n",
              "     delayed healing  partial paresis  muscle stiffness  Alopecia  Obesity  \\\n",
              "515                1                1                 0         0        0   \n",
              "516                1                1                 0         0        0   \n",
              "517                0                1                 1         0        1   \n",
              "518                1                0                 0         1        0   \n",
              "519                0                0                 0         0        0   \n",
              "\n",
              "     class  \n",
              "515      1  \n",
              "516      1  \n",
              "517      1  \n",
              "518      0  \n",
              "519      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f69dca4d-15aa-4516-87aa-8a05efe83c48\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Polyuria</th>\n",
              "      <th>Polydipsia</th>\n",
              "      <th>sudden weight loss</th>\n",
              "      <th>weakness</th>\n",
              "      <th>Polyphagia</th>\n",
              "      <th>Genital thrush</th>\n",
              "      <th>visual blurring</th>\n",
              "      <th>Itching</th>\n",
              "      <th>Irritability</th>\n",
              "      <th>delayed healing</th>\n",
              "      <th>partial paresis</th>\n",
              "      <th>muscle stiffness</th>\n",
              "      <th>Alopecia</th>\n",
              "      <th>Obesity</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>518</th>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>519</th>\n",
              "      <td>42</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f69dca4d-15aa-4516-87aa-8a05efe83c48')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f69dca4d-15aa-4516-87aa-8a05efe83c48 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f69dca4d-15aa-4516-87aa-8a05efe83c48');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Read 'Sylhet.csv'\n",
        "sylhet_df = pd.read_csv(\"./data/Sylhet.csv\")\n",
        "\n",
        "# Drop NaN value\n",
        "print(\"Any null value:\", any(sylhet_df.isnull()))\n",
        "print(\"Any NaN value:\", any(sylhet_df.isna()))\n",
        "print(\"Before Droping NaN Number of Rows:\", len(sylhet_df))\n",
        "\n",
        "sylhet_df = sylhet_df.dropna()\n",
        "print(\"After Droping NaN Number of Rows:\", len(sylhet_df))\n",
        "\n",
        "# Encode textual columns\n",
        "le = LabelEncoder()\n",
        "for each in list(sylhet_df.columns.values)[1:]:\n",
        "    sylhet_df[each] = le.fit_transform(sylhet_df[each])\n",
        "\n",
        "sylhet_df.tail()"
      ],
      "id": "23a25361-566a-4884-9e94-898958b20c6b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ada93c9b-e2ce-4669-8392-5aa02bd7b572"
      },
      "outputs": [],
      "source": [
        "# Correlation contingency table\n",
        "sylhet_corr = sylhet_df.corr()\n",
        "sylhet_corr"
      ],
      "id": "ada93c9b-e2ce-4669-8392-5aa02bd7b572"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "889098a5-d91a-4f51-abe9-c0a2c0d467bf"
      },
      "source": [
        "Moreover, we evaluate the Weighted Average of the correlation of predictors:\n",
        "\n",
        "$\\text{Weighted Average of Correlation}$ $=$ $\\frac{\\text{Sum of Correlation of Predictors}}{\\text{Number of Predictors}}$ $=$ $0.104\n",
        "$"
      ],
      "id": "889098a5-d91a-4f51-abe9-c0a2c0d467bf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76a6a3e0-354c-49c5-92bb-32a4c13d8ff4"
      },
      "outputs": [],
      "source": [
        "# Evaluate the Weighted Average of the correlation of predictors\n",
        "\n",
        "print(\"Weighted Average:\", sum(niddk_corr.iloc[-1,:-1])/16)"
      ],
      "id": "76a6a3e0-354c-49c5-92bb-32a4c13d8ff4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b26f2340-0aaa-4a4f-afc8-0593cf9f4040"
      },
      "source": [
        "-----\n",
        "\n",
        "### 3.1 KNN"
      ],
      "id": "b26f2340-0aaa-4a4f-afc8-0593cf9f4040"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5c1bc27-9c0c-46ab-9083-78094b483852",
        "outputId": "62bdd4c1-3427-4548-9121-4b1997ea8b16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k: 1 Training Score: 1.0 Validation Score:  0.9523809523809523\n",
            "k: 2 Training Score: 0.9578313253012049 Validation Score:  0.9404761904761905\n",
            "k: 3 Training Score: 0.9668674698795181 Validation Score:  0.9404761904761905\n",
            "k: 5 Training Score: 0.927710843373494 Validation Score:  0.9166666666666666\n",
            "k: 7 Training Score: 0.9246987951807228 Validation Score:  0.9047619047619048\n",
            "k: 9 Training Score: 0.9126506024096386 Validation Score:  0.9047619047619048\n",
            "k: 15 Training Score: 0.9126506024096386 Validation Score:  0.8928571428571429\n",
            "k: 31 Training Score: 0.8765060240963856 Validation Score:  0.8571428571428571\n",
            "k: 51 Training Score: 0.822289156626506 Validation Score:  0.8452380952380952\n",
            "k: 332 Training Score: 0.5993975903614458 Validation Score:  0.6785714285714286\n",
            "The best k is 1 and the best val score is 0.9524\n",
            "Best Model Validation Score: 0.9524 \n",
            " Training Score: 1.0000 \n",
            " Test Score: 0.9904\n",
            "Testing: Sensitivity/Recall: 1.0 Specificity: 0.975\n"
          ]
        }
      ],
      "source": [
        "knn_classifier(sylhet_df)"
      ],
      "id": "a5c1bc27-9c0c-46ab-9083-78094b483852"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkGViHgRjKif",
        "outputId": "5eac4cbb-503c-4538-9394-d223c2004d66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs [2000/2000]\n",
            "Training: Losses: 0.4223381281, Accuracy: 0.90625\n",
            "Testing: Losses: 0.3797752559, Accuracy: 0.9326923076923077\n",
            "Epochs [4000/2000]\n",
            "Training: Losses: 0.3427784145, Accuracy: 0.96875\n",
            "Testing: Losses: 0.3576070070, Accuracy: 0.9519230769230769\n",
            "Epochs [6000/2000]\n",
            "Training: Losses: 0.3458070159, Accuracy: 0.96875\n",
            "Testing: Losses: 0.3607226610, Accuracy: 0.9519230769230769\n",
            "Epochs [8000/2000]\n",
            "Training: Losses: 0.3178748190, Accuracy: 1.0\n",
            "Testing: Losses: 0.3609236479, Accuracy: 0.9519230769230769\n",
            "Epochs [10000/2000]\n",
            "Training: Losses: 0.4077432156, Accuracy: 0.90625\n",
            "Testing: Losses: 0.3612022102, Accuracy: 0.9519230769230769\n",
            "Epochs [12000/2000]\n",
            "Training: Losses: 0.4075320065, Accuracy: 0.90625\n",
            "Testing: Losses: 0.3611432612, Accuracy: 0.9519230769230769\n",
            "Epochs [14000/2000]\n",
            "Training: Losses: 0.3134522140, Accuracy: 1.0\n",
            "Testing: Losses: 0.3609302938, Accuracy: 0.9519230769230769\n",
            "Epochs [16000/2000]\n",
            "Training: Losses: 0.3758438528, Accuracy: 0.9375\n",
            "Testing: Losses: 0.3610412478, Accuracy: 0.9519230769230769\n",
            "Epochs [18000/2000]\n",
            "Training: Losses: 0.3784714937, Accuracy: 0.9375\n",
            "Testing: Losses: 0.3531441391, Accuracy: 0.9615384615384616\n",
            "Epochs [20000/2000]\n",
            "Training: Losses: 0.4082419872, Accuracy: 0.90625\n",
            "Testing: Losses: 0.3518560529, Accuracy: 0.9615384615384616\n"
          ]
        }
      ],
      "source": [
        "# Prepare training and testing data\n",
        "train_x, test_x, train_y, test_y = data_split(sylhet_df)\n",
        "\n",
        "train_x = torch.from_numpy(train_x.to_numpy()).float()\n",
        "train_y = torch.squeeze(torch.from_numpy(train_y.to_numpy()).float())\n",
        "\n",
        "test_x = torch.from_numpy(test_x.to_numpy()).float()\n",
        "test_y = torch.squeeze(torch.from_numpy(test_y.to_numpy()).float())\n",
        "\n",
        "# print(train_x.shape, train_y.shape)\n",
        "# print(test_x.shape, test_y.shape)\n",
        "\n",
        "\n",
        "# Building FCNN\n",
        "class bpnn(nn.Module):\n",
        "  # Initialize the neural network with three fully connected hidden layers\n",
        "  def __init__(self, input, hid1, numclases):\n",
        "    super(bpnn, self).__init__()\n",
        "    # an affine operation: y = Wx + b\n",
        "    self.fc1 = nn.Linear(input, hid1)\n",
        "    self.fc2 = nn.Linear(hid1, numclass)\n",
        "  # feedforward\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x)) # Activation using sigmoid function for regularization\n",
        "    x = F.softmax(self.fc2(x))\n",
        "    return x\n",
        "\n",
        "input = train_x.shape[1] \n",
        "hid1 = 10\n",
        "numclass = 2\n",
        "\n",
        "batch = 32\n",
        "\n",
        "modl = bpnn(input, hid1, numclass)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optim = torch.optim.SGD(modl.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(20000):\n",
        "  shuffle_indices = np.random.permutation(np.arange(train_y.shape[0]))\n",
        "  source = train_x[shuffle_indices]\n",
        "  target = train_y[shuffle_indices]\n",
        "\n",
        "  for batch_i in range(0, len(source)//batch):\n",
        "    start_i = batch_i * batch\n",
        "    source_batch = source[start_i:start_i + batch]\n",
        "    target_batch = target[start_i:start_i + batch]\n",
        "    train_pred = modl(source_batch)\n",
        "    train_pred = torch.squeeze(train_pred)\n",
        "    losses = criterion(train_pred, target_batch.type(torch.LongTensor))\n",
        "    optim.zero_grad()\n",
        "    losses.backward()\n",
        "    optim.step() # update weights and bias\n",
        "\n",
        "  if (epoch+1) % 2000 == 0:\n",
        "    train_pred_np = torch.argmax(train_pred, axis=1).numpy()\n",
        "    train_acc = accuracy_score(target_batch, train_pred_np)\n",
        "\n",
        "    test_pred = modl(test_x)\n",
        "    test_pred = torch.squeeze(test_pred)\n",
        "\n",
        "    test_loss = criterion(test_pred, test_y.type(torch.LongTensor))\n",
        "\n",
        "    test_pred_np = torch.argmax(test_pred, axis=1).numpy()\n",
        "    test_acc = accuracy_score(test_y, test_pred_np)\n",
        "\n",
        "\n",
        "    print (f'Epochs [{epoch+1}/2000]')\n",
        "    print (f'Training: Losses: {losses.item():.10f}, Accuracy: {train_acc}')\n",
        "    print (f'Testing: Losses: {test_loss.item():.10f}, Accuracy: {test_acc}')"
      ],
      "id": "XkGViHgRjKif"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "737d6daa-3d3b-4399-a8f4-d6a72000cacb"
      },
      "source": [
        "-----\n",
        "\n",
        "### 3.2 Logistic Regression"
      ],
      "id": "737d6daa-3d3b-4399-a8f4-d6a72000cacb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35d630ee-9665-4df1-b890-17a847678415",
        "outputId": "99e1e9b9-4d68-4158-eec5-b0771ce1e33e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.9326923076923077\n",
            "Testing Accuracy: 0.9519230769230769\n",
            "Testing: Sensitivity/Recall: 0.96875 Specificity: 0.925\n"
          ]
        }
      ],
      "source": [
        "lg_classifier(sylhet_df)"
      ],
      "id": "35d630ee-9665-4df1-b890-17a847678415"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.2 Keras\n",
        "3.3.2.1 One hidden layers with 2 neurons in output layer"
      ],
      "metadata": {
        "id": "dNFiuUvxB5Pb"
      },
      "id": "dNFiuUvxB5Pb"
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, test_x, train_y, test_y = data_split(sylhet_df)\n",
        "\n",
        "train_x = np.float32(train_x)\n",
        "test_x = np.float32(test_x)\n",
        "\n",
        "train_x = train_x/train_x.max()\n",
        "test_x = test_x/test_x.max()\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(2)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_x, train_y, epochs=200, batch_size=1024,\n",
        "          validation_data=(test_x, test_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqlwMKzJB64q",
        "outputId": "859b624e-e1a0-41e2-ff0a-0f409251b976"
      },
      "id": "qqlwMKzJB64q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6799 - accuracy: 0.6154 - val_loss: 0.6794 - val_accuracy: 0.6154\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.6776 - accuracy: 0.6154 - val_loss: 0.6775 - val_accuracy: 0.6154\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.6755 - accuracy: 0.6154 - val_loss: 0.6758 - val_accuracy: 0.6154\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.6737 - accuracy: 0.6154 - val_loss: 0.6743 - val_accuracy: 0.6154\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.6720 - accuracy: 0.6154 - val_loss: 0.6728 - val_accuracy: 0.6154\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.6704 - accuracy: 0.6154 - val_loss: 0.6716 - val_accuracy: 0.6154\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.6689 - accuracy: 0.6154 - val_loss: 0.6704 - val_accuracy: 0.6154\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.6676 - accuracy: 0.6154 - val_loss: 0.6693 - val_accuracy: 0.6154\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6684 - val_accuracy: 0.6154\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.6653 - accuracy: 0.6154 - val_loss: 0.6676 - val_accuracy: 0.6154\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.6643 - accuracy: 0.6154 - val_loss: 0.6669 - val_accuracy: 0.6154\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.6635 - accuracy: 0.6154 - val_loss: 0.6664 - val_accuracy: 0.6154\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.6628 - accuracy: 0.6154 - val_loss: 0.6659 - val_accuracy: 0.6154\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.6622 - accuracy: 0.6154 - val_loss: 0.6656 - val_accuracy: 0.6154\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.6617 - accuracy: 0.6154 - val_loss: 0.6654 - val_accuracy: 0.6154\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.6613 - accuracy: 0.6154 - val_loss: 0.6652 - val_accuracy: 0.6154\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.6610 - accuracy: 0.6154 - val_loss: 0.6651 - val_accuracy: 0.6154\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.6607 - accuracy: 0.6154 - val_loss: 0.6651 - val_accuracy: 0.6154\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.6606 - accuracy: 0.6154 - val_loss: 0.6650 - val_accuracy: 0.6154\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.6604 - accuracy: 0.6154 - val_loss: 0.6651 - val_accuracy: 0.6154\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.6603 - accuracy: 0.6154 - val_loss: 0.6651 - val_accuracy: 0.6154\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.6603 - accuracy: 0.6154 - val_loss: 0.6651 - val_accuracy: 0.6154\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.6602 - accuracy: 0.6154 - val_loss: 0.6651 - val_accuracy: 0.6154\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.6602 - accuracy: 0.6154 - val_loss: 0.6651 - val_accuracy: 0.6154\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.6601 - accuracy: 0.6154 - val_loss: 0.6651 - val_accuracy: 0.6154\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.6601 - accuracy: 0.6154 - val_loss: 0.6650 - val_accuracy: 0.6154\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.6600 - accuracy: 0.6154 - val_loss: 0.6650 - val_accuracy: 0.6154\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.6599 - accuracy: 0.6154 - val_loss: 0.6649 - val_accuracy: 0.6154\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.6598 - accuracy: 0.6154 - val_loss: 0.6647 - val_accuracy: 0.6154\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.6596 - accuracy: 0.6154 - val_loss: 0.6646 - val_accuracy: 0.6154\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.6595 - accuracy: 0.6154 - val_loss: 0.6645 - val_accuracy: 0.6154\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.6593 - accuracy: 0.6154 - val_loss: 0.6643 - val_accuracy: 0.6154\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.6592 - accuracy: 0.6154 - val_loss: 0.6641 - val_accuracy: 0.6154\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.6590 - accuracy: 0.6154 - val_loss: 0.6639 - val_accuracy: 0.6154\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.6588 - accuracy: 0.6154 - val_loss: 0.6637 - val_accuracy: 0.6154\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.6586 - accuracy: 0.6154 - val_loss: 0.6635 - val_accuracy: 0.6154\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.6585 - accuracy: 0.6154 - val_loss: 0.6634 - val_accuracy: 0.6154\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.6583 - accuracy: 0.6154 - val_loss: 0.6632 - val_accuracy: 0.6154\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.6581 - accuracy: 0.6154 - val_loss: 0.6630 - val_accuracy: 0.6154\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.6580 - accuracy: 0.6154 - val_loss: 0.6629 - val_accuracy: 0.6154\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.6578 - accuracy: 0.6154 - val_loss: 0.6627 - val_accuracy: 0.6154\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.6577 - accuracy: 0.6154 - val_loss: 0.6626 - val_accuracy: 0.6154\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.6575 - accuracy: 0.6154 - val_loss: 0.6624 - val_accuracy: 0.6154\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.6574 - accuracy: 0.6154 - val_loss: 0.6623 - val_accuracy: 0.6154\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.6572 - accuracy: 0.6154 - val_loss: 0.6621 - val_accuracy: 0.6154\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.6571 - accuracy: 0.6154 - val_loss: 0.6620 - val_accuracy: 0.6154\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.6569 - accuracy: 0.6154 - val_loss: 0.6619 - val_accuracy: 0.6154\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.6568 - accuracy: 0.6154 - val_loss: 0.6617 - val_accuracy: 0.6154\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.6566 - accuracy: 0.6154 - val_loss: 0.6616 - val_accuracy: 0.6154\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.6564 - accuracy: 0.6154 - val_loss: 0.6614 - val_accuracy: 0.6154\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6562 - accuracy: 0.6154 - val_loss: 0.6613 - val_accuracy: 0.6154\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.6560 - accuracy: 0.6154 - val_loss: 0.6611 - val_accuracy: 0.6154\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.6558 - accuracy: 0.6154 - val_loss: 0.6610 - val_accuracy: 0.6154\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.6556 - accuracy: 0.6154 - val_loss: 0.6608 - val_accuracy: 0.6154\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.6554 - accuracy: 0.6154 - val_loss: 0.6607 - val_accuracy: 0.6154\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.6552 - accuracy: 0.6154 - val_loss: 0.6605 - val_accuracy: 0.6154\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.6550 - accuracy: 0.6154 - val_loss: 0.6604 - val_accuracy: 0.6154\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.6548 - accuracy: 0.6154 - val_loss: 0.6602 - val_accuracy: 0.6154\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.6545 - accuracy: 0.6154 - val_loss: 0.6601 - val_accuracy: 0.6154\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.6543 - accuracy: 0.6154 - val_loss: 0.6599 - val_accuracy: 0.6154\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.6541 - accuracy: 0.6154 - val_loss: 0.6597 - val_accuracy: 0.6154\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.6538 - accuracy: 0.6154 - val_loss: 0.6596 - val_accuracy: 0.6154\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.6536 - accuracy: 0.6154 - val_loss: 0.6594 - val_accuracy: 0.6154\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.6533 - accuracy: 0.6154 - val_loss: 0.6592 - val_accuracy: 0.6154\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.6531 - accuracy: 0.6154 - val_loss: 0.6590 - val_accuracy: 0.6154\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.6528 - accuracy: 0.6154 - val_loss: 0.6588 - val_accuracy: 0.6154\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.6525 - accuracy: 0.6154 - val_loss: 0.6586 - val_accuracy: 0.6154\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.6522 - accuracy: 0.6154 - val_loss: 0.6583 - val_accuracy: 0.6154\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.6520 - accuracy: 0.6154 - val_loss: 0.6581 - val_accuracy: 0.6154\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.6517 - accuracy: 0.6154 - val_loss: 0.6578 - val_accuracy: 0.6154\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.6513 - accuracy: 0.6154 - val_loss: 0.6576 - val_accuracy: 0.6154\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.6510 - accuracy: 0.6154 - val_loss: 0.6573 - val_accuracy: 0.6154\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.6507 - accuracy: 0.6154 - val_loss: 0.6571 - val_accuracy: 0.6154\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.6504 - accuracy: 0.6154 - val_loss: 0.6568 - val_accuracy: 0.6154\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.6501 - accuracy: 0.6154 - val_loss: 0.6565 - val_accuracy: 0.6154\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.6497 - accuracy: 0.6154 - val_loss: 0.6563 - val_accuracy: 0.6154\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.6494 - accuracy: 0.6154 - val_loss: 0.6560 - val_accuracy: 0.6154\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.6490 - accuracy: 0.6154 - val_loss: 0.6557 - val_accuracy: 0.6154\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.6486 - accuracy: 0.6154 - val_loss: 0.6554 - val_accuracy: 0.6154\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.6483 - accuracy: 0.6154 - val_loss: 0.6551 - val_accuracy: 0.6154\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.6479 - accuracy: 0.6154 - val_loss: 0.6548 - val_accuracy: 0.6154\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.6475 - accuracy: 0.6154 - val_loss: 0.6545 - val_accuracy: 0.6154\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.6471 - accuracy: 0.6154 - val_loss: 0.6542 - val_accuracy: 0.6154\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.6466 - accuracy: 0.6154 - val_loss: 0.6539 - val_accuracy: 0.6154\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.6462 - accuracy: 0.6154 - val_loss: 0.6536 - val_accuracy: 0.6154\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.6458 - accuracy: 0.6154 - val_loss: 0.6532 - val_accuracy: 0.6154\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.6453 - accuracy: 0.6154 - val_loss: 0.6529 - val_accuracy: 0.6154\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.6449 - accuracy: 0.6154 - val_loss: 0.6526 - val_accuracy: 0.6154\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.6444 - accuracy: 0.6154 - val_loss: 0.6522 - val_accuracy: 0.6154\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.6439 - accuracy: 0.6154 - val_loss: 0.6519 - val_accuracy: 0.6154\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.6435 - accuracy: 0.6154 - val_loss: 0.6515 - val_accuracy: 0.6154\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.6430 - accuracy: 0.6154 - val_loss: 0.6512 - val_accuracy: 0.6154\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.6425 - accuracy: 0.6154 - val_loss: 0.6508 - val_accuracy: 0.6154\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.6420 - accuracy: 0.6154 - val_loss: 0.6504 - val_accuracy: 0.6154\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.6414 - accuracy: 0.6154 - val_loss: 0.6500 - val_accuracy: 0.6154\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.6409 - accuracy: 0.6154 - val_loss: 0.6495 - val_accuracy: 0.6154\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.6403 - accuracy: 0.6154 - val_loss: 0.6491 - val_accuracy: 0.6154\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.6397 - accuracy: 0.6154 - val_loss: 0.6486 - val_accuracy: 0.6154\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.6391 - accuracy: 0.6154 - val_loss: 0.6481 - val_accuracy: 0.6154\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.6385 - accuracy: 0.6154 - val_loss: 0.6477 - val_accuracy: 0.6154\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.6379 - accuracy: 0.6154 - val_loss: 0.6472 - val_accuracy: 0.6154\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.6373 - accuracy: 0.6154 - val_loss: 0.6467 - val_accuracy: 0.6154\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.6367 - accuracy: 0.6154 - val_loss: 0.6462 - val_accuracy: 0.6154\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.6360 - accuracy: 0.6154 - val_loss: 0.6456 - val_accuracy: 0.6154\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.6354 - accuracy: 0.6154 - val_loss: 0.6451 - val_accuracy: 0.6154\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.6347 - accuracy: 0.6154 - val_loss: 0.6446 - val_accuracy: 0.6154\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.6341 - accuracy: 0.6154 - val_loss: 0.6440 - val_accuracy: 0.6154\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.6334 - accuracy: 0.6154 - val_loss: 0.6435 - val_accuracy: 0.6154\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.6327 - accuracy: 0.6154 - val_loss: 0.6429 - val_accuracy: 0.6154\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.6320 - accuracy: 0.6154 - val_loss: 0.6423 - val_accuracy: 0.6154\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.6312 - accuracy: 0.6154 - val_loss: 0.6417 - val_accuracy: 0.6154\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.6305 - accuracy: 0.6154 - val_loss: 0.6410 - val_accuracy: 0.6154\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.6297 - accuracy: 0.6154 - val_loss: 0.6403 - val_accuracy: 0.6154\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.6289 - accuracy: 0.6130 - val_loss: 0.6396 - val_accuracy: 0.6154\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.6279 - accuracy: 0.6130 - val_loss: 0.6388 - val_accuracy: 0.6250\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.6269 - accuracy: 0.6130 - val_loss: 0.6380 - val_accuracy: 0.6250\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.6259 - accuracy: 0.6274 - val_loss: 0.6372 - val_accuracy: 0.6250\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.6250 - accuracy: 0.6274 - val_loss: 0.6365 - val_accuracy: 0.6250\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.6241 - accuracy: 0.6274 - val_loss: 0.6357 - val_accuracy: 0.6250\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.6233 - accuracy: 0.6274 - val_loss: 0.6350 - val_accuracy: 0.6346\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.6224 - accuracy: 0.6322 - val_loss: 0.6342 - val_accuracy: 0.6731\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.6214 - accuracy: 0.6635 - val_loss: 0.6334 - val_accuracy: 0.6635\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.6205 - accuracy: 0.6635 - val_loss: 0.6326 - val_accuracy: 0.6731\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.6196 - accuracy: 0.6635 - val_loss: 0.6317 - val_accuracy: 0.6731\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.6186 - accuracy: 0.6611 - val_loss: 0.6309 - val_accuracy: 0.6731\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6176 - accuracy: 0.6611 - val_loss: 0.6300 - val_accuracy: 0.6731\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.6166 - accuracy: 0.6611 - val_loss: 0.6292 - val_accuracy: 0.6731\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.6156 - accuracy: 0.6611 - val_loss: 0.6283 - val_accuracy: 0.6731\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.6146 - accuracy: 0.6611 - val_loss: 0.6273 - val_accuracy: 0.6731\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.6135 - accuracy: 0.6611 - val_loss: 0.6263 - val_accuracy: 0.6731\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.6125 - accuracy: 0.6611 - val_loss: 0.6253 - val_accuracy: 0.6827\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.6114 - accuracy: 0.6635 - val_loss: 0.6242 - val_accuracy: 0.6827\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.6103 - accuracy: 0.6707 - val_loss: 0.6232 - val_accuracy: 0.6827\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.6092 - accuracy: 0.6707 - val_loss: 0.6221 - val_accuracy: 0.6827\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.6080 - accuracy: 0.6731 - val_loss: 0.6209 - val_accuracy: 0.6827\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.6068 - accuracy: 0.6731 - val_loss: 0.6198 - val_accuracy: 0.6731\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.6056 - accuracy: 0.6755 - val_loss: 0.6186 - val_accuracy: 0.6731\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.6044 - accuracy: 0.6755 - val_loss: 0.6173 - val_accuracy: 0.6731\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.6032 - accuracy: 0.6755 - val_loss: 0.6160 - val_accuracy: 0.6731\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.6019 - accuracy: 0.6779 - val_loss: 0.6147 - val_accuracy: 0.6731\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.6007 - accuracy: 0.6779 - val_loss: 0.6134 - val_accuracy: 0.6731\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.5994 - accuracy: 0.6851 - val_loss: 0.6120 - val_accuracy: 0.6731\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.5980 - accuracy: 0.6851 - val_loss: 0.6106 - val_accuracy: 0.6731\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.5967 - accuracy: 0.6923 - val_loss: 0.6092 - val_accuracy: 0.6731\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.5953 - accuracy: 0.6923 - val_loss: 0.6077 - val_accuracy: 0.6731\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.5939 - accuracy: 0.6923 - val_loss: 0.6061 - val_accuracy: 0.6731\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.5925 - accuracy: 0.6923 - val_loss: 0.6046 - val_accuracy: 0.6923\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.5910 - accuracy: 0.7043 - val_loss: 0.6029 - val_accuracy: 0.6923\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.5895 - accuracy: 0.7019 - val_loss: 0.6013 - val_accuracy: 0.6923\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.5880 - accuracy: 0.7019 - val_loss: 0.5996 - val_accuracy: 0.7115\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.5865 - accuracy: 0.7091 - val_loss: 0.5979 - val_accuracy: 0.7115\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5849 - accuracy: 0.7091 - val_loss: 0.5962 - val_accuracy: 0.7115\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.5833 - accuracy: 0.7067 - val_loss: 0.5944 - val_accuracy: 0.7115\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.5817 - accuracy: 0.7115 - val_loss: 0.5926 - val_accuracy: 0.7019\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5801 - accuracy: 0.7236 - val_loss: 0.5907 - val_accuracy: 0.7115\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.5784 - accuracy: 0.7356 - val_loss: 0.5888 - val_accuracy: 0.7308\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.5767 - accuracy: 0.7476 - val_loss: 0.5869 - val_accuracy: 0.7404\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.5749 - accuracy: 0.7428 - val_loss: 0.5849 - val_accuracy: 0.7404\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.5732 - accuracy: 0.7428 - val_loss: 0.5829 - val_accuracy: 0.7404\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.5714 - accuracy: 0.7428 - val_loss: 0.5808 - val_accuracy: 0.7404\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.5696 - accuracy: 0.7428 - val_loss: 0.5787 - val_accuracy: 0.7404\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.5677 - accuracy: 0.7428 - val_loss: 0.5766 - val_accuracy: 0.7404\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.5659 - accuracy: 0.7452 - val_loss: 0.5744 - val_accuracy: 0.7404\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.5640 - accuracy: 0.7452 - val_loss: 0.5722 - val_accuracy: 0.7404\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.5621 - accuracy: 0.7452 - val_loss: 0.5700 - val_accuracy: 0.7404\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.5602 - accuracy: 0.7452 - val_loss: 0.5678 - val_accuracy: 0.7404\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 0.5582 - accuracy: 0.7452 - val_loss: 0.5655 - val_accuracy: 0.7404\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.5562 - accuracy: 0.7452 - val_loss: 0.5632 - val_accuracy: 0.7500\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.5542 - accuracy: 0.7476 - val_loss: 0.5610 - val_accuracy: 0.7500\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.5522 - accuracy: 0.7476 - val_loss: 0.5587 - val_accuracy: 0.7500\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.5501 - accuracy: 0.7548 - val_loss: 0.5563 - val_accuracy: 0.7500\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.5480 - accuracy: 0.7548 - val_loss: 0.5539 - val_accuracy: 0.7500\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.5459 - accuracy: 0.7548 - val_loss: 0.5515 - val_accuracy: 0.7500\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.5438 - accuracy: 0.7548 - val_loss: 0.5491 - val_accuracy: 0.7500\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.5416 - accuracy: 0.7572 - val_loss: 0.5467 - val_accuracy: 0.7500\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.5395 - accuracy: 0.7620 - val_loss: 0.5443 - val_accuracy: 0.7500\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.5373 - accuracy: 0.7644 - val_loss: 0.5417 - val_accuracy: 0.7500\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.5351 - accuracy: 0.7716 - val_loss: 0.5392 - val_accuracy: 0.7500\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.5329 - accuracy: 0.7740 - val_loss: 0.5367 - val_accuracy: 0.7500\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.5306 - accuracy: 0.7788 - val_loss: 0.5342 - val_accuracy: 0.7500\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.5284 - accuracy: 0.7788 - val_loss: 0.5317 - val_accuracy: 0.7500\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.5261 - accuracy: 0.7788 - val_loss: 0.5292 - val_accuracy: 0.7596\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.5238 - accuracy: 0.7788 - val_loss: 0.5266 - val_accuracy: 0.7788\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.5215 - accuracy: 0.7861 - val_loss: 0.5240 - val_accuracy: 0.7788\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.5192 - accuracy: 0.7861 - val_loss: 0.5214 - val_accuracy: 0.7885\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.5169 - accuracy: 0.7861 - val_loss: 0.5188 - val_accuracy: 0.7885\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.5145 - accuracy: 0.7909 - val_loss: 0.5162 - val_accuracy: 0.7885\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.5122 - accuracy: 0.7933 - val_loss: 0.5136 - val_accuracy: 0.7885\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.5098 - accuracy: 0.7933 - val_loss: 0.5110 - val_accuracy: 0.7885\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.5074 - accuracy: 0.7933 - val_loss: 0.5083 - val_accuracy: 0.7885\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.5050 - accuracy: 0.8029 - val_loss: 0.5057 - val_accuracy: 0.7885\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.5026 - accuracy: 0.8005 - val_loss: 0.5030 - val_accuracy: 0.7885\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.5002 - accuracy: 0.8029 - val_loss: 0.5004 - val_accuracy: 0.7885\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.4978 - accuracy: 0.8125 - val_loss: 0.4978 - val_accuracy: 0.7885\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.4954 - accuracy: 0.8125 - val_loss: 0.4951 - val_accuracy: 0.7885\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.4930 - accuracy: 0.8125 - val_loss: 0.4925 - val_accuracy: 0.7885\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.4906 - accuracy: 0.8125 - val_loss: 0.4898 - val_accuracy: 0.7885\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.4882 - accuracy: 0.8125 - val_loss: 0.4871 - val_accuracy: 0.7885\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.4857 - accuracy: 0.8125 - val_loss: 0.4845 - val_accuracy: 0.7885\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.4833 - accuracy: 0.8125 - val_loss: 0.4818 - val_accuracy: 0.7885\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe4920d4150>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee69d18c-fe3a-4abe-a760-839c986496fa"
      },
      "source": [
        "-----\n",
        "## 4. Birla Institute of Technology, Mesra Diabetes Dataset\n",
        "\n",
        "This dataset was collected in 2019 by Neha Prerna Tigga and Dr. Shruti Garg of the Department of Computer Science and Engineering, BIT Mesra, Ranchi-835215 for research, non-commercial purposes only. An article is also published implementing this dataset. For more information and citation of this dataset please refer:\n",
        "\n",
        "Tigga, N. P., & Garg, S. (2020). Prediction of Type 2 Diabetes using Machine Learning Classification Methods. Procedia Computer Science, 167, 706-716. DOI: https://doi.org/10.1016/j.procs.2020.03.336\n",
        "\n",
        "There is a total of 952 instances with 17 independent predictor variables and one binary target or dependent variable, `Diabetes`\n",
        "\n",
        "####  Attribute Information\n",
        "| Column Name    | Expression    |\n",
        "| :------------- | :------------- |\n",
        "| Age |less than 40, 40-49, 50-59, 60 or older|\n",
        "| Gender        |Male, Female|\n",
        "| Family_Diabetes        |yes, no|\n",
        "| highBP |yes, no|\n",
        "| PhysicallyActive        |none, less than half an hr, more than half an hr, one hr or more|\n",
        "| BMI        |Body mass index|\n",
        "| Smoking       |yes, no|\n",
        "| Alcohol        |yes, no|\n",
        "| Sleep        |in numerical hrs|\n",
        "| SoundSleep        |in numerical hrs|\n",
        "| RegularMedicine        |yes, no|\n",
        "| JunkFood        |occasionally, sometimes, often, very often, always|\n",
        "| Stress        |not at all, sometimes, very often, always|\n",
        "| BPLevel      |normal, high, low|\n",
        "| Pregancies        |Times|\n",
        "| Pdiabetes    |0, yes, no|\n",
        "| UriationFreq       |not much, quite often|\n",
        "| Diabetic   |yes, no|\n"
      ],
      "id": "ee69d18c-fe3a-4abe-a760-839c986496fa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3c3b119-b532-467d-840b-40551f6d77a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "95c655ad-70d0-4b1b-c457-94d4cb3eea9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Any null value: True\n",
            "Any NaN value: True\n",
            "Before Droping NA Number of Rows: 952\n",
            "After Droping NA Number of Rows: 905\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Age  Gender  Family_Diabetes  highBP  PhysicallyActive   BMI  Smoking  \\\n",
              "900    3       1                1       0                 1  25.0        0   \n",
              "901    2       1                1       1                 1  27.0        0   \n",
              "902    2       1                0       1                 2  23.0        0   \n",
              "903    2       1                0       1                 0  27.0        0   \n",
              "904    2       0                1       1                 3  30.0        0   \n",
              "\n",
              "     Alcohol  Sleep  SoundSleep  RegularMedicine  JunkFood  Stress  BPLevel  \\\n",
              "900        0      8           6                0         2       2        4   \n",
              "901        0      6           5                1         1       2        2   \n",
              "902        0      6           5                1         1       2        2   \n",
              "903        1      6           5                1         1       3        2   \n",
              "904        0      7           4                1         1       2        2   \n",
              "\n",
              "     Pregancies  Pdiabetes  UriationFreq  Diabetic  \n",
              "900         0.0          0             0         1  \n",
              "901         0.0          0             1         1  \n",
              "902         0.0          0             0         0  \n",
              "903         0.0          0             0         0  \n",
              "904         2.0          0             1         1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1316f2a5-2402-4b5b-b9ae-11a7749aafd8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Family_Diabetes</th>\n",
              "      <th>highBP</th>\n",
              "      <th>PhysicallyActive</th>\n",
              "      <th>BMI</th>\n",
              "      <th>Smoking</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Sleep</th>\n",
              "      <th>SoundSleep</th>\n",
              "      <th>RegularMedicine</th>\n",
              "      <th>JunkFood</th>\n",
              "      <th>Stress</th>\n",
              "      <th>BPLevel</th>\n",
              "      <th>Pregancies</th>\n",
              "      <th>Pdiabetes</th>\n",
              "      <th>UriationFreq</th>\n",
              "      <th>Diabetic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>900</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>901</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>902</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>903</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>904</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1316f2a5-2402-4b5b-b9ae-11a7749aafd8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1316f2a5-2402-4b5b-b9ae-11a7749aafd8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1316f2a5-2402-4b5b-b9ae-11a7749aafd8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Read 'BIT_2019.csv'\n",
        "bit_df = pd.read_csv(\"./data/BIT_2019.csv\")\n",
        "\n",
        "# Drop NaN value\n",
        "print(\"Any null value:\", any(bit_df.isnull()))\n",
        "print(\"Any NaN value:\", any(bit_df.isna()))\n",
        "print(\"Before Droping NA Number of Rows:\", len(bit_df))\n",
        "\n",
        "bit_df = bit_df.dropna()\n",
        "print(\"After Droping NA Number of Rows:\", len(bit_df))\n",
        "\n",
        "# Reset the Index after dropping NaN\n",
        "bit_df = bit_df.reset_index(drop=True)\n",
        "\n",
        "# Encode textual columns\n",
        "le = LabelEncoder()\n",
        "for each in ['Age', 'Gender', 'Family_Diabetes', 'highBP', 'PhysicallyActive', 'Smoking', 'Alcohol', 'RegularMedicine', 'JunkFood', 'Stress', 'BPLevel', 'Pdiabetes', 'UriationFreq', 'Diabetic']:\n",
        "    bit_df[each] = le.fit_transform(bit_df[each])\n",
        "\n",
        "\n",
        "bit_df.tail()\n"
      ],
      "id": "b3c3b119-b532-467d-840b-40551f6d77a0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "034f3a4a-a332-4dea-a6d9-45bcc37d5e83"
      },
      "outputs": [],
      "source": [
        "# Correlation contingency table\n",
        "bit_corr = bit_df.corr()\n",
        "bit_corr"
      ],
      "id": "034f3a4a-a332-4dea-a6d9-45bcc37d5e83"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74c76793-cf32-4a7e-9397-0e33f27e4de3"
      },
      "source": [
        "Moreover, we evaluate the Weighted Average of the correlation of predictors:\n",
        "\n",
        "$\\text{Weighted Average of Correlation}$ $=$ $\\frac{\\text{Sum of Correlation of Predictors}}{\\text{Number of Predictors}}$ $=$ $0.061\n",
        "$"
      ],
      "id": "74c76793-cf32-4a7e-9397-0e33f27e4de3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2ee0671-19cc-4fe0-9b38-f1fe088192b6"
      },
      "outputs": [],
      "source": [
        "# Evaluate the Weighted Average of the correlation of predictors\n",
        "\n",
        "print(\"Weighted Average:\", sum(bit_corr.iloc[-1,:-1])/17)"
      ],
      "id": "b2ee0671-19cc-4fe0-9b38-f1fe088192b6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a224617a-c912-4d4a-8bb0-33641e1c2d95"
      },
      "source": [
        "-----\n",
        "\n",
        "### 4.1 KNN"
      ],
      "id": "a224617a-c912-4d4a-8bb0-33641e1c2d95"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3ca2a7c-ed6c-471a-8f02-cbd7a7848e62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "093375ba-d0d6-4491-bde4-f6285bf47eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k: 1 Training Score: 0.9758203799654577 Validation Score:  0.9172413793103448\n",
            "k: 2 Training Score: 0.9620034542314335 Validation Score:  0.9103448275862069\n",
            "k: 3 Training Score: 0.9585492227979274 Validation Score:  0.8689655172413793\n",
            "k: 5 Training Score: 0.92573402417962 Validation Score:  0.8\n",
            "k: 7 Training Score: 0.9067357512953368 Validation Score:  0.8206896551724138\n",
            "k: 9 Training Score: 0.8791018998272885 Validation Score:  0.8344827586206897\n",
            "k: 15 Training Score: 0.8670120898100173 Validation Score:  0.7862068965517242\n",
            "k: 31 Training Score: 0.8687392055267703 Validation Score:  0.8275862068965517\n",
            "k: 51 Training Score: 0.8255613126079447 Validation Score:  0.7793103448275862\n",
            "k: 579 Training Score: 0.7202072538860104 Validation Score:  0.7172413793103448\n",
            "The best k is 1 and the best val score is 0.9172\n",
            "Best Model Validation Score: 0.9172 \n",
            " Training Score: 0.9758 \n",
            " Test Score: 0.9282\n",
            "Testing: Sensitivity/Recall: 0.8833333333333333 Specificity: 0.9504132231404959\n"
          ]
        }
      ],
      "source": [
        "knn_classifier(bit_df)"
      ],
      "id": "a3ca2a7c-ed6c-471a-8f02-cbd7a7848e62"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85f01a07-da97-4f9a-bff3-97a16443b0c7"
      },
      "source": [
        "-----\n",
        "\n",
        "### 4.2 Logistic Regression"
      ],
      "id": "85f01a07-da97-4f9a-bff3-97a16443b0c7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7377f66-f2c1-4384-b703-7b3de3b6ad8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16158e70-4539-4d3f-9717-e35fddc33613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.8729281767955801\n",
            "Testing Accuracy: 0.8342541436464088\n",
            "Testing: Sensitivity/Recall: 0.6333333333333333 Specificity: 0.9338842975206612\n"
          ]
        }
      ],
      "source": [
        "lg_classifier(bit_df)"
      ],
      "id": "a7377f66-f2c1-4384-b703-7b3de3b6ad8c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3.2 Keras\n",
        "4.3.2.1 One hidden layers with 2 neurons in output layer"
      ],
      "metadata": {
        "id": "w3-zq8JfCfo0"
      },
      "id": "w3-zq8JfCfo0"
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, test_x, train_y, test_y = data_split(bit_df)\n",
        "\n",
        "train_x = np.float32(train_x)\n",
        "test_x = np.float32(test_x)\n",
        "\n",
        "train_x = train_x/train_x.max()\n",
        "test_x = test_x/test_x.max()\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(2)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_x, train_y, epochs=200, batch_size=1024,\n",
        "          validation_data=(test_x, test_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7DZj9ioCeKY",
        "outputId": "552ba992-951d-40a0-89e5-49f5899be07a"
      },
      "id": "o7DZj9ioCeKY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 1s 988ms/step - loss: 0.6746 - accuracy: 0.7196 - val_loss: 0.6696 - val_accuracy: 0.6685\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.6677 - accuracy: 0.7196 - val_loss: 0.6640 - val_accuracy: 0.6685\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6608 - accuracy: 0.7196 - val_loss: 0.6588 - val_accuracy: 0.6685\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6540 - accuracy: 0.7196 - val_loss: 0.6540 - val_accuracy: 0.6685\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6475 - accuracy: 0.7196 - val_loss: 0.6497 - val_accuracy: 0.6685\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.6413 - accuracy: 0.7196 - val_loss: 0.6459 - val_accuracy: 0.6685\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6355 - accuracy: 0.7196 - val_loss: 0.6426 - val_accuracy: 0.6685\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.6301 - accuracy: 0.7196 - val_loss: 0.6397 - val_accuracy: 0.6685\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.6250 - accuracy: 0.7196 - val_loss: 0.6373 - val_accuracy: 0.6685\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.6203 - accuracy: 0.7196 - val_loss: 0.6354 - val_accuracy: 0.6685\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.6161 - accuracy: 0.7196 - val_loss: 0.6340 - val_accuracy: 0.6685\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.6123 - accuracy: 0.7196 - val_loss: 0.6331 - val_accuracy: 0.6685\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.6089 - accuracy: 0.7196 - val_loss: 0.6326 - val_accuracy: 0.6685\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6059 - accuracy: 0.7196 - val_loss: 0.6325 - val_accuracy: 0.6685\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6033 - accuracy: 0.7196 - val_loss: 0.6328 - val_accuracy: 0.6685\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.6011 - accuracy: 0.7196 - val_loss: 0.6335 - val_accuracy: 0.6685\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.5992 - accuracy: 0.7196 - val_loss: 0.6345 - val_accuracy: 0.6685\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.5975 - accuracy: 0.7196 - val_loss: 0.6356 - val_accuracy: 0.6685\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.5961 - accuracy: 0.7196 - val_loss: 0.6370 - val_accuracy: 0.6685\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.5950 - accuracy: 0.7196 - val_loss: 0.6384 - val_accuracy: 0.6685\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.5940 - accuracy: 0.7196 - val_loss: 0.6399 - val_accuracy: 0.6685\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.5932 - accuracy: 0.7196 - val_loss: 0.6414 - val_accuracy: 0.6685\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.5926 - accuracy: 0.7196 - val_loss: 0.6429 - val_accuracy: 0.6685\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.5921 - accuracy: 0.7196 - val_loss: 0.6443 - val_accuracy: 0.6685\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.5917 - accuracy: 0.7196 - val_loss: 0.6455 - val_accuracy: 0.6685\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.5913 - accuracy: 0.7196 - val_loss: 0.6465 - val_accuracy: 0.6685\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.5910 - accuracy: 0.7196 - val_loss: 0.6473 - val_accuracy: 0.6685\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.5906 - accuracy: 0.7196 - val_loss: 0.6478 - val_accuracy: 0.6685\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.5903 - accuracy: 0.7196 - val_loss: 0.6480 - val_accuracy: 0.6685\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.5898 - accuracy: 0.7196 - val_loss: 0.6480 - val_accuracy: 0.6685\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.5894 - accuracy: 0.7196 - val_loss: 0.6477 - val_accuracy: 0.6685\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.5888 - accuracy: 0.7196 - val_loss: 0.6471 - val_accuracy: 0.6685\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.5883 - accuracy: 0.7196 - val_loss: 0.6463 - val_accuracy: 0.6685\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.5877 - accuracy: 0.7196 - val_loss: 0.6453 - val_accuracy: 0.6685\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.5870 - accuracy: 0.7196 - val_loss: 0.6440 - val_accuracy: 0.6685\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.5863 - accuracy: 0.7196 - val_loss: 0.6426 - val_accuracy: 0.6685\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.5856 - accuracy: 0.7196 - val_loss: 0.6411 - val_accuracy: 0.6685\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.5849 - accuracy: 0.7196 - val_loss: 0.6394 - val_accuracy: 0.6685\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.5842 - accuracy: 0.7196 - val_loss: 0.6377 - val_accuracy: 0.6685\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.5835 - accuracy: 0.7196 - val_loss: 0.6360 - val_accuracy: 0.6685\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.5828 - accuracy: 0.7196 - val_loss: 0.6343 - val_accuracy: 0.6685\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.5822 - accuracy: 0.7196 - val_loss: 0.6326 - val_accuracy: 0.6685\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.5816 - accuracy: 0.7196 - val_loss: 0.6310 - val_accuracy: 0.6685\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.5810 - accuracy: 0.7196 - val_loss: 0.6295 - val_accuracy: 0.6685\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.5804 - accuracy: 0.7196 - val_loss: 0.6281 - val_accuracy: 0.6685\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.5799 - accuracy: 0.7196 - val_loss: 0.6268 - val_accuracy: 0.6685\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5794 - accuracy: 0.7196 - val_loss: 0.6256 - val_accuracy: 0.6685\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.5788 - accuracy: 0.7196 - val_loss: 0.6244 - val_accuracy: 0.6685\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.5783 - accuracy: 0.7196 - val_loss: 0.6235 - val_accuracy: 0.6685\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.5777 - accuracy: 0.7196 - val_loss: 0.6225 - val_accuracy: 0.6685\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.5771 - accuracy: 0.7196 - val_loss: 0.6217 - val_accuracy: 0.6685\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.5765 - accuracy: 0.7196 - val_loss: 0.6210 - val_accuracy: 0.6685\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.5759 - accuracy: 0.7196 - val_loss: 0.6203 - val_accuracy: 0.6685\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5752 - accuracy: 0.7196 - val_loss: 0.6197 - val_accuracy: 0.6685\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.5745 - accuracy: 0.7196 - val_loss: 0.6192 - val_accuracy: 0.6685\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.5738 - accuracy: 0.7196 - val_loss: 0.6187 - val_accuracy: 0.6685\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.5731 - accuracy: 0.7196 - val_loss: 0.6182 - val_accuracy: 0.6685\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.5724 - accuracy: 0.7196 - val_loss: 0.6178 - val_accuracy: 0.6685\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.5717 - accuracy: 0.7196 - val_loss: 0.6173 - val_accuracy: 0.6685\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.5710 - accuracy: 0.7196 - val_loss: 0.6168 - val_accuracy: 0.6685\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.5702 - accuracy: 0.7196 - val_loss: 0.6162 - val_accuracy: 0.6685\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.5695 - accuracy: 0.7196 - val_loss: 0.6155 - val_accuracy: 0.6685\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.5688 - accuracy: 0.7196 - val_loss: 0.6147 - val_accuracy: 0.6685\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.5680 - accuracy: 0.7196 - val_loss: 0.6138 - val_accuracy: 0.6685\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.5672 - accuracy: 0.7196 - val_loss: 0.6128 - val_accuracy: 0.6685\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.5665 - accuracy: 0.7196 - val_loss: 0.6117 - val_accuracy: 0.6685\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.5657 - accuracy: 0.7196 - val_loss: 0.6105 - val_accuracy: 0.6685\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.5648 - accuracy: 0.7196 - val_loss: 0.6092 - val_accuracy: 0.6685\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.5640 - accuracy: 0.7196 - val_loss: 0.6079 - val_accuracy: 0.6685\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.5631 - accuracy: 0.7196 - val_loss: 0.6065 - val_accuracy: 0.6685\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.5623 - accuracy: 0.7196 - val_loss: 0.6051 - val_accuracy: 0.6685\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.5614 - accuracy: 0.7196 - val_loss: 0.6037 - val_accuracy: 0.6685\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.5605 - accuracy: 0.7196 - val_loss: 0.6023 - val_accuracy: 0.6685\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.5596 - accuracy: 0.7196 - val_loss: 0.6009 - val_accuracy: 0.6685\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.5587 - accuracy: 0.7196 - val_loss: 0.5997 - val_accuracy: 0.6685\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.5578 - accuracy: 0.7196 - val_loss: 0.5984 - val_accuracy: 0.6685\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.5568 - accuracy: 0.7196 - val_loss: 0.5973 - val_accuracy: 0.6685\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.5559 - accuracy: 0.7196 - val_loss: 0.5961 - val_accuracy: 0.6685\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.5549 - accuracy: 0.7196 - val_loss: 0.5951 - val_accuracy: 0.6685\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.5539 - accuracy: 0.7196 - val_loss: 0.5940 - val_accuracy: 0.6685\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.5529 - accuracy: 0.7196 - val_loss: 0.5929 - val_accuracy: 0.6685\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.5519 - accuracy: 0.7196 - val_loss: 0.5918 - val_accuracy: 0.6685\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.5509 - accuracy: 0.7196 - val_loss: 0.5907 - val_accuracy: 0.6685\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.5498 - accuracy: 0.7196 - val_loss: 0.5894 - val_accuracy: 0.6685\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.5488 - accuracy: 0.7196 - val_loss: 0.5881 - val_accuracy: 0.6685\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.5477 - accuracy: 0.7196 - val_loss: 0.5866 - val_accuracy: 0.6685\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.5466 - accuracy: 0.7196 - val_loss: 0.5851 - val_accuracy: 0.6685\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.5454 - accuracy: 0.7196 - val_loss: 0.5835 - val_accuracy: 0.6685\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.5441 - accuracy: 0.7196 - val_loss: 0.5819 - val_accuracy: 0.6685\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.5429 - accuracy: 0.7196 - val_loss: 0.5803 - val_accuracy: 0.6685\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.5417 - accuracy: 0.7196 - val_loss: 0.5787 - val_accuracy: 0.6685\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.5405 - accuracy: 0.7196 - val_loss: 0.5771 - val_accuracy: 0.6685\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.5393 - accuracy: 0.7196 - val_loss: 0.5755 - val_accuracy: 0.6685\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.5381 - accuracy: 0.7196 - val_loss: 0.5738 - val_accuracy: 0.6685\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.5369 - accuracy: 0.7196 - val_loss: 0.5722 - val_accuracy: 0.6685\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.5356 - accuracy: 0.7196 - val_loss: 0.5706 - val_accuracy: 0.6685\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.5344 - accuracy: 0.7196 - val_loss: 0.5691 - val_accuracy: 0.6685\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.5331 - accuracy: 0.7196 - val_loss: 0.5676 - val_accuracy: 0.6685\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.5318 - accuracy: 0.7196 - val_loss: 0.5661 - val_accuracy: 0.6685\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.5306 - accuracy: 0.7196 - val_loss: 0.5646 - val_accuracy: 0.6685\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.5293 - accuracy: 0.7196 - val_loss: 0.5630 - val_accuracy: 0.6685\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.5280 - accuracy: 0.7196 - val_loss: 0.5614 - val_accuracy: 0.6685\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.5266 - accuracy: 0.7196 - val_loss: 0.5598 - val_accuracy: 0.6685\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.5253 - accuracy: 0.7196 - val_loss: 0.5581 - val_accuracy: 0.6685\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.5240 - accuracy: 0.7196 - val_loss: 0.5564 - val_accuracy: 0.6740\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.5226 - accuracy: 0.7196 - val_loss: 0.5547 - val_accuracy: 0.6740\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.5213 - accuracy: 0.7196 - val_loss: 0.5529 - val_accuracy: 0.6740\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.5199 - accuracy: 0.7210 - val_loss: 0.5510 - val_accuracy: 0.6851\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.5185 - accuracy: 0.7224 - val_loss: 0.5492 - val_accuracy: 0.6851\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.5171 - accuracy: 0.7224 - val_loss: 0.5474 - val_accuracy: 0.6851\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.5158 - accuracy: 0.7279 - val_loss: 0.5457 - val_accuracy: 0.6851\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.5144 - accuracy: 0.7334 - val_loss: 0.5440 - val_accuracy: 0.6851\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.5130 - accuracy: 0.7362 - val_loss: 0.5424 - val_accuracy: 0.6851\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.5116 - accuracy: 0.7362 - val_loss: 0.5407 - val_accuracy: 0.6851\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.5101 - accuracy: 0.7417 - val_loss: 0.5389 - val_accuracy: 0.6851\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.5087 - accuracy: 0.7417 - val_loss: 0.5372 - val_accuracy: 0.6851\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.5073 - accuracy: 0.7417 - val_loss: 0.5353 - val_accuracy: 0.6851\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.5059 - accuracy: 0.7472 - val_loss: 0.5335 - val_accuracy: 0.6851\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.5044 - accuracy: 0.7472 - val_loss: 0.5318 - val_accuracy: 0.6851\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.5030 - accuracy: 0.7514 - val_loss: 0.5301 - val_accuracy: 0.6851\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.5016 - accuracy: 0.7528 - val_loss: 0.5285 - val_accuracy: 0.6906\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.5001 - accuracy: 0.7472 - val_loss: 0.5269 - val_accuracy: 0.6906\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.4987 - accuracy: 0.7472 - val_loss: 0.5251 - val_accuracy: 0.7017\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.4972 - accuracy: 0.7514 - val_loss: 0.5233 - val_accuracy: 0.7017\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.4958 - accuracy: 0.7514 - val_loss: 0.5215 - val_accuracy: 0.7127\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.4943 - accuracy: 0.7459 - val_loss: 0.5197 - val_accuracy: 0.7127\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.4929 - accuracy: 0.7472 - val_loss: 0.5179 - val_accuracy: 0.7127\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.4915 - accuracy: 0.7472 - val_loss: 0.5163 - val_accuracy: 0.7072\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.4900 - accuracy: 0.7472 - val_loss: 0.5147 - val_accuracy: 0.7072\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.4886 - accuracy: 0.7486 - val_loss: 0.5132 - val_accuracy: 0.7127\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.4872 - accuracy: 0.7486 - val_loss: 0.5117 - val_accuracy: 0.7238\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.4857 - accuracy: 0.7528 - val_loss: 0.5100 - val_accuracy: 0.7238\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.4843 - accuracy: 0.7555 - val_loss: 0.5083 - val_accuracy: 0.7293\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.4829 - accuracy: 0.7555 - val_loss: 0.5065 - val_accuracy: 0.7348\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.4815 - accuracy: 0.7555 - val_loss: 0.5048 - val_accuracy: 0.7348\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.4800 - accuracy: 0.7541 - val_loss: 0.5032 - val_accuracy: 0.7459\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.4786 - accuracy: 0.7541 - val_loss: 0.5017 - val_accuracy: 0.7459\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.4772 - accuracy: 0.7500 - val_loss: 0.5003 - val_accuracy: 0.7514\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.4758 - accuracy: 0.7500 - val_loss: 0.4987 - val_accuracy: 0.7514\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.4744 - accuracy: 0.7500 - val_loss: 0.4971 - val_accuracy: 0.7514\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.4730 - accuracy: 0.7500 - val_loss: 0.4955 - val_accuracy: 0.7569\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.4715 - accuracy: 0.7555 - val_loss: 0.4939 - val_accuracy: 0.7569\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.4701 - accuracy: 0.7583 - val_loss: 0.4924 - val_accuracy: 0.7569\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.4687 - accuracy: 0.7749 - val_loss: 0.4910 - val_accuracy: 0.7569\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.4673 - accuracy: 0.7790 - val_loss: 0.4896 - val_accuracy: 0.7624\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.4659 - accuracy: 0.7790 - val_loss: 0.4882 - val_accuracy: 0.7624\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.4645 - accuracy: 0.7790 - val_loss: 0.4867 - val_accuracy: 0.7624\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.4632 - accuracy: 0.7818 - val_loss: 0.4852 - val_accuracy: 0.7624\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.4618 - accuracy: 0.7818 - val_loss: 0.4837 - val_accuracy: 0.7680\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.4604 - accuracy: 0.7859 - val_loss: 0.4823 - val_accuracy: 0.7680\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.4590 - accuracy: 0.7859 - val_loss: 0.4809 - val_accuracy: 0.7680\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.4577 - accuracy: 0.7818 - val_loss: 0.4794 - val_accuracy: 0.7680\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4563 - accuracy: 0.7818 - val_loss: 0.4780 - val_accuracy: 0.7680\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4550 - accuracy: 0.7887 - val_loss: 0.4766 - val_accuracy: 0.7680\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4536 - accuracy: 0.7887 - val_loss: 0.4753 - val_accuracy: 0.7680\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.4523 - accuracy: 0.7942 - val_loss: 0.4740 - val_accuracy: 0.7569\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.4510 - accuracy: 0.7942 - val_loss: 0.4728 - val_accuracy: 0.7569\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4496 - accuracy: 0.7942 - val_loss: 0.4717 - val_accuracy: 0.7569\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4483 - accuracy: 0.7887 - val_loss: 0.4706 - val_accuracy: 0.7624\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.4471 - accuracy: 0.7887 - val_loss: 0.4695 - val_accuracy: 0.7790\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.4458 - accuracy: 0.7887 - val_loss: 0.4683 - val_accuracy: 0.7790\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.4445 - accuracy: 0.7942 - val_loss: 0.4670 - val_accuracy: 0.7790\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.4432 - accuracy: 0.7942 - val_loss: 0.4658 - val_accuracy: 0.7845\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.4420 - accuracy: 0.7914 - val_loss: 0.4647 - val_accuracy: 0.7790\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.4408 - accuracy: 0.7956 - val_loss: 0.4638 - val_accuracy: 0.7790\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.4395 - accuracy: 0.7956 - val_loss: 0.4629 - val_accuracy: 0.7790\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.4382 - accuracy: 0.7970 - val_loss: 0.4619 - val_accuracy: 0.7790\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.4370 - accuracy: 0.7970 - val_loss: 0.4607 - val_accuracy: 0.7790\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.4358 - accuracy: 0.7970 - val_loss: 0.4596 - val_accuracy: 0.7790\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.4346 - accuracy: 0.7970 - val_loss: 0.4587 - val_accuracy: 0.7790\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.4335 - accuracy: 0.7970 - val_loss: 0.4580 - val_accuracy: 0.7790\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.4323 - accuracy: 0.7956 - val_loss: 0.4574 - val_accuracy: 0.7901\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.4311 - accuracy: 0.7956 - val_loss: 0.4566 - val_accuracy: 0.7901\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.4300 - accuracy: 0.7956 - val_loss: 0.4557 - val_accuracy: 0.7845\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4289 - accuracy: 0.7956 - val_loss: 0.4547 - val_accuracy: 0.7956\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.4277 - accuracy: 0.7956 - val_loss: 0.4538 - val_accuracy: 0.7956\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.4266 - accuracy: 0.7956 - val_loss: 0.4531 - val_accuracy: 0.7956\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.4255 - accuracy: 0.7956 - val_loss: 0.4527 - val_accuracy: 0.7956\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.4245 - accuracy: 0.8025 - val_loss: 0.4520 - val_accuracy: 0.7956\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.4234 - accuracy: 0.8025 - val_loss: 0.4511 - val_accuracy: 0.7956\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.4223 - accuracy: 0.7997 - val_loss: 0.4502 - val_accuracy: 0.7956\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.4213 - accuracy: 0.8025 - val_loss: 0.4495 - val_accuracy: 0.7956\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.4203 - accuracy: 0.8025 - val_loss: 0.4492 - val_accuracy: 0.7956\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4192 - accuracy: 0.8025 - val_loss: 0.4488 - val_accuracy: 0.7956\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.4182 - accuracy: 0.8025 - val_loss: 0.4481 - val_accuracy: 0.7956\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4172 - accuracy: 0.8025 - val_loss: 0.4473 - val_accuracy: 0.8011\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4163 - accuracy: 0.8080 - val_loss: 0.4466 - val_accuracy: 0.8011\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.4153 - accuracy: 0.8080 - val_loss: 0.4462 - val_accuracy: 0.8011\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.4143 - accuracy: 0.8122 - val_loss: 0.4460 - val_accuracy: 0.8011\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.4134 - accuracy: 0.8122 - val_loss: 0.4457 - val_accuracy: 0.7956\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.4125 - accuracy: 0.8122 - val_loss: 0.4451 - val_accuracy: 0.7956\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.4116 - accuracy: 0.8122 - val_loss: 0.4444 - val_accuracy: 0.7956\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4106 - accuracy: 0.8122 - val_loss: 0.4439 - val_accuracy: 0.7956\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.4098 - accuracy: 0.8122 - val_loss: 0.4437 - val_accuracy: 0.7956\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.4089 - accuracy: 0.8108 - val_loss: 0.4436 - val_accuracy: 0.7845\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.4080 - accuracy: 0.8108 - val_loss: 0.4432 - val_accuracy: 0.7845\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.4071 - accuracy: 0.8135 - val_loss: 0.4426 - val_accuracy: 0.7956\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.4063 - accuracy: 0.8135 - val_loss: 0.4421 - val_accuracy: 0.7956\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.4055 - accuracy: 0.8135 - val_loss: 0.4420 - val_accuracy: 0.7956\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.4046 - accuracy: 0.8135 - val_loss: 0.4419 - val_accuracy: 0.7956\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8d76c262d0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99bc447b-4cf8-4efe-acd3-5f3d3204b76a"
      },
      "source": [
        "-----\n",
        "## 5. Diabetes data of Dr John Schorling\n",
        "\n",
        "These data are courtesy of Dr John Schorling, Department of Medicine, University of Virginia School of Medicine. The data consist of 19 variables on 403 subjects from 1046 subjects who were interviewed in a study to understand the prevalence of obesity, diabetes, and other cardiovascular risk factors in central Virginia for African Americans. According to Dr John Hong, Diabetes Mellitus Type II (adult onset diabetes) is associated most strongly with obesity. The waist/hip ratio may be a predictor in diabetes and heart disease. DM II is also agssociated with hypertension - they may both be part of \"Syndrome X\". The 403 subjects were the ones who were actually screened for diabetes. Glycosolated hemoglobin > 7.0 is usually taken as a positive diagnosis of diabetes.\n",
        "\n",
        "####  Attribute Information\n",
        "\n",
        "A data frame with 205 observations on the following 12 variables.\n",
        "\n",
        "| Column Name    | Expression    |\n",
        "| :------------- | :------------- |\n",
        "| id|subject id |\n",
        "| chol| Total Cholesterol|\n",
        "| stab.glu|Stabilized Glucose |\n",
        "|  hdl| High Density Lipoprotein|\n",
        "| ratio |Cholesterol/HDL Ratio |\n",
        "|  glyhb| Glycosolated Hemoglobin|\n",
        "|  location| a factor with levels (Buckingham,Louisa)|\n",
        "| age |age (years) |\n",
        "|  gender| male or female|\n",
        "|  height|height (inches)|\n",
        "|  weight| weight (pounds)|\n",
        "|  frame| a factor with levels (small,medium,large)|\n",
        "|  bp.1s| First Systolic Blood Pressure|\n",
        "|  bp.1d| First Diastolic Blood Pressure|\n",
        "|  bp.2s|Second Diastolic Blood Pressure |\n",
        "|  bp.2d| Second Diastolic Blood Pressure|\n",
        "|  waist| waist in inches|\n",
        "|  hip|hip in inches |\n",
        "|  time.ppn| Postprandial Time when Labs were Drawn in minutes|\n",
        "|  AgeGroups| Categorized age|\n",
        "|  height.europe|height (cm) |\n",
        "|  weight.europe| weight (kg)|\n",
        "|  BMI| Categorized BMI|\n"
      ],
      "id": "99bc447b-4cf8-4efe-acd3-5f3d3204b76a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2146c395-8103-4505-b740-e0b1e5bf9084",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "bfc61d0e-ed80-4d19-f3b3-bf652af55d57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Droping NA Number of Rows: 403\n",
            "After Droping NA Number of Rows: 366\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      chol  stab.glu    hdl  ratio  age  gender  height  weight  frame  bp.1s  \\\n",
              "361  301.0        90  118.0    2.6   89       0    61.0   115.0      1  218.0   \n",
              "362  296.0       369   46.0    6.4   53       1    69.0   173.0      1  138.0   \n",
              "363  284.0        89   54.0    5.3   51       0    63.0   154.0      1  140.0   \n",
              "364  194.0       269   38.0    5.1   29       0    69.0   167.0      2  120.0   \n",
              "365  199.0        76   52.0    3.8   41       0    63.0   197.0      1  120.0   \n",
              "\n",
              "     bp.1d  waist   hip  time.ppn  AgeGroups        BMI  glyhb  \n",
              "361   90.0   31.0  41.0     210.0          4  21.728862    0.0  \n",
              "362   94.0   35.0  39.0     210.0          1  25.547402    1.0  \n",
              "363  100.0   32.0  43.0     180.0          1  27.279628    0.0  \n",
              "364   70.0   33.0  40.0      20.0          3  24.661365    1.0  \n",
              "365   78.0   41.0  48.0     255.0          0  34.896666    0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-efc82f50-05f7-4cce-9c43-485bed134d46\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chol</th>\n",
              "      <th>stab.glu</th>\n",
              "      <th>hdl</th>\n",
              "      <th>ratio</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>frame</th>\n",
              "      <th>bp.1s</th>\n",
              "      <th>bp.1d</th>\n",
              "      <th>waist</th>\n",
              "      <th>hip</th>\n",
              "      <th>time.ppn</th>\n",
              "      <th>AgeGroups</th>\n",
              "      <th>BMI</th>\n",
              "      <th>glyhb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>301.0</td>\n",
              "      <td>90</td>\n",
              "      <td>118.0</td>\n",
              "      <td>2.6</td>\n",
              "      <td>89</td>\n",
              "      <td>0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>1</td>\n",
              "      <td>218.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>210.0</td>\n",
              "      <td>4</td>\n",
              "      <td>21.728862</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>296.0</td>\n",
              "      <td>369</td>\n",
              "      <td>46.0</td>\n",
              "      <td>6.4</td>\n",
              "      <td>53</td>\n",
              "      <td>1</td>\n",
              "      <td>69.0</td>\n",
              "      <td>173.0</td>\n",
              "      <td>1</td>\n",
              "      <td>138.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>210.0</td>\n",
              "      <td>1</td>\n",
              "      <td>25.547402</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>284.0</td>\n",
              "      <td>89</td>\n",
              "      <td>54.0</td>\n",
              "      <td>5.3</td>\n",
              "      <td>51</td>\n",
              "      <td>0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>154.0</td>\n",
              "      <td>1</td>\n",
              "      <td>140.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>1</td>\n",
              "      <td>27.279628</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>194.0</td>\n",
              "      <td>269</td>\n",
              "      <td>38.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>167.0</td>\n",
              "      <td>2</td>\n",
              "      <td>120.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>3</td>\n",
              "      <td>24.661365</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>365</th>\n",
              "      <td>199.0</td>\n",
              "      <td>76</td>\n",
              "      <td>52.0</td>\n",
              "      <td>3.8</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>197.0</td>\n",
              "      <td>1</td>\n",
              "      <td>120.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>255.0</td>\n",
              "      <td>0</td>\n",
              "      <td>34.896666</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-efc82f50-05f7-4cce-9c43-485bed134d46')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-efc82f50-05f7-4cce-9c43-485bed134d46 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-efc82f50-05f7-4cce-9c43-485bed134d46');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Read 'John.csv'\n",
        "john_df = pd.read_csv(\"./data/John.csv\")\n",
        "\n",
        "\n",
        "# Delete redundant and useless/irrelevant columns\n",
        "del john_df['bp.2s']\n",
        "del john_df['bp.2d']\n",
        "del john_df['height.europe']\n",
        "del john_df['weight.europe']\n",
        "del john_df['id']\n",
        "del john_df['location']\n",
        "\n",
        "# Drop NaN value\n",
        "print(\"Before Droping NA Number of Rows:\", len(john_df))\n",
        "\n",
        "john_df = john_df.dropna()\n",
        "print(\"After Droping NA Number of Rows:\", len(john_df))\n",
        "\n",
        "# Reset the Index after dropping NaN\n",
        "john_df = john_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Encode textual columns\n",
        "le = LabelEncoder()\n",
        "for each in ['gender', 'frame', 'AgeGroups']:\n",
        "    john_df[each] = le.fit_transform(john_df[each])\n",
        "\n",
        "\n",
        "john_df.loc[john_df.glyhb <= 7, 'glyhb'] = 0\n",
        "john_df.loc[john_df.glyhb > 7, 'glyhb'] = 1\n",
        "\n",
        "# Move 'glyhb' columns to the end of dataframe\n",
        "john_df['glyhb'] = john_df.pop('glyhb')\n",
        "\n",
        "john_df.tail()"
      ],
      "id": "2146c395-8103-4505-b740-e0b1e5bf9084"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "923824dc-c02c-451d-b840-811a8b7bc902"
      },
      "outputs": [],
      "source": [
        "# Correlation contingency table\n",
        "john_corr = john_df.corr()\n",
        "john_corr"
      ],
      "id": "923824dc-c02c-451d-b840-811a8b7bc902"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f71f7b4-0250-41c0-898f-4b4dab0c7d43"
      },
      "source": [
        "Moreover, we evaluate the Weighted Average of the correlation of predictors:\n",
        "\n",
        "$\\text{Weighted Average of Correlation}$ $=$ $\\frac{\\text{Sum of Correlation of Predictors}}{\\text{Number of Predictors}}$ $=$ $0.134\n",
        "$"
      ],
      "id": "9f71f7b4-0250-41c0-898f-4b4dab0c7d43"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3172356-3209-407f-8a96-1cb33be36d1c"
      },
      "outputs": [],
      "source": [
        "# Evaluate the Weighted Average of the correlation of predictors\n",
        "\n",
        "print(\"Weighted Average:\", sum(john_corr.iloc[-1,:-1])/16)"
      ],
      "id": "c3172356-3209-407f-8a96-1cb33be36d1c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5518f8d2-2a03-437d-b9fd-6e17ab5be468"
      },
      "source": [
        "-----\n",
        "\n",
        "### 5.1 KNN"
      ],
      "id": "5518f8d2-2a03-437d-b9fd-6e17ab5be468"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88d777f6-ff47-477d-a44c-0e0c83c5aea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3978e403-d576-437f-8220-0bc6ec6e135b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k: 1 Training Score: 1.0 Validation Score:  0.7796610169491526\n",
            "k: 2 Training Score: 0.9141630901287554 Validation Score:  0.864406779661017\n",
            "k: 3 Training Score: 0.9184549356223176 Validation Score:  0.864406779661017\n",
            "k: 5 Training Score: 0.9098712446351931 Validation Score:  0.8813559322033898\n",
            "k: 7 Training Score: 0.8969957081545065 Validation Score:  0.8983050847457628\n",
            "k: 9 Training Score: 0.8927038626609443 Validation Score:  0.9152542372881356\n",
            "k: 15 Training Score: 0.8841201716738197 Validation Score:  0.9152542372881356\n",
            "k: 31 Training Score: 0.8626609442060086 Validation Score:  0.8813559322033898\n",
            "k: 51 Training Score: 0.8412017167381974 Validation Score:  0.864406779661017\n",
            "k: 233 Training Score: 0.8369098712446352 Validation Score:  0.864406779661017\n",
            "The best k is 15 and the best val score is 0.9153\n",
            "Best Model Validation Score: 0.9153 \n",
            " Training Score: 0.8841 \n",
            " Test Score: 0.8919\n",
            "Testing: Sensitivity/Recall: 0.2 Specificity: 1.0\n"
          ]
        }
      ],
      "source": [
        "knn_classifier(john_df)"
      ],
      "id": "88d777f6-ff47-477d-a44c-0e0c83c5aea9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8623546e-d26f-4948-920c-1483f8802a8b"
      },
      "source": [
        "-----\n",
        "\n",
        "### 5.2 Logistic Regression"
      ],
      "id": "8623546e-d26f-4948-920c-1483f8802a8b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e172a3a-1dfb-49f5-8f93-2f936d996a67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23da2e24-3b29-4976-9d45-91ee075fa8cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.9315068493150684\n",
            "Testing Accuracy: 0.9324324324324325\n",
            "Testing: Sensitivity/Recall: 0.6 Specificity: 0.984375\n"
          ]
        }
      ],
      "source": [
        "lg_classifier(john_df)"
      ],
      "id": "9e172a3a-1dfb-49f5-8f93-2f936d996a67"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.3.2 Keras\n",
        "5.3.2.1 One hidden layers with 2 neurons in output layer"
      ],
      "metadata": {
        "id": "JhtH6YYwC1Cw"
      },
      "id": "JhtH6YYwC1Cw"
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, test_x, train_y, test_y = data_split(john_df)\n",
        "\n",
        "train_x = np.float32(train_x)\n",
        "test_x = np.float32(test_x)\n",
        "\n",
        "train_x = train_x/train_x.max()\n",
        "test_x = test_x/test_x.max()\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(2)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_x, train_y, epochs=200, batch_size=1024,\n",
        "          validation_data=(test_x, test_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln1TGOXTC6T7",
        "outputId": "d456e272-948d-4551-bbcf-43da8162906e"
      },
      "id": "ln1TGOXTC6T7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 1s 639ms/step - loss: 0.7007 - accuracy: 0.1404 - val_loss: 0.6956 - val_accuracy: 0.2838\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6955 - accuracy: 0.3116 - val_loss: 0.6895 - val_accuracy: 0.7973\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6903 - accuracy: 0.7089 - val_loss: 0.6836 - val_accuracy: 0.8514\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6852 - accuracy: 0.8356 - val_loss: 0.6778 - val_accuracy: 0.8649\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6801 - accuracy: 0.8425 - val_loss: 0.6722 - val_accuracy: 0.8649\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6751 - accuracy: 0.8425 - val_loss: 0.6666 - val_accuracy: 0.8649\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6701 - accuracy: 0.8425 - val_loss: 0.6610 - val_accuracy: 0.8649\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6651 - accuracy: 0.8425 - val_loss: 0.6554 - val_accuracy: 0.8649\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6601 - accuracy: 0.8425 - val_loss: 0.6498 - val_accuracy: 0.8649\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6551 - accuracy: 0.8425 - val_loss: 0.6442 - val_accuracy: 0.8649\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6501 - accuracy: 0.8425 - val_loss: 0.6386 - val_accuracy: 0.8649\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6452 - accuracy: 0.8425 - val_loss: 0.6330 - val_accuracy: 0.8649\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6402 - accuracy: 0.8425 - val_loss: 0.6275 - val_accuracy: 0.8649\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6352 - accuracy: 0.8425 - val_loss: 0.6220 - val_accuracy: 0.8649\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6302 - accuracy: 0.8425 - val_loss: 0.6163 - val_accuracy: 0.8649\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6251 - accuracy: 0.8425 - val_loss: 0.6107 - val_accuracy: 0.8649\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6199 - accuracy: 0.8425 - val_loss: 0.6049 - val_accuracy: 0.8649\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6145 - accuracy: 0.8425 - val_loss: 0.5989 - val_accuracy: 0.8649\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6090 - accuracy: 0.8425 - val_loss: 0.5927 - val_accuracy: 0.8649\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6033 - accuracy: 0.8425 - val_loss: 0.5865 - val_accuracy: 0.8649\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.5976 - accuracy: 0.8425 - val_loss: 0.5801 - val_accuracy: 0.8649\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.5919 - accuracy: 0.8425 - val_loss: 0.5737 - val_accuracy: 0.8649\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.5861 - accuracy: 0.8425 - val_loss: 0.5671 - val_accuracy: 0.8649\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.5802 - accuracy: 0.8425 - val_loss: 0.5605 - val_accuracy: 0.8649\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.5742 - accuracy: 0.8425 - val_loss: 0.5539 - val_accuracy: 0.8649\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.5683 - accuracy: 0.8425 - val_loss: 0.5474 - val_accuracy: 0.8649\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5625 - accuracy: 0.8425 - val_loss: 0.5410 - val_accuracy: 0.8649\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.5568 - accuracy: 0.8425 - val_loss: 0.5347 - val_accuracy: 0.8649\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.5511 - accuracy: 0.8425 - val_loss: 0.5284 - val_accuracy: 0.8649\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.5455 - accuracy: 0.8425 - val_loss: 0.5222 - val_accuracy: 0.8649\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.5399 - accuracy: 0.8425 - val_loss: 0.5160 - val_accuracy: 0.8649\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.5345 - accuracy: 0.8425 - val_loss: 0.5100 - val_accuracy: 0.8649\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.5291 - accuracy: 0.8425 - val_loss: 0.5041 - val_accuracy: 0.8649\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.5239 - accuracy: 0.8425 - val_loss: 0.4983 - val_accuracy: 0.8649\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.5188 - accuracy: 0.8425 - val_loss: 0.4926 - val_accuracy: 0.8649\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.5138 - accuracy: 0.8425 - val_loss: 0.4871 - val_accuracy: 0.8649\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.5089 - accuracy: 0.8425 - val_loss: 0.4817 - val_accuracy: 0.8649\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.5042 - accuracy: 0.8425 - val_loss: 0.4765 - val_accuracy: 0.8649\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4996 - accuracy: 0.8425 - val_loss: 0.4715 - val_accuracy: 0.8649\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4952 - accuracy: 0.8425 - val_loss: 0.4666 - val_accuracy: 0.8649\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4909 - accuracy: 0.8425 - val_loss: 0.4619 - val_accuracy: 0.8649\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.4869 - accuracy: 0.8425 - val_loss: 0.4574 - val_accuracy: 0.8649\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4830 - accuracy: 0.8425 - val_loss: 0.4532 - val_accuracy: 0.8649\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4793 - accuracy: 0.8425 - val_loss: 0.4491 - val_accuracy: 0.8649\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4758 - accuracy: 0.8425 - val_loss: 0.4452 - val_accuracy: 0.8649\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4725 - accuracy: 0.8425 - val_loss: 0.4416 - val_accuracy: 0.8649\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4694 - accuracy: 0.8425 - val_loss: 0.4382 - val_accuracy: 0.8649\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4665 - accuracy: 0.8425 - val_loss: 0.4351 - val_accuracy: 0.8649\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4639 - accuracy: 0.8425 - val_loss: 0.4322 - val_accuracy: 0.8649\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.4615 - accuracy: 0.8425 - val_loss: 0.4295 - val_accuracy: 0.8649\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4593 - accuracy: 0.8425 - val_loss: 0.4271 - val_accuracy: 0.8649\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4573 - accuracy: 0.8425 - val_loss: 0.4248 - val_accuracy: 0.8649\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4555 - accuracy: 0.8425 - val_loss: 0.4228 - val_accuracy: 0.8649\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.4539 - accuracy: 0.8425 - val_loss: 0.4210 - val_accuracy: 0.8649\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4525 - accuracy: 0.8425 - val_loss: 0.4194 - val_accuracy: 0.8649\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4513 - accuracy: 0.8425 - val_loss: 0.4180 - val_accuracy: 0.8649\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4502 - accuracy: 0.8425 - val_loss: 0.4167 - val_accuracy: 0.8649\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4492 - accuracy: 0.8425 - val_loss: 0.4157 - val_accuracy: 0.8649\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4484 - accuracy: 0.8425 - val_loss: 0.4147 - val_accuracy: 0.8649\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4477 - accuracy: 0.8425 - val_loss: 0.4139 - val_accuracy: 0.8649\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4471 - accuracy: 0.8425 - val_loss: 0.4131 - val_accuracy: 0.8649\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.4466 - accuracy: 0.8425 - val_loss: 0.4124 - val_accuracy: 0.8649\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4461 - accuracy: 0.8425 - val_loss: 0.4118 - val_accuracy: 0.8649\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.4457 - accuracy: 0.8425 - val_loss: 0.4113 - val_accuracy: 0.8649\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4454 - accuracy: 0.8425 - val_loss: 0.4108 - val_accuracy: 0.8649\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4450 - accuracy: 0.8425 - val_loss: 0.4103 - val_accuracy: 0.8649\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.4447 - accuracy: 0.8425 - val_loss: 0.4099 - val_accuracy: 0.8649\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4444 - accuracy: 0.8425 - val_loss: 0.4095 - val_accuracy: 0.8649\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4440 - accuracy: 0.8425 - val_loss: 0.4091 - val_accuracy: 0.8649\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4437 - accuracy: 0.8425 - val_loss: 0.4087 - val_accuracy: 0.8649\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4434 - accuracy: 0.8425 - val_loss: 0.4083 - val_accuracy: 0.8649\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4430 - accuracy: 0.8425 - val_loss: 0.4079 - val_accuracy: 0.8649\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4426 - accuracy: 0.8425 - val_loss: 0.4075 - val_accuracy: 0.8649\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4422 - accuracy: 0.8425 - val_loss: 0.4071 - val_accuracy: 0.8649\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4419 - accuracy: 0.8425 - val_loss: 0.4067 - val_accuracy: 0.8649\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4415 - accuracy: 0.8425 - val_loss: 0.4063 - val_accuracy: 0.8649\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.4410 - accuracy: 0.8425 - val_loss: 0.4059 - val_accuracy: 0.8649\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4406 - accuracy: 0.8425 - val_loss: 0.4056 - val_accuracy: 0.8649\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4402 - accuracy: 0.8425 - val_loss: 0.4052 - val_accuracy: 0.8649\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4398 - accuracy: 0.8425 - val_loss: 0.4048 - val_accuracy: 0.8649\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4393 - accuracy: 0.8425 - val_loss: 0.4044 - val_accuracy: 0.8649\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4389 - accuracy: 0.8425 - val_loss: 0.4041 - val_accuracy: 0.8649\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4384 - accuracy: 0.8425 - val_loss: 0.4037 - val_accuracy: 0.8649\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4380 - accuracy: 0.8425 - val_loss: 0.4033 - val_accuracy: 0.8649\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4376 - accuracy: 0.8425 - val_loss: 0.4029 - val_accuracy: 0.8649\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4371 - accuracy: 0.8425 - val_loss: 0.4026 - val_accuracy: 0.8649\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4367 - accuracy: 0.8425 - val_loss: 0.4022 - val_accuracy: 0.8649\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4363 - accuracy: 0.8425 - val_loss: 0.4018 - val_accuracy: 0.8649\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.4358 - accuracy: 0.8425 - val_loss: 0.4014 - val_accuracy: 0.8649\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4354 - accuracy: 0.8425 - val_loss: 0.4010 - val_accuracy: 0.8649\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4349 - accuracy: 0.8425 - val_loss: 0.4005 - val_accuracy: 0.8649\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4345 - accuracy: 0.8425 - val_loss: 0.4001 - val_accuracy: 0.8649\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4340 - accuracy: 0.8425 - val_loss: 0.3997 - val_accuracy: 0.8649\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4336 - accuracy: 0.8425 - val_loss: 0.3992 - val_accuracy: 0.8649\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4331 - accuracy: 0.8425 - val_loss: 0.3988 - val_accuracy: 0.8649\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4326 - accuracy: 0.8425 - val_loss: 0.3983 - val_accuracy: 0.8649\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4321 - accuracy: 0.8425 - val_loss: 0.3978 - val_accuracy: 0.8649\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4317 - accuracy: 0.8425 - val_loss: 0.3973 - val_accuracy: 0.8649\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4312 - accuracy: 0.8425 - val_loss: 0.3968 - val_accuracy: 0.8649\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4307 - accuracy: 0.8425 - val_loss: 0.3964 - val_accuracy: 0.8649\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4302 - accuracy: 0.8425 - val_loss: 0.3958 - val_accuracy: 0.8649\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4297 - accuracy: 0.8425 - val_loss: 0.3953 - val_accuracy: 0.8649\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4293 - accuracy: 0.8425 - val_loss: 0.3948 - val_accuracy: 0.8649\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4288 - accuracy: 0.8425 - val_loss: 0.3943 - val_accuracy: 0.8649\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.4283 - accuracy: 0.8425 - val_loss: 0.3938 - val_accuracy: 0.8649\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4278 - accuracy: 0.8425 - val_loss: 0.3933 - val_accuracy: 0.8649\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4273 - accuracy: 0.8425 - val_loss: 0.3928 - val_accuracy: 0.8649\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4268 - accuracy: 0.8425 - val_loss: 0.3922 - val_accuracy: 0.8649\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4263 - accuracy: 0.8425 - val_loss: 0.3917 - val_accuracy: 0.8649\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.4258 - accuracy: 0.8425 - val_loss: 0.3912 - val_accuracy: 0.8649\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4252 - accuracy: 0.8425 - val_loss: 0.3907 - val_accuracy: 0.8649\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.4247 - accuracy: 0.8425 - val_loss: 0.3902 - val_accuracy: 0.8649\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4242 - accuracy: 0.8425 - val_loss: 0.3896 - val_accuracy: 0.8649\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4237 - accuracy: 0.8425 - val_loss: 0.3891 - val_accuracy: 0.8649\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4231 - accuracy: 0.8425 - val_loss: 0.3886 - val_accuracy: 0.8649\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.4226 - accuracy: 0.8425 - val_loss: 0.3880 - val_accuracy: 0.8649\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4220 - accuracy: 0.8425 - val_loss: 0.3875 - val_accuracy: 0.8649\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4215 - accuracy: 0.8425 - val_loss: 0.3869 - val_accuracy: 0.8649\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4209 - accuracy: 0.8425 - val_loss: 0.3864 - val_accuracy: 0.8649\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4204 - accuracy: 0.8425 - val_loss: 0.3858 - val_accuracy: 0.8649\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4198 - accuracy: 0.8425 - val_loss: 0.3853 - val_accuracy: 0.8649\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4192 - accuracy: 0.8425 - val_loss: 0.3847 - val_accuracy: 0.8649\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4187 - accuracy: 0.8425 - val_loss: 0.3841 - val_accuracy: 0.8649\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4181 - accuracy: 0.8425 - val_loss: 0.3835 - val_accuracy: 0.8649\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.4175 - accuracy: 0.8425 - val_loss: 0.3829 - val_accuracy: 0.8649\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4169 - accuracy: 0.8425 - val_loss: 0.3823 - val_accuracy: 0.8649\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.4163 - accuracy: 0.8425 - val_loss: 0.3818 - val_accuracy: 0.8649\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4157 - accuracy: 0.8425 - val_loss: 0.3812 - val_accuracy: 0.8649\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4151 - accuracy: 0.8425 - val_loss: 0.3806 - val_accuracy: 0.8649\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4145 - accuracy: 0.8425 - val_loss: 0.3799 - val_accuracy: 0.8649\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4138 - accuracy: 0.8425 - val_loss: 0.3793 - val_accuracy: 0.8649\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4132 - accuracy: 0.8425 - val_loss: 0.3787 - val_accuracy: 0.8649\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4125 - accuracy: 0.8425 - val_loss: 0.3780 - val_accuracy: 0.8649\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4119 - accuracy: 0.8425 - val_loss: 0.3774 - val_accuracy: 0.8649\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4112 - accuracy: 0.8425 - val_loss: 0.3767 - val_accuracy: 0.8649\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4105 - accuracy: 0.8425 - val_loss: 0.3760 - val_accuracy: 0.8649\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4098 - accuracy: 0.8425 - val_loss: 0.3753 - val_accuracy: 0.8649\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.4091 - accuracy: 0.8425 - val_loss: 0.3746 - val_accuracy: 0.8649\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4084 - accuracy: 0.8425 - val_loss: 0.3739 - val_accuracy: 0.8649\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4076 - accuracy: 0.8425 - val_loss: 0.3731 - val_accuracy: 0.8649\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4069 - accuracy: 0.8425 - val_loss: 0.3723 - val_accuracy: 0.8649\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.4061 - accuracy: 0.8425 - val_loss: 0.3716 - val_accuracy: 0.8649\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4054 - accuracy: 0.8425 - val_loss: 0.3708 - val_accuracy: 0.8649\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4046 - accuracy: 0.8425 - val_loss: 0.3700 - val_accuracy: 0.8649\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4038 - accuracy: 0.8425 - val_loss: 0.3692 - val_accuracy: 0.8649\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4031 - accuracy: 0.8425 - val_loss: 0.3684 - val_accuracy: 0.8649\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.4023 - accuracy: 0.8425 - val_loss: 0.3676 - val_accuracy: 0.8649\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4015 - accuracy: 0.8425 - val_loss: 0.3668 - val_accuracy: 0.8649\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4007 - accuracy: 0.8425 - val_loss: 0.3660 - val_accuracy: 0.8649\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.3999 - accuracy: 0.8425 - val_loss: 0.3652 - val_accuracy: 0.8649\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.3990 - accuracy: 0.8425 - val_loss: 0.3643 - val_accuracy: 0.8649\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.3982 - accuracy: 0.8425 - val_loss: 0.3634 - val_accuracy: 0.8649\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.3973 - accuracy: 0.8425 - val_loss: 0.3626 - val_accuracy: 0.8649\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.3965 - accuracy: 0.8425 - val_loss: 0.3617 - val_accuracy: 0.8649\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.3956 - accuracy: 0.8425 - val_loss: 0.3609 - val_accuracy: 0.8649\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.3947 - accuracy: 0.8425 - val_loss: 0.3600 - val_accuracy: 0.8649\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.3938 - accuracy: 0.8425 - val_loss: 0.3591 - val_accuracy: 0.8649\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.3929 - accuracy: 0.8425 - val_loss: 0.3582 - val_accuracy: 0.8649\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.3919 - accuracy: 0.8425 - val_loss: 0.3573 - val_accuracy: 0.8649\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.3910 - accuracy: 0.8425 - val_loss: 0.3564 - val_accuracy: 0.8649\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3900 - accuracy: 0.8425 - val_loss: 0.3555 - val_accuracy: 0.8649\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3890 - accuracy: 0.8425 - val_loss: 0.3545 - val_accuracy: 0.8649\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.3880 - accuracy: 0.8425 - val_loss: 0.3534 - val_accuracy: 0.8649\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.3869 - accuracy: 0.8425 - val_loss: 0.3523 - val_accuracy: 0.8649\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.3858 - accuracy: 0.8425 - val_loss: 0.3513 - val_accuracy: 0.8649\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.3846 - accuracy: 0.8425 - val_loss: 0.3503 - val_accuracy: 0.8649\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.3835 - accuracy: 0.8425 - val_loss: 0.3493 - val_accuracy: 0.8649\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.3824 - accuracy: 0.8425 - val_loss: 0.3482 - val_accuracy: 0.8649\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3814 - accuracy: 0.8425 - val_loss: 0.3472 - val_accuracy: 0.8649\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.3803 - accuracy: 0.8425 - val_loss: 0.3461 - val_accuracy: 0.8649\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.3792 - accuracy: 0.8425 - val_loss: 0.3449 - val_accuracy: 0.8649\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.3781 - accuracy: 0.8425 - val_loss: 0.3437 - val_accuracy: 0.8649\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.3770 - accuracy: 0.8425 - val_loss: 0.3425 - val_accuracy: 0.8649\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.3759 - accuracy: 0.8425 - val_loss: 0.3413 - val_accuracy: 0.8649\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.3748 - accuracy: 0.8425 - val_loss: 0.3401 - val_accuracy: 0.8649\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.3736 - accuracy: 0.8425 - val_loss: 0.3389 - val_accuracy: 0.8649\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3725 - accuracy: 0.8425 - val_loss: 0.3377 - val_accuracy: 0.8649\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.3713 - accuracy: 0.8425 - val_loss: 0.3366 - val_accuracy: 0.8649\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.3701 - accuracy: 0.8425 - val_loss: 0.3354 - val_accuracy: 0.8649\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.3689 - accuracy: 0.8425 - val_loss: 0.3342 - val_accuracy: 0.8649\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.3677 - accuracy: 0.8425 - val_loss: 0.3331 - val_accuracy: 0.8649\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.3665 - accuracy: 0.8425 - val_loss: 0.3319 - val_accuracy: 0.8649\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.3653 - accuracy: 0.8425 - val_loss: 0.3307 - val_accuracy: 0.8649\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3641 - accuracy: 0.8425 - val_loss: 0.3294 - val_accuracy: 0.8649\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.3628 - accuracy: 0.8425 - val_loss: 0.3282 - val_accuracy: 0.8649\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.3616 - accuracy: 0.8425 - val_loss: 0.3268 - val_accuracy: 0.8649\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3603 - accuracy: 0.8425 - val_loss: 0.3255 - val_accuracy: 0.8649\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.3591 - accuracy: 0.8425 - val_loss: 0.3242 - val_accuracy: 0.8649\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.3578 - accuracy: 0.8425 - val_loss: 0.3229 - val_accuracy: 0.8649\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.3566 - accuracy: 0.8425 - val_loss: 0.3216 - val_accuracy: 0.8649\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.3553 - accuracy: 0.8425 - val_loss: 0.3204 - val_accuracy: 0.8649\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.3540 - accuracy: 0.8425 - val_loss: 0.3191 - val_accuracy: 0.8649\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.3527 - accuracy: 0.8425 - val_loss: 0.3179 - val_accuracy: 0.8649\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.3514 - accuracy: 0.8425 - val_loss: 0.3166 - val_accuracy: 0.8649\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.3501 - accuracy: 0.8425 - val_loss: 0.3153 - val_accuracy: 0.8649\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.3488 - accuracy: 0.8425 - val_loss: 0.3140 - val_accuracy: 0.8649\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.3475 - accuracy: 0.8425 - val_loss: 0.3127 - val_accuracy: 0.8649\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.3461 - accuracy: 0.8425 - val_loss: 0.3113 - val_accuracy: 0.8649\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.3448 - accuracy: 0.8459 - val_loss: 0.3099 - val_accuracy: 0.8649\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.3435 - accuracy: 0.8459 - val_loss: 0.3085 - val_accuracy: 0.8649\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8d76ae5e90>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
